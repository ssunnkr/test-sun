{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66231fa",
   "metadata": {},
   "source": [
    "# GD16.CV.프로젝트: 행동스티커 만들기 - 모델 바꿔보기\n",
    "##### !. aiffel cloud 플랫폼을 사용하여 가상환경 Python 3.9.7버전에서 작성되었습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03d0913d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.7\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb24147b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5550594",
   "metadata": {},
   "source": [
    "### 목차\n",
    "- STEP 0. 주요 라이브러리 import.  \n",
    "- STEP 1. Data 불러오기   \n",
    "- STEP 2. 학습 엔진 만들기  \n",
    "- STEP 3. 예측 엔진 만들기 \n",
    "- 회고록   \n",
    "- Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd718d1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8ae613",
   "metadata": {},
   "source": [
    "### (서론)\n",
    "MPII Humann Pose Dataset을 사용해서 Human Pose Estimation task를 위한 모델을 훈련시키고자 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85beab58",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f92a16",
   "metadata": {},
   "source": [
    "### (본문)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567371e5",
   "metadata": {},
   "source": [
    "#### STEP 0. 주요 라이브러리 import. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6050bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주의! ray를 tensorflow보다 먼저 import하면 오류가 발생할 수 있습니다\n",
    "import io, json, os, math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import ray\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/mpii'\n",
    "IMAGE_PATH = os.path.join(PROJECT_PATH, 'images')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'models')\n",
    "TFRECORD_PATH = os.path.join(PROJECT_PATH, 'tfrecords_mpii')\n",
    "TRAIN_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'train.json')\n",
    "VALID_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'validation.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6cc167",
   "metadata": {},
   "source": [
    "#### STEP 1. Data 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a234213b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"joints_vis\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"joints\": [\n",
      "    [\n",
      "      620.0,\n",
      "      394.0\n",
      "    ],\n",
      "    [\n",
      "      616.0,\n",
      "      269.0\n",
      "    ],\n",
      "    [\n",
      "      573.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      188.0\n",
      "    ],\n",
      "    [\n",
      "      661.0,\n",
      "      221.0\n",
      "    ],\n",
      "    [\n",
      "      656.0,\n",
      "      231.0\n",
      "    ],\n",
      "    [\n",
      "      610.0,\n",
      "      187.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      176.0\n",
      "    ],\n",
      "    [\n",
      "      637.0201,\n",
      "      189.8183\n",
      "    ],\n",
      "    [\n",
      "      695.9799,\n",
      "      108.1817\n",
      "    ],\n",
      "    [\n",
      "      606.0,\n",
      "      217.0\n",
      "    ],\n",
      "    [\n",
      "      553.0,\n",
      "      161.0\n",
      "    ],\n",
      "    [\n",
      "      601.0,\n",
      "      167.0\n",
      "    ],\n",
      "    [\n",
      "      692.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      693.0,\n",
      "      240.0\n",
      "    ],\n",
      "    [\n",
      "      688.0,\n",
      "      313.0\n",
      "    ]\n",
      "  ],\n",
      "  \"image\": \"015601864.jpg\",\n",
      "  \"scale\": 3.021046,\n",
      "  \"center\": [\n",
      "    594.0,\n",
      "    257.0\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    json_formatted_str = json.dumps(train_annos[0], indent=2)\n",
    "    print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59dfe1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f2f6519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n"
     ]
    }
   ],
   "source": [
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    test = parse_one_annotation(train_annos[0], IMAGE_PATH)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12f51119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfexample(anno):\n",
    "\n",
    "    # byte 인코딩을 위한 함수\n",
    "    def _bytes_feature(value):\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f494c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkify(l, n):\n",
    "    size = len(l) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        results.append(l[start:start + size])\n",
    "        start += size\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d658984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "64\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "test_chunks = chunkify([0] * 1000, 64)\n",
    "print(test_chunks)\n",
    "print(len(test_chunks))\n",
    "print(len(test_chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48a4b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for ' + path)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno in chunk:\n",
    "            tf_example = generate_tfexample(anno)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print('finished building tf records for ' + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84b273b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, '{}/{}_{}_of_{}.tfrecords'.format(\n",
    "                TFRECORD_PATH,\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9bda880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 15:24:26,908\tWARNING services.py:1729 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=3.98gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to parse annotations.\n",
      "First train annotation:  {'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n",
      "First val annotation:  {'filename': '005808361.jpg', 'filepath': '/aiffel/aiffel/mpii/images/005808361.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[804.0, 711.0], [816.0, 510.0], [908.0, 438.0], [1040.0, 454.0], [906.0, 528.0], [883.0, 707.0], [974.0, 446.0], [985.0, 253.0], [982.7591, 235.9694], [962.2409, 80.0306], [869.0, 214.0], [798.0, 340.0], [902.0, 253.0], [1067.0, 253.0], [1167.0, 353.0], [1142.0, 478.0]], 'center': [966.0, 340.0], 'scale': 4.718488}\n",
      "Start to build TF Records.\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0008_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "Successfully wrote 25204 annotations to TF Records.\n"
     ]
    }
   ],
   "source": [
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "\n",
    "ray.init()\n",
    "\n",
    "print('Start to parse annotations.')\n",
    "if not os.path.exists(TFRECORD_PATH):\n",
    "    os.makedirs(TFRECORD_PATH)\n",
    "\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    train_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH)\n",
    "        for anno in train_annos\n",
    "    ]\n",
    "    print('First train annotation: ', train_annotations[0])\n",
    "\n",
    "with open(VALID_JSON) as val_json:\n",
    "    val_annos = json.load(val_json)\n",
    "    val_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH) \n",
    "        for anno in val_annos\n",
    "    ]\n",
    "    print('First val annotation: ', val_annotations[0])\n",
    "    \n",
    "print('Start to build TF Records.')\n",
    "build_tf_records(train_annotations, num_train_shards, 'train')\n",
    "build_tf_records(val_annotations, num_val_shards, 'val')\n",
    "\n",
    "print('Successfully wrote {} annotations to TF Records.'.format(\n",
    "    len(train_annotations) + len(val_annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84956df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfexample(example):\n",
    "    image_feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example, image_feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40d44777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0008_of_0008.tfrecords\n"
     ]
    }
   ],
   "source": [
    "def crop_roi(image, features, margin=0.2):\n",
    "    img_shape = tf.shape(image)\n",
    "    img_height = img_shape[0]\n",
    "    img_width = img_shape[1]\n",
    "    img_depth = img_shape[2]\n",
    "\n",
    "    keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "    keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "    center_x = features['image/object/center/x']\n",
    "    center_y = features['image/object/center/y']\n",
    "    body_height = features['image/object/scale'] * 200.0\n",
    "\n",
    "    # keypoint 중 유효한값(visible = 1) 만 사용합니다.\n",
    "    masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "    masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "\n",
    "    # min, max 값을 찾습니다.\n",
    "    keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "    keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "    keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "    keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "\n",
    "    # 높이 값을 이용해서 x, y 위치를 재조정 합니다. 박스를 정사각형으로 사용하기 위해 아래와 같이 사용합니다.\n",
    "    xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "\n",
    "    # 이미지 크기를 벗어나는 점을 재조정 해줍니다.\n",
    "    effective_xmin = xmin if xmin > 0 else 0\n",
    "    effective_ymin = ymin if ymin > 0 else 0\n",
    "    effective_xmax = xmax if xmax < img_width else img_width\n",
    "    effective_ymax = ymax if ymax < img_height else img_height\n",
    "    effective_height = effective_ymax - effective_ymin\n",
    "    effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "    image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "    new_shape = tf.shape(image)\n",
    "    new_height = new_shape[0]\n",
    "    new_width = new_shape[1]\n",
    "\n",
    "    effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "    effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "\n",
    "    return image, effective_keypoint_x, effective_keypoint_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77fbd83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_2d_guassian(height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "    heatmap = tf.zeros((height, width))\n",
    "\n",
    "    xmin = x0 - 3 * sigma\n",
    "    ymin = y0 - 3 * sigma\n",
    "    xmax = x0 + 3 * sigma\n",
    "    ymax = y0 + 3 * sigma\n",
    "    \n",
    "    if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n",
    "        return heatmap\n",
    "\n",
    "    size = 6 * sigma + 1\n",
    "    x, y = tf.meshgrid(tf.range(0, 6 * sigma + 1, 1), tf.range(0, 6 * sigma + 1, 1), indexing='xy')\n",
    "\n",
    "    center_x = size // 2\n",
    "    center_y = size // 2\n",
    "\n",
    "    gaussian_patch = tf.cast(tf.math.exp(\n",
    "        -(tf.math.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale,\n",
    "                             dtype=tf.float32)\n",
    "\n",
    "    patch_xmin = tf.math.maximum(0, -xmin)\n",
    "    patch_ymin = tf.math.maximum(0, -ymin)\n",
    "    patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "    patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "    heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "    heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "    heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "    heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "    indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "    updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for j in tf.range(patch_ymin, patch_ymax):\n",
    "        for i in tf.range(patch_xmin, patch_xmax):\n",
    "            indices = indices.write(count, [heatmap_ymin + j, heatmap_xmin + i])\n",
    "            updates = updates.write(count, gaussian_patch[j][i])\n",
    "            count += 1\n",
    "\n",
    "    heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "def make_heatmaps(features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "    v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "    x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "    y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "\n",
    "    num_heatmap = heatmap_shape[2]\n",
    "    heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "    for i in range(num_heatmap):\n",
    "        gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "        heatmap_array = heatmap_array.write(i, gaussian)\n",
    "\n",
    "    heatmaps = heatmap_array.stack()\n",
    "    heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0])  # change to (64, 64, 16)\n",
    "\n",
    "    return heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7771522",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y, self.heatmap_shape)\n",
    "\n",
    "        return image, heatmaps\n",
    "\n",
    "        \n",
    "    def crop_roi(self, image, features, margin=0.2):\n",
    "        img_shape = tf.shape(image)\n",
    "        img_height = img_shape[0]\n",
    "        img_width = img_shape[1]\n",
    "        img_depth = img_shape[2]\n",
    "\n",
    "        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "        center_x = features['image/object/center/x']\n",
    "        center_y = features['image/object/center/y']\n",
    "        body_height = features['image/object/scale'] * 200.0\n",
    "        \n",
    "        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "        \n",
    "        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "        \n",
    "        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        \n",
    "        effective_xmin = xmin if xmin > 0 else 0\n",
    "        effective_ymin = ymin if ymin > 0 else 0\n",
    "        effective_xmax = xmax if xmax < img_width else img_width\n",
    "        effective_ymax = ymax if ymax < img_height else img_height\n",
    "        effective_height = effective_ymax - effective_ymin\n",
    "        effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "        new_shape = tf.shape(image)\n",
    "        new_height = new_shape[0]\n",
    "        new_width = new_shape[1]\n",
    "        \n",
    "        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "        \n",
    "        return image, effective_keypoint_x, effective_keypoint_y\n",
    "        \n",
    "    \n",
    "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "        \n",
    "        heatmap = tf.zeros((height, width))\n",
    "\n",
    "        xmin = x0 - 3 * sigma\n",
    "        ymin = y0 - 3 * sigma\n",
    "        xmax = x0 + 3 * sigma\n",
    "        ymax = y0 + 3 * sigma\n",
    "\n",
    "        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
    "            return heatmap\n",
    "\n",
    "        size = 6 * sigma + 1\n",
    "        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "        center_x = size // 2\n",
    "        center_y = size // 2\n",
    "\n",
    "        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
    "\n",
    "        patch_xmin = tf.math.maximum(0, -xmin)\n",
    "        patch_ymin = tf.math.maximum(0, -ymin)\n",
    "        patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "        patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "        heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "        heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "        heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "        heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in tf.range(patch_ymin, patch_ymax):\n",
    "            for i in tf.range(patch_xmin, patch_xmax):\n",
    "                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
    "                updates = updates.write(count, gaussian_patch[j][i])\n",
    "                count += 1\n",
    "                \n",
    "        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "\n",
    "    def make_heatmaps(self, features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "        x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "        y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "        \n",
    "        num_heatmap = heatmap_shape[2]\n",
    "        heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "        for i in range(num_heatmap):\n",
    "            gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "            heatmap_array = heatmap_array.write(i, gaussian)\n",
    "        \n",
    "        heatmaps = heatmap_array.stack()\n",
    "        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
    "        \n",
    "        return heatmaps\n",
    "\n",
    "    def parse_tfexample(self, example):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example,\n",
    "                                          image_feature_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "491229c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n",
    "    identity = inputs\n",
    "    if downsample:\n",
    "        identity = Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=1,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=3,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = Add()([identity, x])\n",
    "    return x\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6b8405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourglassModule(inputs, order, filters, num_residual):\n",
    "    \n",
    "    up1 = BottleneckBlock(inputs, filters, downsample=False)\n",
    "    for i in range(num_residual):\n",
    "        up1 = BottleneckBlock(up1, filters, downsample=False)\n",
    "\n",
    "    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n",
    "    for i in range(num_residual):\n",
    "        low1 = BottleneckBlock(low1, filters, downsample=False)\n",
    "\n",
    "    low2 = low1\n",
    "    if order > 1:\n",
    "        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n",
    "    else:\n",
    "        for i in range(num_residual):\n",
    "            low2 = BottleneckBlock(low2, filters, downsample=False)\n",
    "\n",
    "    low3 = low2\n",
    "    for i in range(num_residual):\n",
    "        low3 = BottleneckBlock(low3, filters, downsample=False)\n",
    "\n",
    "    up2 = UpSampling2D(size=2)(low3)\n",
    "\n",
    "    return up2 + up1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2af867ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearLayer(inputs, filters):\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdf43fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StackedHourglassNetwork(\n",
    "        input_shape=(256, 256, 3), \n",
    "        num_stack=4, \n",
    "        num_residual=1,\n",
    "        num_heatmap=16):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=True)\n",
    "    x = MaxPool2D(pool_size=2, strides=2)(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=False)\n",
    "    x = BottleneckBlock(x, 256, downsample=True)\n",
    "\n",
    "    ys = []\n",
    "    for i in range(num_stack):\n",
    "        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n",
    "        for i in range(num_residual):\n",
    "            x = BottleneckBlock(x, 256, downsample=False)\n",
    "\n",
    "        x = LinearLayer(x, 256)\n",
    "\n",
    "        y = Conv2D(\n",
    "            filters=num_heatmap,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if i < num_stack - 1:\n",
    "            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n",
    "            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n",
    "            x = Add()([y_intermediate_1, y_intermediate_2])\n",
    "\n",
    "    return tf.keras.Model(inputs, ys, name='stacked_hourglass')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f5d235",
   "metadata": {},
   "source": [
    "#### STEP 2. 학습 엔진 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ba23d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.best_model = None\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = 0\n",
    "        for output in outputs:\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(\n",
    "                tf.math.square(labels - output) * weights) * (\n",
    "                    1. / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.lr_decay()\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = MODEL_PATH + '/model-epoch-{}-loss-{:.4f}.h5'.format(epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6626fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "\n",
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd540866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd48b76f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:374: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n",
      "  warnings.warn(\"To make it possible to preserve tf.data options across \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 2.39597583 epoch total loss 2.39597583\n",
      "Trained batch 2 batch loss 2.2968595 epoch total loss 2.34641767\n",
      "Trained batch 3 batch loss 2.41589451 epoch total loss 2.36957669\n",
      "Trained batch 4 batch loss 2.28198051 epoch total loss 2.34767771\n",
      "Trained batch 5 batch loss 2.28533292 epoch total loss 2.33520865\n",
      "Trained batch 6 batch loss 2.16702175 epoch total loss 2.30717754\n",
      "Trained batch 7 batch loss 2.09877253 epoch total loss 2.27740526\n",
      "Trained batch 8 batch loss 2.06588387 epoch total loss 2.25096512\n",
      "Trained batch 9 batch loss 2.11902857 epoch total loss 2.23630548\n",
      "Trained batch 10 batch loss 2.07591295 epoch total loss 2.2202661\n",
      "Trained batch 11 batch loss 2.04608083 epoch total loss 2.2044313\n",
      "Trained batch 12 batch loss 1.83469987 epoch total loss 2.17362022\n",
      "Trained batch 13 batch loss 1.88870442 epoch total loss 2.1517036\n",
      "Trained batch 14 batch loss 1.75231314 epoch total loss 2.12317586\n",
      "Trained batch 15 batch loss 1.91705918 epoch total loss 2.1094346\n",
      "Trained batch 16 batch loss 1.93352115 epoch total loss 2.09844\n",
      "Trained batch 17 batch loss 1.92285955 epoch total loss 2.08811164\n",
      "Trained batch 18 batch loss 1.95718038 epoch total loss 2.08083773\n",
      "Trained batch 19 batch loss 1.95066118 epoch total loss 2.07398629\n",
      "Trained batch 20 batch loss 1.96882606 epoch total loss 2.06872821\n",
      "Trained batch 21 batch loss 1.96281123 epoch total loss 2.06368446\n",
      "Trained batch 22 batch loss 1.94380832 epoch total loss 2.05823565\n",
      "Trained batch 23 batch loss 1.71793163 epoch total loss 2.04343987\n",
      "Trained batch 24 batch loss 1.70046687 epoch total loss 2.02914929\n",
      "Trained batch 25 batch loss 1.66022503 epoch total loss 2.01439214\n",
      "Trained batch 26 batch loss 1.66448653 epoch total loss 2.00093436\n",
      "Trained batch 27 batch loss 1.665048 epoch total loss 1.98849404\n",
      "Trained batch 28 batch loss 1.73451185 epoch total loss 1.97942328\n",
      "Trained batch 29 batch loss 1.79275179 epoch total loss 1.97298634\n",
      "Trained batch 30 batch loss 1.93646193 epoch total loss 1.97176886\n",
      "Trained batch 31 batch loss 1.89613235 epoch total loss 1.969329\n",
      "Trained batch 32 batch loss 1.8913765 epoch total loss 1.96689296\n",
      "Trained batch 33 batch loss 1.87260544 epoch total loss 1.96403575\n",
      "Trained batch 34 batch loss 1.87016225 epoch total loss 1.96127474\n",
      "Trained batch 35 batch loss 1.85398698 epoch total loss 1.9582094\n",
      "Trained batch 36 batch loss 1.89900315 epoch total loss 1.95656478\n",
      "Trained batch 37 batch loss 1.90795922 epoch total loss 1.9552511\n",
      "Trained batch 38 batch loss 1.88230479 epoch total loss 1.95333135\n",
      "Trained batch 39 batch loss 1.84425211 epoch total loss 1.95053458\n",
      "Trained batch 40 batch loss 1.76263571 epoch total loss 1.94583702\n",
      "Trained batch 41 batch loss 1.80354357 epoch total loss 1.94236648\n",
      "Trained batch 42 batch loss 1.60424113 epoch total loss 1.9343158\n",
      "Trained batch 43 batch loss 1.72285104 epoch total loss 1.92939806\n",
      "Trained batch 44 batch loss 1.80720317 epoch total loss 1.92662096\n",
      "Trained batch 45 batch loss 1.84382474 epoch total loss 1.92478108\n",
      "Trained batch 46 batch loss 1.82631993 epoch total loss 1.92264056\n",
      "Trained batch 47 batch loss 1.77322435 epoch total loss 1.91946149\n",
      "Trained batch 48 batch loss 1.73371387 epoch total loss 1.91559172\n",
      "Trained batch 49 batch loss 1.67856324 epoch total loss 1.91075444\n",
      "Trained batch 50 batch loss 1.7395 epoch total loss 1.90732944\n",
      "Trained batch 51 batch loss 1.76696301 epoch total loss 1.90457702\n",
      "Trained batch 52 batch loss 1.77331948 epoch total loss 1.902053\n",
      "Trained batch 53 batch loss 1.71588767 epoch total loss 1.89854038\n",
      "Trained batch 54 batch loss 1.72001755 epoch total loss 1.89523447\n",
      "Trained batch 55 batch loss 1.71104169 epoch total loss 1.89188552\n",
      "Trained batch 56 batch loss 1.73292851 epoch total loss 1.88904691\n",
      "Trained batch 57 batch loss 1.82232475 epoch total loss 1.88787639\n",
      "Trained batch 58 batch loss 1.79917395 epoch total loss 1.88634706\n",
      "Trained batch 59 batch loss 1.83442295 epoch total loss 1.88546693\n",
      "Trained batch 60 batch loss 1.79539073 epoch total loss 1.88396561\n",
      "Trained batch 61 batch loss 1.77909076 epoch total loss 1.88224626\n",
      "Trained batch 62 batch loss 1.77597499 epoch total loss 1.88053226\n",
      "Trained batch 63 batch loss 1.74549 epoch total loss 1.87838876\n",
      "Trained batch 64 batch loss 1.780388 epoch total loss 1.87685752\n",
      "Trained batch 65 batch loss 1.74572849 epoch total loss 1.87484014\n",
      "Trained batch 66 batch loss 1.72241497 epoch total loss 1.87253058\n",
      "Trained batch 67 batch loss 1.75406837 epoch total loss 1.87076247\n",
      "Trained batch 68 batch loss 1.71882379 epoch total loss 1.86852813\n",
      "Trained batch 69 batch loss 1.73842275 epoch total loss 1.86664259\n",
      "Trained batch 70 batch loss 1.74102128 epoch total loss 1.86484814\n",
      "Trained batch 71 batch loss 1.71526325 epoch total loss 1.86274123\n",
      "Trained batch 72 batch loss 1.65377092 epoch total loss 1.85983896\n",
      "Trained batch 73 batch loss 1.61603081 epoch total loss 1.85649908\n",
      "Trained batch 74 batch loss 1.62799716 epoch total loss 1.85341108\n",
      "Trained batch 75 batch loss 1.54564273 epoch total loss 1.84930742\n",
      "Trained batch 76 batch loss 1.7187624 epoch total loss 1.84758985\n",
      "Trained batch 77 batch loss 1.68548548 epoch total loss 1.8454845\n",
      "Trained batch 78 batch loss 1.7863431 epoch total loss 1.84472632\n",
      "Trained batch 79 batch loss 1.76009154 epoch total loss 1.84365499\n",
      "Trained batch 80 batch loss 1.72669065 epoch total loss 1.84219289\n",
      "Trained batch 81 batch loss 1.74150825 epoch total loss 1.84094977\n",
      "Trained batch 82 batch loss 1.74336982 epoch total loss 1.83975959\n",
      "Trained batch 83 batch loss 1.66131663 epoch total loss 1.83760977\n",
      "Trained batch 84 batch loss 1.62252593 epoch total loss 1.83504927\n",
      "Trained batch 85 batch loss 1.64634848 epoch total loss 1.83282924\n",
      "Trained batch 86 batch loss 1.69570029 epoch total loss 1.83123457\n",
      "Trained batch 87 batch loss 1.68310964 epoch total loss 1.82953191\n",
      "Trained batch 88 batch loss 1.7015866 epoch total loss 1.82807803\n",
      "Trained batch 89 batch loss 1.7321887 epoch total loss 1.82700062\n",
      "Trained batch 90 batch loss 1.7267766 epoch total loss 1.82588708\n",
      "Trained batch 91 batch loss 1.73936844 epoch total loss 1.82493627\n",
      "Trained batch 92 batch loss 1.75439346 epoch total loss 1.82416952\n",
      "Trained batch 93 batch loss 1.75441492 epoch total loss 1.82341933\n",
      "Trained batch 94 batch loss 1.71457481 epoch total loss 1.82226145\n",
      "Trained batch 95 batch loss 1.71776879 epoch total loss 1.82116151\n",
      "Trained batch 96 batch loss 1.71983528 epoch total loss 1.82010603\n",
      "Trained batch 97 batch loss 1.72429442 epoch total loss 1.81911826\n",
      "Trained batch 98 batch loss 1.70326781 epoch total loss 1.81793606\n",
      "Trained batch 99 batch loss 1.72311306 epoch total loss 1.81697822\n",
      "Trained batch 100 batch loss 1.67651367 epoch total loss 1.81557357\n",
      "Trained batch 101 batch loss 1.66602981 epoch total loss 1.81409299\n",
      "Trained batch 102 batch loss 1.50011802 epoch total loss 1.81101477\n",
      "Trained batch 103 batch loss 1.57703567 epoch total loss 1.80874324\n",
      "Trained batch 104 batch loss 1.72531939 epoch total loss 1.8079412\n",
      "Trained batch 105 batch loss 1.75869393 epoch total loss 1.80747211\n",
      "Trained batch 106 batch loss 1.70434773 epoch total loss 1.80649924\n",
      "Trained batch 107 batch loss 1.61908031 epoch total loss 1.8047477\n",
      "Trained batch 108 batch loss 1.56602466 epoch total loss 1.80253732\n",
      "Trained batch 109 batch loss 1.45635736 epoch total loss 1.79936135\n",
      "Trained batch 110 batch loss 1.44926262 epoch total loss 1.7961787\n",
      "Trained batch 111 batch loss 1.64830792 epoch total loss 1.79484653\n",
      "Trained batch 112 batch loss 1.65611124 epoch total loss 1.79360783\n",
      "Trained batch 113 batch loss 1.67663217 epoch total loss 1.79257274\n",
      "Trained batch 114 batch loss 1.74664974 epoch total loss 1.79216981\n",
      "Trained batch 115 batch loss 1.7626996 epoch total loss 1.79191351\n",
      "Trained batch 116 batch loss 1.78496313 epoch total loss 1.79185355\n",
      "Trained batch 117 batch loss 1.72210765 epoch total loss 1.79125738\n",
      "Trained batch 118 batch loss 1.72487426 epoch total loss 1.79069483\n",
      "Trained batch 119 batch loss 1.77567339 epoch total loss 1.79056859\n",
      "Trained batch 120 batch loss 1.7396009 epoch total loss 1.79014385\n",
      "Trained batch 121 batch loss 1.66181755 epoch total loss 1.78908336\n",
      "Trained batch 122 batch loss 1.73158956 epoch total loss 1.78861201\n",
      "Trained batch 123 batch loss 1.52245724 epoch total loss 1.78644812\n",
      "Trained batch 124 batch loss 1.58482361 epoch total loss 1.78482211\n",
      "Trained batch 125 batch loss 1.62700713 epoch total loss 1.78355968\n",
      "Trained batch 126 batch loss 1.63176954 epoch total loss 1.78235507\n",
      "Trained batch 127 batch loss 1.6190536 epoch total loss 1.78106916\n",
      "Trained batch 128 batch loss 1.64239788 epoch total loss 1.77998579\n",
      "Trained batch 129 batch loss 1.67624176 epoch total loss 1.7791816\n",
      "Trained batch 130 batch loss 1.54984307 epoch total loss 1.77741742\n",
      "Trained batch 131 batch loss 1.50115156 epoch total loss 1.77530849\n",
      "Trained batch 132 batch loss 1.67644918 epoch total loss 1.77455962\n",
      "Trained batch 133 batch loss 1.61873055 epoch total loss 1.77338791\n",
      "Trained batch 134 batch loss 1.73705637 epoch total loss 1.77311683\n",
      "Trained batch 135 batch loss 1.72934842 epoch total loss 1.7727927\n",
      "Trained batch 136 batch loss 1.65587068 epoch total loss 1.77193296\n",
      "Trained batch 137 batch loss 1.64230299 epoch total loss 1.7709868\n",
      "Trained batch 138 batch loss 1.62521017 epoch total loss 1.76993036\n",
      "Trained batch 139 batch loss 1.72864044 epoch total loss 1.76963329\n",
      "Trained batch 140 batch loss 1.66659391 epoch total loss 1.76889741\n",
      "Trained batch 141 batch loss 1.73168039 epoch total loss 1.76863337\n",
      "Trained batch 142 batch loss 1.67231631 epoch total loss 1.76795506\n",
      "Trained batch 143 batch loss 1.65428245 epoch total loss 1.76716018\n",
      "Trained batch 144 batch loss 1.48063898 epoch total loss 1.76517045\n",
      "Trained batch 145 batch loss 1.60617399 epoch total loss 1.76407385\n",
      "Trained batch 146 batch loss 1.42267585 epoch total loss 1.76173544\n",
      "Trained batch 147 batch loss 1.4374795 epoch total loss 1.75952959\n",
      "Trained batch 148 batch loss 1.62257719 epoch total loss 1.75860429\n",
      "Trained batch 149 batch loss 1.71728957 epoch total loss 1.75832701\n",
      "Trained batch 150 batch loss 1.7254262 epoch total loss 1.75810766\n",
      "Trained batch 151 batch loss 1.78135169 epoch total loss 1.75826156\n",
      "Trained batch 152 batch loss 1.6799711 epoch total loss 1.75774646\n",
      "Trained batch 153 batch loss 1.57628787 epoch total loss 1.75656044\n",
      "Trained batch 154 batch loss 1.71682191 epoch total loss 1.75630248\n",
      "Trained batch 155 batch loss 1.70695186 epoch total loss 1.75598395\n",
      "Trained batch 156 batch loss 1.70598912 epoch total loss 1.75566351\n",
      "Trained batch 157 batch loss 1.73734 epoch total loss 1.75554681\n",
      "Trained batch 158 batch loss 1.61670256 epoch total loss 1.754668\n",
      "Trained batch 159 batch loss 1.67423201 epoch total loss 1.75416207\n",
      "Trained batch 160 batch loss 1.71027613 epoch total loss 1.75388777\n",
      "Trained batch 161 batch loss 1.68063343 epoch total loss 1.75343275\n",
      "Trained batch 162 batch loss 1.71355963 epoch total loss 1.75318658\n",
      "Trained batch 163 batch loss 1.59209919 epoch total loss 1.75219834\n",
      "Trained batch 164 batch loss 1.54256988 epoch total loss 1.75092018\n",
      "Trained batch 165 batch loss 1.63489318 epoch total loss 1.75021696\n",
      "Trained batch 166 batch loss 1.71992922 epoch total loss 1.75003457\n",
      "Trained batch 167 batch loss 1.67241096 epoch total loss 1.74956977\n",
      "Trained batch 168 batch loss 1.5859772 epoch total loss 1.74859595\n",
      "Trained batch 169 batch loss 1.67926335 epoch total loss 1.74818575\n",
      "Trained batch 170 batch loss 1.72635901 epoch total loss 1.74805725\n",
      "Trained batch 171 batch loss 1.68559742 epoch total loss 1.74769211\n",
      "Trained batch 172 batch loss 1.63455582 epoch total loss 1.74703431\n",
      "Trained batch 173 batch loss 1.69410706 epoch total loss 1.74672842\n",
      "Trained batch 174 batch loss 1.6910454 epoch total loss 1.74640834\n",
      "Trained batch 175 batch loss 1.62902653 epoch total loss 1.74573767\n",
      "Trained batch 176 batch loss 1.61513281 epoch total loss 1.74499559\n",
      "Trained batch 177 batch loss 1.70741737 epoch total loss 1.7447834\n",
      "Trained batch 178 batch loss 1.522506 epoch total loss 1.74353456\n",
      "Trained batch 179 batch loss 1.52089238 epoch total loss 1.74229085\n",
      "Trained batch 180 batch loss 1.62903535 epoch total loss 1.74166155\n",
      "Trained batch 181 batch loss 1.67564237 epoch total loss 1.74129677\n",
      "Trained batch 182 batch loss 1.71539843 epoch total loss 1.74115443\n",
      "Trained batch 183 batch loss 1.61996114 epoch total loss 1.74049222\n",
      "Trained batch 184 batch loss 1.62264013 epoch total loss 1.73985171\n",
      "Trained batch 185 batch loss 1.55659056 epoch total loss 1.73886108\n",
      "Trained batch 186 batch loss 1.63282526 epoch total loss 1.73829091\n",
      "Trained batch 187 batch loss 1.54551637 epoch total loss 1.73726\n",
      "Trained batch 188 batch loss 1.59512448 epoch total loss 1.73650396\n",
      "Trained batch 189 batch loss 1.65515184 epoch total loss 1.73607349\n",
      "Trained batch 190 batch loss 1.65373456 epoch total loss 1.73564017\n",
      "Trained batch 191 batch loss 1.73704076 epoch total loss 1.73564744\n",
      "Trained batch 192 batch loss 1.7665242 epoch total loss 1.73580825\n",
      "Trained batch 193 batch loss 1.70050097 epoch total loss 1.73562527\n",
      "Trained batch 194 batch loss 1.69683051 epoch total loss 1.73542535\n",
      "Trained batch 195 batch loss 1.59256387 epoch total loss 1.73469269\n",
      "Trained batch 196 batch loss 1.65578628 epoch total loss 1.73429012\n",
      "Trained batch 197 batch loss 1.71228933 epoch total loss 1.73417842\n",
      "Trained batch 198 batch loss 1.73167133 epoch total loss 1.73416567\n",
      "Trained batch 199 batch loss 1.68550968 epoch total loss 1.73392117\n",
      "Trained batch 200 batch loss 1.76126909 epoch total loss 1.7340579\n",
      "Trained batch 201 batch loss 1.7270329 epoch total loss 1.73402286\n",
      "Trained batch 202 batch loss 1.62499237 epoch total loss 1.7334832\n",
      "Trained batch 203 batch loss 1.57694817 epoch total loss 1.73271203\n",
      "Trained batch 204 batch loss 1.52683711 epoch total loss 1.7317028\n",
      "Trained batch 205 batch loss 1.38341606 epoch total loss 1.73000383\n",
      "Trained batch 206 batch loss 1.51690781 epoch total loss 1.72896934\n",
      "Trained batch 207 batch loss 1.43247867 epoch total loss 1.72753704\n",
      "Trained batch 208 batch loss 1.34704363 epoch total loss 1.72570777\n",
      "Trained batch 209 batch loss 1.3636384 epoch total loss 1.72397542\n",
      "Trained batch 210 batch loss 1.44533587 epoch total loss 1.7226485\n",
      "Trained batch 211 batch loss 1.48147416 epoch total loss 1.72150552\n",
      "Trained batch 212 batch loss 1.59088147 epoch total loss 1.72088945\n",
      "Trained batch 213 batch loss 1.67644536 epoch total loss 1.72068083\n",
      "Trained batch 214 batch loss 1.69387436 epoch total loss 1.72055554\n",
      "Trained batch 215 batch loss 1.75470161 epoch total loss 1.72071433\n",
      "Trained batch 216 batch loss 1.7108233 epoch total loss 1.72066855\n",
      "Trained batch 217 batch loss 1.67009366 epoch total loss 1.7204355\n",
      "Trained batch 218 batch loss 1.71953726 epoch total loss 1.72043145\n",
      "Trained batch 219 batch loss 1.68313706 epoch total loss 1.7202611\n",
      "Trained batch 220 batch loss 1.6897366 epoch total loss 1.72012234\n",
      "Trained batch 221 batch loss 1.70568156 epoch total loss 1.72005701\n",
      "Trained batch 222 batch loss 1.69520593 epoch total loss 1.71994519\n",
      "Trained batch 223 batch loss 1.68220806 epoch total loss 1.71977592\n",
      "Trained batch 224 batch loss 1.71249795 epoch total loss 1.71974349\n",
      "Trained batch 225 batch loss 1.7244575 epoch total loss 1.71976435\n",
      "Trained batch 226 batch loss 1.70046616 epoch total loss 1.719679\n",
      "Trained batch 227 batch loss 1.65747619 epoch total loss 1.71940494\n",
      "Trained batch 228 batch loss 1.68513882 epoch total loss 1.71925473\n",
      "Trained batch 229 batch loss 1.62548494 epoch total loss 1.71884525\n",
      "Trained batch 230 batch loss 1.66350555 epoch total loss 1.71860468\n",
      "Trained batch 231 batch loss 1.64011908 epoch total loss 1.71826494\n",
      "Trained batch 232 batch loss 1.64930618 epoch total loss 1.71796763\n",
      "Trained batch 233 batch loss 1.58469164 epoch total loss 1.71739554\n",
      "Trained batch 234 batch loss 1.6121527 epoch total loss 1.71694577\n",
      "Trained batch 235 batch loss 1.71076417 epoch total loss 1.71691942\n",
      "Trained batch 236 batch loss 1.65376985 epoch total loss 1.71665192\n",
      "Trained batch 237 batch loss 1.61094 epoch total loss 1.71620584\n",
      "Trained batch 238 batch loss 1.60322714 epoch total loss 1.71573114\n",
      "Trained batch 239 batch loss 1.64423871 epoch total loss 1.71543205\n",
      "Trained batch 240 batch loss 1.657359 epoch total loss 1.71519\n",
      "Trained batch 241 batch loss 1.60294938 epoch total loss 1.71472418\n",
      "Trained batch 242 batch loss 1.54200506 epoch total loss 1.71401048\n",
      "Trained batch 243 batch loss 1.68382061 epoch total loss 1.71388614\n",
      "Trained batch 244 batch loss 1.62930429 epoch total loss 1.71353948\n",
      "Trained batch 245 batch loss 1.61247993 epoch total loss 1.71312702\n",
      "Trained batch 246 batch loss 1.59352016 epoch total loss 1.71264076\n",
      "Trained batch 247 batch loss 1.54899073 epoch total loss 1.7119782\n",
      "Trained batch 248 batch loss 1.52045202 epoch total loss 1.71120584\n",
      "Trained batch 249 batch loss 1.5851773 epoch total loss 1.71069968\n",
      "Trained batch 250 batch loss 1.64792752 epoch total loss 1.71044862\n",
      "Trained batch 251 batch loss 1.70888901 epoch total loss 1.71044242\n",
      "Trained batch 252 batch loss 1.73114288 epoch total loss 1.71052456\n",
      "Trained batch 253 batch loss 1.71271825 epoch total loss 1.71053314\n",
      "Trained batch 254 batch loss 1.71332026 epoch total loss 1.71054411\n",
      "Trained batch 255 batch loss 1.71528053 epoch total loss 1.71056271\n",
      "Trained batch 256 batch loss 1.62700915 epoch total loss 1.71023631\n",
      "Trained batch 257 batch loss 1.44210935 epoch total loss 1.70919299\n",
      "Trained batch 258 batch loss 1.31219172 epoch total loss 1.70765424\n",
      "Trained batch 259 batch loss 1.42969358 epoch total loss 1.706581\n",
      "Trained batch 260 batch loss 1.47191477 epoch total loss 1.70567846\n",
      "Trained batch 261 batch loss 1.56220651 epoch total loss 1.70512879\n",
      "Trained batch 262 batch loss 1.53434777 epoch total loss 1.70447695\n",
      "Trained batch 263 batch loss 1.56518245 epoch total loss 1.70394731\n",
      "Trained batch 264 batch loss 1.60162294 epoch total loss 1.70355976\n",
      "Trained batch 265 batch loss 1.68173766 epoch total loss 1.70347738\n",
      "Trained batch 266 batch loss 1.47130287 epoch total loss 1.70260465\n",
      "Trained batch 267 batch loss 1.60998607 epoch total loss 1.70225775\n",
      "Trained batch 268 batch loss 1.60472143 epoch total loss 1.70189381\n",
      "Trained batch 269 batch loss 1.55646753 epoch total loss 1.70135319\n",
      "Trained batch 270 batch loss 1.65687156 epoch total loss 1.70118833\n",
      "Trained batch 271 batch loss 1.68097782 epoch total loss 1.70111382\n",
      "Trained batch 272 batch loss 1.68952072 epoch total loss 1.70107114\n",
      "Trained batch 273 batch loss 1.74184513 epoch total loss 1.70122051\n",
      "Trained batch 274 batch loss 1.66879857 epoch total loss 1.70110214\n",
      "Trained batch 275 batch loss 1.66821778 epoch total loss 1.70098257\n",
      "Trained batch 276 batch loss 1.60729885 epoch total loss 1.70064318\n",
      "Trained batch 277 batch loss 1.61411047 epoch total loss 1.70033073\n",
      "Trained batch 278 batch loss 1.74458301 epoch total loss 1.70048988\n",
      "Trained batch 279 batch loss 1.7242285 epoch total loss 1.70057499\n",
      "Trained batch 280 batch loss 1.67042303 epoch total loss 1.70046723\n",
      "Trained batch 281 batch loss 1.67889738 epoch total loss 1.70039046\n",
      "Trained batch 282 batch loss 1.65879261 epoch total loss 1.70024288\n",
      "Trained batch 283 batch loss 1.46303177 epoch total loss 1.69940472\n",
      "Trained batch 284 batch loss 1.57068384 epoch total loss 1.69895148\n",
      "Trained batch 285 batch loss 1.6471709 epoch total loss 1.69876981\n",
      "Trained batch 286 batch loss 1.63660407 epoch total loss 1.69855237\n",
      "Trained batch 287 batch loss 1.67167854 epoch total loss 1.69845879\n",
      "Trained batch 288 batch loss 1.64105964 epoch total loss 1.69825947\n",
      "Trained batch 289 batch loss 1.62942982 epoch total loss 1.69802129\n",
      "Trained batch 290 batch loss 1.49910212 epoch total loss 1.69733536\n",
      "Trained batch 291 batch loss 1.55508387 epoch total loss 1.6968466\n",
      "Trained batch 292 batch loss 1.53717053 epoch total loss 1.69629967\n",
      "Trained batch 293 batch loss 1.62409258 epoch total loss 1.69605327\n",
      "Trained batch 294 batch loss 1.63441789 epoch total loss 1.6958437\n",
      "Trained batch 295 batch loss 1.67023218 epoch total loss 1.69575679\n",
      "Trained batch 296 batch loss 1.67576087 epoch total loss 1.6956892\n",
      "Trained batch 297 batch loss 1.69378114 epoch total loss 1.69568276\n",
      "Trained batch 298 batch loss 1.70215774 epoch total loss 1.69570446\n",
      "Trained batch 299 batch loss 1.70910215 epoch total loss 1.69574928\n",
      "Trained batch 300 batch loss 1.6591661 epoch total loss 1.69562745\n",
      "Trained batch 301 batch loss 1.59884858 epoch total loss 1.69530594\n",
      "Trained batch 302 batch loss 1.61101282 epoch total loss 1.69502687\n",
      "Trained batch 303 batch loss 1.62846327 epoch total loss 1.69480705\n",
      "Trained batch 304 batch loss 1.65322268 epoch total loss 1.6946702\n",
      "Trained batch 305 batch loss 1.73463154 epoch total loss 1.69480121\n",
      "Trained batch 306 batch loss 1.67729175 epoch total loss 1.69474399\n",
      "Trained batch 307 batch loss 1.6703546 epoch total loss 1.6946646\n",
      "Trained batch 308 batch loss 1.66626608 epoch total loss 1.69457233\n",
      "Trained batch 309 batch loss 1.59056067 epoch total loss 1.6942358\n",
      "Trained batch 310 batch loss 1.59471202 epoch total loss 1.69391477\n",
      "Trained batch 311 batch loss 1.5938102 epoch total loss 1.69359291\n",
      "Trained batch 312 batch loss 1.64271927 epoch total loss 1.69342983\n",
      "Trained batch 313 batch loss 1.5350883 epoch total loss 1.6929239\n",
      "Trained batch 314 batch loss 1.58004856 epoch total loss 1.69256461\n",
      "Trained batch 315 batch loss 1.64666867 epoch total loss 1.69241881\n",
      "Trained batch 316 batch loss 1.60650146 epoch total loss 1.69214702\n",
      "Trained batch 317 batch loss 1.54184341 epoch total loss 1.69167292\n",
      "Trained batch 318 batch loss 1.51729727 epoch total loss 1.69112444\n",
      "Trained batch 319 batch loss 1.54024339 epoch total loss 1.69065142\n",
      "Trained batch 320 batch loss 1.54647827 epoch total loss 1.69020081\n",
      "Trained batch 321 batch loss 1.57635403 epoch total loss 1.68984616\n",
      "Trained batch 322 batch loss 1.56523991 epoch total loss 1.6894592\n",
      "Trained batch 323 batch loss 1.51673925 epoch total loss 1.68892443\n",
      "Trained batch 324 batch loss 1.45557332 epoch total loss 1.68820417\n",
      "Trained batch 325 batch loss 1.52061677 epoch total loss 1.68768859\n",
      "Trained batch 326 batch loss 1.57184744 epoch total loss 1.68733323\n",
      "Trained batch 327 batch loss 1.46658635 epoch total loss 1.68665814\n",
      "Trained batch 328 batch loss 1.55091524 epoch total loss 1.68624437\n",
      "Trained batch 329 batch loss 1.4452641 epoch total loss 1.68551183\n",
      "Trained batch 330 batch loss 1.62823129 epoch total loss 1.68533826\n",
      "Trained batch 331 batch loss 1.55948126 epoch total loss 1.6849581\n",
      "Trained batch 332 batch loss 1.60604107 epoch total loss 1.68472028\n",
      "Trained batch 333 batch loss 1.54576707 epoch total loss 1.68430305\n",
      "Trained batch 334 batch loss 1.57304561 epoch total loss 1.68397\n",
      "Trained batch 335 batch loss 1.52785718 epoch total loss 1.68350399\n",
      "Trained batch 336 batch loss 1.59307146 epoch total loss 1.68323481\n",
      "Trained batch 337 batch loss 1.56677222 epoch total loss 1.68288922\n",
      "Trained batch 338 batch loss 1.53228939 epoch total loss 1.68244362\n",
      "Trained batch 339 batch loss 1.53834367 epoch total loss 1.68201852\n",
      "Trained batch 340 batch loss 1.57062185 epoch total loss 1.68169093\n",
      "Trained batch 341 batch loss 1.5291549 epoch total loss 1.68124366\n",
      "Trained batch 342 batch loss 1.60796571 epoch total loss 1.68102944\n",
      "Trained batch 343 batch loss 1.57915795 epoch total loss 1.68073237\n",
      "Trained batch 344 batch loss 1.60317206 epoch total loss 1.68050683\n",
      "Trained batch 345 batch loss 1.59893334 epoch total loss 1.68027043\n",
      "Trained batch 346 batch loss 1.58277988 epoch total loss 1.67998862\n",
      "Trained batch 347 batch loss 1.49792194 epoch total loss 1.67946398\n",
      "Trained batch 348 batch loss 1.59262431 epoch total loss 1.67921448\n",
      "Trained batch 349 batch loss 1.60347795 epoch total loss 1.6789974\n",
      "Trained batch 350 batch loss 1.50680804 epoch total loss 1.67850554\n",
      "Trained batch 351 batch loss 1.53291512 epoch total loss 1.67809069\n",
      "Trained batch 352 batch loss 1.5986712 epoch total loss 1.67786515\n",
      "Trained batch 353 batch loss 1.58848202 epoch total loss 1.67761195\n",
      "Trained batch 354 batch loss 1.46744072 epoch total loss 1.67701828\n",
      "Trained batch 355 batch loss 1.53037345 epoch total loss 1.67660534\n",
      "Trained batch 356 batch loss 1.59067166 epoch total loss 1.67636395\n",
      "Trained batch 357 batch loss 1.56106567 epoch total loss 1.67604113\n",
      "Trained batch 358 batch loss 1.51391971 epoch total loss 1.67558825\n",
      "Trained batch 359 batch loss 1.68149447 epoch total loss 1.67560482\n",
      "Trained batch 360 batch loss 1.55910063 epoch total loss 1.67528105\n",
      "Trained batch 361 batch loss 1.60606623 epoch total loss 1.67508936\n",
      "Trained batch 362 batch loss 1.63605213 epoch total loss 1.67498159\n",
      "Trained batch 363 batch loss 1.66088867 epoch total loss 1.67494273\n",
      "Trained batch 364 batch loss 1.60578704 epoch total loss 1.67475271\n",
      "Trained batch 365 batch loss 1.61751962 epoch total loss 1.67459583\n",
      "Trained batch 366 batch loss 1.57183075 epoch total loss 1.67431509\n",
      "Trained batch 367 batch loss 1.46939385 epoch total loss 1.67375684\n",
      "Trained batch 368 batch loss 1.47232866 epoch total loss 1.67320943\n",
      "Trained batch 369 batch loss 1.62694788 epoch total loss 1.67308414\n",
      "Trained batch 370 batch loss 1.58380735 epoch total loss 1.67284286\n",
      "Trained batch 371 batch loss 1.6355021 epoch total loss 1.67274213\n",
      "Trained batch 372 batch loss 1.47425139 epoch total loss 1.67220855\n",
      "Trained batch 373 batch loss 1.40482974 epoch total loss 1.67149174\n",
      "Trained batch 374 batch loss 1.52703226 epoch total loss 1.6711055\n",
      "Trained batch 375 batch loss 1.53845763 epoch total loss 1.67075181\n",
      "Trained batch 376 batch loss 1.3532474 epoch total loss 1.66990745\n",
      "Trained batch 377 batch loss 1.40941 epoch total loss 1.66921651\n",
      "Trained batch 378 batch loss 1.35266089 epoch total loss 1.66837907\n",
      "Trained batch 379 batch loss 1.44880342 epoch total loss 1.66779971\n",
      "Trained batch 380 batch loss 1.41133046 epoch total loss 1.66712475\n",
      "Trained batch 381 batch loss 1.391711 epoch total loss 1.66640186\n",
      "Trained batch 382 batch loss 1.51377368 epoch total loss 1.66600239\n",
      "Trained batch 383 batch loss 1.47601986 epoch total loss 1.66550636\n",
      "Trained batch 384 batch loss 1.54891312 epoch total loss 1.66520262\n",
      "Trained batch 385 batch loss 1.48544705 epoch total loss 1.66473579\n",
      "Trained batch 386 batch loss 1.42432904 epoch total loss 1.66411293\n",
      "Trained batch 387 batch loss 1.48422265 epoch total loss 1.66364813\n",
      "Trained batch 388 batch loss 1.63319683 epoch total loss 1.66356969\n",
      "Trained batch 389 batch loss 1.61109006 epoch total loss 1.66343474\n",
      "Trained batch 390 batch loss 1.69822264 epoch total loss 1.66352403\n",
      "Trained batch 391 batch loss 1.636289 epoch total loss 1.66345429\n",
      "Trained batch 392 batch loss 1.53586793 epoch total loss 1.66312885\n",
      "Trained batch 393 batch loss 1.51949501 epoch total loss 1.66276336\n",
      "Trained batch 394 batch loss 1.53443861 epoch total loss 1.66243768\n",
      "Trained batch 395 batch loss 1.37774611 epoch total loss 1.66171694\n",
      "Trained batch 396 batch loss 1.41001964 epoch total loss 1.66108131\n",
      "Trained batch 397 batch loss 1.44761992 epoch total loss 1.66054368\n",
      "Trained batch 398 batch loss 1.53193879 epoch total loss 1.6602205\n",
      "Trained batch 399 batch loss 1.614501 epoch total loss 1.66010594\n",
      "Trained batch 400 batch loss 1.62126446 epoch total loss 1.66000891\n",
      "Trained batch 401 batch loss 1.64615512 epoch total loss 1.65997434\n",
      "Trained batch 402 batch loss 1.59663129 epoch total loss 1.65981674\n",
      "Trained batch 403 batch loss 1.6023252 epoch total loss 1.65967405\n",
      "Trained batch 404 batch loss 1.53381515 epoch total loss 1.65936244\n",
      "Trained batch 405 batch loss 1.48514462 epoch total loss 1.65893233\n",
      "Trained batch 406 batch loss 1.42920065 epoch total loss 1.65836656\n",
      "Trained batch 407 batch loss 1.55207396 epoch total loss 1.65810537\n",
      "Trained batch 408 batch loss 1.57498193 epoch total loss 1.65790164\n",
      "Trained batch 409 batch loss 1.62229037 epoch total loss 1.65781462\n",
      "Trained batch 410 batch loss 1.57063472 epoch total loss 1.65760195\n",
      "Trained batch 411 batch loss 1.58380985 epoch total loss 1.65742242\n",
      "Trained batch 412 batch loss 1.58191299 epoch total loss 1.6572392\n",
      "Trained batch 413 batch loss 1.56169474 epoch total loss 1.65700781\n",
      "Trained batch 414 batch loss 1.43982089 epoch total loss 1.65648317\n",
      "Trained batch 415 batch loss 1.63151765 epoch total loss 1.65642309\n",
      "Trained batch 416 batch loss 1.42223835 epoch total loss 1.65586019\n",
      "Trained batch 417 batch loss 1.52048206 epoch total loss 1.65553558\n",
      "Trained batch 418 batch loss 1.54551291 epoch total loss 1.65527236\n",
      "Trained batch 419 batch loss 1.53307736 epoch total loss 1.65498078\n",
      "Trained batch 420 batch loss 1.58102167 epoch total loss 1.65480459\n",
      "Trained batch 421 batch loss 1.51939094 epoch total loss 1.65448308\n",
      "Trained batch 422 batch loss 1.41603947 epoch total loss 1.65391791\n",
      "Trained batch 423 batch loss 1.36247873 epoch total loss 1.653229\n",
      "Trained batch 424 batch loss 1.48713279 epoch total loss 1.65283716\n",
      "Trained batch 425 batch loss 1.48738241 epoch total loss 1.65244782\n",
      "Trained batch 426 batch loss 1.44757307 epoch total loss 1.65196693\n",
      "Trained batch 427 batch loss 1.50457716 epoch total loss 1.65162182\n",
      "Trained batch 428 batch loss 1.55333233 epoch total loss 1.6513921\n",
      "Trained batch 429 batch loss 1.65082431 epoch total loss 1.65139079\n",
      "Trained batch 430 batch loss 1.54240429 epoch total loss 1.65113735\n",
      "Trained batch 431 batch loss 1.47934842 epoch total loss 1.65073884\n",
      "Trained batch 432 batch loss 1.6920104 epoch total loss 1.65083444\n",
      "Trained batch 433 batch loss 1.74388576 epoch total loss 1.65104938\n",
      "Trained batch 434 batch loss 1.50105476 epoch total loss 1.65070367\n",
      "Trained batch 435 batch loss 1.61856639 epoch total loss 1.65062988\n",
      "Trained batch 436 batch loss 1.68386197 epoch total loss 1.65070605\n",
      "Trained batch 437 batch loss 1.75979936 epoch total loss 1.65095568\n",
      "Trained batch 438 batch loss 1.71696603 epoch total loss 1.65110648\n",
      "Trained batch 439 batch loss 1.67660403 epoch total loss 1.65116453\n",
      "Trained batch 440 batch loss 1.71368575 epoch total loss 1.65130651\n",
      "Trained batch 441 batch loss 1.56687438 epoch total loss 1.65111518\n",
      "Trained batch 442 batch loss 1.57487583 epoch total loss 1.65094268\n",
      "Trained batch 443 batch loss 1.54116154 epoch total loss 1.65069485\n",
      "Trained batch 444 batch loss 1.64611602 epoch total loss 1.65068448\n",
      "Trained batch 445 batch loss 1.55646968 epoch total loss 1.65047276\n",
      "Trained batch 446 batch loss 1.581038 epoch total loss 1.65031719\n",
      "Trained batch 447 batch loss 1.65128171 epoch total loss 1.65031934\n",
      "Trained batch 448 batch loss 1.55491805 epoch total loss 1.65010643\n",
      "Trained batch 449 batch loss 1.56598282 epoch total loss 1.64991903\n",
      "Trained batch 450 batch loss 1.58544397 epoch total loss 1.64977574\n",
      "Trained batch 451 batch loss 1.73658705 epoch total loss 1.64996827\n",
      "Trained batch 452 batch loss 1.7037158 epoch total loss 1.65008724\n",
      "Trained batch 453 batch loss 1.59309149 epoch total loss 1.64996135\n",
      "Trained batch 454 batch loss 1.6874764 epoch total loss 1.65004408\n",
      "Trained batch 455 batch loss 1.66832161 epoch total loss 1.65008426\n",
      "Trained batch 456 batch loss 1.64562368 epoch total loss 1.65007448\n",
      "Trained batch 457 batch loss 1.59984565 epoch total loss 1.64996457\n",
      "Trained batch 458 batch loss 1.60861731 epoch total loss 1.64987433\n",
      "Trained batch 459 batch loss 1.54616344 epoch total loss 1.64964831\n",
      "Trained batch 460 batch loss 1.57727575 epoch total loss 1.64949107\n",
      "Trained batch 461 batch loss 1.61073852 epoch total loss 1.64940691\n",
      "Trained batch 462 batch loss 1.58479166 epoch total loss 1.64926708\n",
      "Trained batch 463 batch loss 1.51197267 epoch total loss 1.64897048\n",
      "Trained batch 464 batch loss 1.58770585 epoch total loss 1.6488384\n",
      "Trained batch 465 batch loss 1.59462166 epoch total loss 1.64872181\n",
      "Trained batch 466 batch loss 1.57955015 epoch total loss 1.64857328\n",
      "Trained batch 467 batch loss 1.51825857 epoch total loss 1.64829421\n",
      "Trained batch 468 batch loss 1.53748453 epoch total loss 1.64805746\n",
      "Trained batch 469 batch loss 1.5691781 epoch total loss 1.64788926\n",
      "Trained batch 470 batch loss 1.64332414 epoch total loss 1.64787948\n",
      "Trained batch 471 batch loss 1.62939024 epoch total loss 1.64784026\n",
      "Trained batch 472 batch loss 1.64725947 epoch total loss 1.64783907\n",
      "Trained batch 473 batch loss 1.63433981 epoch total loss 1.64781046\n",
      "Trained batch 474 batch loss 1.56137681 epoch total loss 1.64762819\n",
      "Trained batch 475 batch loss 1.64655244 epoch total loss 1.64762592\n",
      "Trained batch 476 batch loss 1.56419802 epoch total loss 1.64745069\n",
      "Trained batch 477 batch loss 1.59262216 epoch total loss 1.64733577\n",
      "Trained batch 478 batch loss 1.52325249 epoch total loss 1.64707625\n",
      "Trained batch 479 batch loss 1.57148361 epoch total loss 1.64691842\n",
      "Trained batch 480 batch loss 1.56654334 epoch total loss 1.64675093\n",
      "Trained batch 481 batch loss 1.59235549 epoch total loss 1.6466378\n",
      "Trained batch 482 batch loss 1.62871599 epoch total loss 1.6466006\n",
      "Trained batch 483 batch loss 1.70228457 epoch total loss 1.64671588\n",
      "Trained batch 484 batch loss 1.70864427 epoch total loss 1.64684379\n",
      "Trained batch 485 batch loss 1.46963274 epoch total loss 1.6464783\n",
      "Trained batch 486 batch loss 1.48780584 epoch total loss 1.64615178\n",
      "Trained batch 487 batch loss 1.51411593 epoch total loss 1.6458807\n",
      "Trained batch 488 batch loss 1.58077121 epoch total loss 1.64574718\n",
      "Trained batch 489 batch loss 1.52343678 epoch total loss 1.64549708\n",
      "Trained batch 490 batch loss 1.54096651 epoch total loss 1.6452837\n",
      "Trained batch 491 batch loss 1.55607784 epoch total loss 1.64510202\n",
      "Trained batch 492 batch loss 1.57971406 epoch total loss 1.64496911\n",
      "Trained batch 493 batch loss 1.56930184 epoch total loss 1.64481568\n",
      "Trained batch 494 batch loss 1.52233112 epoch total loss 1.64456773\n",
      "Trained batch 495 batch loss 1.55251694 epoch total loss 1.64438164\n",
      "Trained batch 496 batch loss 1.61276317 epoch total loss 1.64431798\n",
      "Trained batch 497 batch loss 1.56532812 epoch total loss 1.64415908\n",
      "Trained batch 498 batch loss 1.54193926 epoch total loss 1.6439538\n",
      "Trained batch 499 batch loss 1.5545789 epoch total loss 1.64377463\n",
      "Trained batch 500 batch loss 1.60876095 epoch total loss 1.64370453\n",
      "Trained batch 501 batch loss 1.55880237 epoch total loss 1.64353502\n",
      "Trained batch 502 batch loss 1.54431045 epoch total loss 1.64333737\n",
      "Trained batch 503 batch loss 1.54090536 epoch total loss 1.64313376\n",
      "Trained batch 504 batch loss 1.5809232 epoch total loss 1.64301038\n",
      "Trained batch 505 batch loss 1.59401202 epoch total loss 1.64291322\n",
      "Trained batch 506 batch loss 1.64041603 epoch total loss 1.64290833\n",
      "Trained batch 507 batch loss 1.52709222 epoch total loss 1.64267993\n",
      "Trained batch 508 batch loss 1.5662607 epoch total loss 1.64252961\n",
      "Trained batch 509 batch loss 1.64209807 epoch total loss 1.64252877\n",
      "Trained batch 510 batch loss 1.65249348 epoch total loss 1.6425482\n",
      "Trained batch 511 batch loss 1.55564666 epoch total loss 1.64237821\n",
      "Trained batch 512 batch loss 1.54848671 epoch total loss 1.64219475\n",
      "Trained batch 513 batch loss 1.50842714 epoch total loss 1.64193404\n",
      "Trained batch 514 batch loss 1.62214327 epoch total loss 1.64189541\n",
      "Trained batch 515 batch loss 1.65193522 epoch total loss 1.64191496\n",
      "Trained batch 516 batch loss 1.64776659 epoch total loss 1.64192629\n",
      "Trained batch 517 batch loss 1.55022216 epoch total loss 1.64174891\n",
      "Trained batch 518 batch loss 1.55602336 epoch total loss 1.64158344\n",
      "Trained batch 519 batch loss 1.5469451 epoch total loss 1.64140105\n",
      "Trained batch 520 batch loss 1.63301754 epoch total loss 1.64138484\n",
      "Trained batch 521 batch loss 1.54278636 epoch total loss 1.64119565\n",
      "Trained batch 522 batch loss 1.54125857 epoch total loss 1.6410042\n",
      "Trained batch 523 batch loss 1.53061008 epoch total loss 1.6407932\n",
      "Trained batch 524 batch loss 1.53237867 epoch total loss 1.64058626\n",
      "Trained batch 525 batch loss 1.53885078 epoch total loss 1.64039254\n",
      "Trained batch 526 batch loss 1.58360672 epoch total loss 1.64028454\n",
      "Trained batch 527 batch loss 1.55615544 epoch total loss 1.64012492\n",
      "Trained batch 528 batch loss 1.54121947 epoch total loss 1.63993752\n",
      "Trained batch 529 batch loss 1.64215314 epoch total loss 1.63994169\n",
      "Trained batch 530 batch loss 1.54626858 epoch total loss 1.63976502\n",
      "Trained batch 531 batch loss 1.57442558 epoch total loss 1.63964188\n",
      "Trained batch 532 batch loss 1.51984608 epoch total loss 1.63941669\n",
      "Trained batch 533 batch loss 1.43180597 epoch total loss 1.63902724\n",
      "Trained batch 534 batch loss 1.43939865 epoch total loss 1.6386534\n",
      "Trained batch 535 batch loss 1.47843933 epoch total loss 1.63835394\n",
      "Trained batch 536 batch loss 1.56162393 epoch total loss 1.63821077\n",
      "Trained batch 537 batch loss 1.52579832 epoch total loss 1.63800156\n",
      "Trained batch 538 batch loss 1.522524 epoch total loss 1.63778687\n",
      "Trained batch 539 batch loss 1.60493076 epoch total loss 1.63772583\n",
      "Trained batch 540 batch loss 1.44985 epoch total loss 1.63737798\n",
      "Trained batch 541 batch loss 1.43962359 epoch total loss 1.63701236\n",
      "Trained batch 542 batch loss 1.54526234 epoch total loss 1.6368432\n",
      "Trained batch 543 batch loss 1.60853565 epoch total loss 1.63679099\n",
      "Trained batch 544 batch loss 1.54293573 epoch total loss 1.63661849\n",
      "Trained batch 545 batch loss 1.64129448 epoch total loss 1.63662708\n",
      "Trained batch 546 batch loss 1.60777712 epoch total loss 1.63657427\n",
      "Trained batch 547 batch loss 1.62606835 epoch total loss 1.63655508\n",
      "Trained batch 548 batch loss 1.64590466 epoch total loss 1.63657212\n",
      "Trained batch 549 batch loss 1.51432014 epoch total loss 1.63634956\n",
      "Trained batch 550 batch loss 1.48790956 epoch total loss 1.63607967\n",
      "Trained batch 551 batch loss 1.64902353 epoch total loss 1.63610315\n",
      "Trained batch 552 batch loss 1.6457777 epoch total loss 1.63612068\n",
      "Trained batch 553 batch loss 1.64475417 epoch total loss 1.63613629\n",
      "Trained batch 554 batch loss 1.60331154 epoch total loss 1.63607717\n",
      "Trained batch 555 batch loss 1.57284474 epoch total loss 1.63596308\n",
      "Trained batch 556 batch loss 1.61524928 epoch total loss 1.63592589\n",
      "Trained batch 557 batch loss 1.59877658 epoch total loss 1.63585913\n",
      "Trained batch 558 batch loss 1.55508864 epoch total loss 1.63571441\n",
      "Trained batch 559 batch loss 1.62815988 epoch total loss 1.63570094\n",
      "Trained batch 560 batch loss 1.63344908 epoch total loss 1.63569689\n",
      "Trained batch 561 batch loss 1.5961175 epoch total loss 1.63562632\n",
      "Trained batch 562 batch loss 1.47885466 epoch total loss 1.63534737\n",
      "Trained batch 563 batch loss 1.34128928 epoch total loss 1.63482511\n",
      "Trained batch 564 batch loss 1.38847446 epoch total loss 1.63438833\n",
      "Trained batch 565 batch loss 1.5066365 epoch total loss 1.63416231\n",
      "Trained batch 566 batch loss 1.57671046 epoch total loss 1.63406086\n",
      "Trained batch 567 batch loss 1.56155956 epoch total loss 1.63393295\n",
      "Trained batch 568 batch loss 1.54859829 epoch total loss 1.63378274\n",
      "Trained batch 569 batch loss 1.35438025 epoch total loss 1.63329172\n",
      "Trained batch 570 batch loss 1.42290974 epoch total loss 1.63292253\n",
      "Trained batch 571 batch loss 1.52432263 epoch total loss 1.63273239\n",
      "Trained batch 572 batch loss 1.61080897 epoch total loss 1.63269401\n",
      "Trained batch 573 batch loss 1.62031639 epoch total loss 1.63267243\n",
      "Trained batch 574 batch loss 1.65213466 epoch total loss 1.6327064\n",
      "Trained batch 575 batch loss 1.66942096 epoch total loss 1.6327703\n",
      "Trained batch 576 batch loss 1.63795829 epoch total loss 1.63277924\n",
      "Trained batch 577 batch loss 1.58726835 epoch total loss 1.63270032\n",
      "Trained batch 578 batch loss 1.6457057 epoch total loss 1.63272285\n",
      "Trained batch 579 batch loss 1.55669332 epoch total loss 1.63259161\n",
      "Trained batch 580 batch loss 1.52658176 epoch total loss 1.63240886\n",
      "Trained batch 581 batch loss 1.59189701 epoch total loss 1.63233912\n",
      "Trained batch 582 batch loss 1.619982 epoch total loss 1.6323179\n",
      "Trained batch 583 batch loss 1.61312234 epoch total loss 1.632285\n",
      "Trained batch 584 batch loss 1.52381694 epoch total loss 1.63209915\n",
      "Trained batch 585 batch loss 1.539029 epoch total loss 1.63194\n",
      "Trained batch 586 batch loss 1.57331061 epoch total loss 1.63184\n",
      "Trained batch 587 batch loss 1.53425086 epoch total loss 1.63167369\n",
      "Trained batch 588 batch loss 1.48923886 epoch total loss 1.63143158\n",
      "Trained batch 589 batch loss 1.50931823 epoch total loss 1.63122427\n",
      "Trained batch 590 batch loss 1.54436588 epoch total loss 1.63107705\n",
      "Trained batch 591 batch loss 1.53164971 epoch total loss 1.63090885\n",
      "Trained batch 592 batch loss 1.51106095 epoch total loss 1.63070643\n",
      "Trained batch 593 batch loss 1.51172149 epoch total loss 1.63050568\n",
      "Trained batch 594 batch loss 1.5879252 epoch total loss 1.63043404\n",
      "Trained batch 595 batch loss 1.62827659 epoch total loss 1.63043046\n",
      "Trained batch 596 batch loss 1.64270616 epoch total loss 1.63045108\n",
      "Trained batch 597 batch loss 1.57674634 epoch total loss 1.63036108\n",
      "Trained batch 598 batch loss 1.53976905 epoch total loss 1.63020968\n",
      "Trained batch 599 batch loss 1.54956341 epoch total loss 1.63007498\n",
      "Trained batch 600 batch loss 1.49898601 epoch total loss 1.62985647\n",
      "Trained batch 601 batch loss 1.56827807 epoch total loss 1.62975407\n",
      "Trained batch 602 batch loss 1.41201723 epoch total loss 1.62939227\n",
      "Trained batch 603 batch loss 1.46524131 epoch total loss 1.62912011\n",
      "Trained batch 604 batch loss 1.5089519 epoch total loss 1.62892115\n",
      "Trained batch 605 batch loss 1.54284739 epoch total loss 1.62877893\n",
      "Trained batch 606 batch loss 1.49587858 epoch total loss 1.62855959\n",
      "Trained batch 607 batch loss 1.52097607 epoch total loss 1.62838233\n",
      "Trained batch 608 batch loss 1.541605 epoch total loss 1.62823963\n",
      "Trained batch 609 batch loss 1.52408266 epoch total loss 1.62806869\n",
      "Trained batch 610 batch loss 1.39965248 epoch total loss 1.62769425\n",
      "Trained batch 611 batch loss 1.34946823 epoch total loss 1.62723887\n",
      "Trained batch 612 batch loss 1.33869576 epoch total loss 1.6267674\n",
      "Trained batch 613 batch loss 1.53245759 epoch total loss 1.62661362\n",
      "Trained batch 614 batch loss 1.50520802 epoch total loss 1.62641585\n",
      "Trained batch 615 batch loss 1.39168334 epoch total loss 1.62603414\n",
      "Trained batch 616 batch loss 1.53988934 epoch total loss 1.62589431\n",
      "Trained batch 617 batch loss 1.52257872 epoch total loss 1.62572694\n",
      "Trained batch 618 batch loss 1.42481661 epoch total loss 1.62540174\n",
      "Trained batch 619 batch loss 1.46960449 epoch total loss 1.62515008\n",
      "Trained batch 620 batch loss 1.50953794 epoch total loss 1.62496352\n",
      "Trained batch 621 batch loss 1.60456944 epoch total loss 1.62493074\n",
      "Trained batch 622 batch loss 1.40407014 epoch total loss 1.62457561\n",
      "Trained batch 623 batch loss 1.20231628 epoch total loss 1.62389779\n",
      "Trained batch 624 batch loss 1.20671582 epoch total loss 1.62322927\n",
      "Trained batch 625 batch loss 1.42212009 epoch total loss 1.62290752\n",
      "Trained batch 626 batch loss 1.73237467 epoch total loss 1.6230824\n",
      "Trained batch 627 batch loss 1.70742655 epoch total loss 1.62321687\n",
      "Trained batch 628 batch loss 1.62875581 epoch total loss 1.62322569\n",
      "Trained batch 629 batch loss 1.64456046 epoch total loss 1.62325954\n",
      "Trained batch 630 batch loss 1.62373686 epoch total loss 1.62326026\n",
      "Trained batch 631 batch loss 1.5915401 epoch total loss 1.62321\n",
      "Trained batch 632 batch loss 1.53895593 epoch total loss 1.62307668\n",
      "Trained batch 633 batch loss 1.59684682 epoch total loss 1.62303507\n",
      "Trained batch 634 batch loss 1.52917528 epoch total loss 1.62288702\n",
      "Trained batch 635 batch loss 1.60725987 epoch total loss 1.62286258\n",
      "Trained batch 636 batch loss 1.58957076 epoch total loss 1.62281024\n",
      "Trained batch 637 batch loss 1.60361028 epoch total loss 1.62278008\n",
      "Trained batch 638 batch loss 1.62758875 epoch total loss 1.62278759\n",
      "Trained batch 639 batch loss 1.58487773 epoch total loss 1.62272823\n",
      "Trained batch 640 batch loss 1.56979644 epoch total loss 1.62264562\n",
      "Trained batch 641 batch loss 1.58209181 epoch total loss 1.6225822\n",
      "Trained batch 642 batch loss 1.59196067 epoch total loss 1.62253439\n",
      "Trained batch 643 batch loss 1.63082242 epoch total loss 1.62254739\n",
      "Trained batch 644 batch loss 1.52146411 epoch total loss 1.62239051\n",
      "Trained batch 645 batch loss 1.49479365 epoch total loss 1.62219262\n",
      "Trained batch 646 batch loss 1.5434649 epoch total loss 1.62207067\n",
      "Trained batch 647 batch loss 1.54919672 epoch total loss 1.62195802\n",
      "Trained batch 648 batch loss 1.52150035 epoch total loss 1.62180305\n",
      "Trained batch 649 batch loss 1.46902061 epoch total loss 1.62156761\n",
      "Trained batch 650 batch loss 1.39637625 epoch total loss 1.62122107\n",
      "Trained batch 651 batch loss 1.59793222 epoch total loss 1.6211853\n",
      "Trained batch 652 batch loss 1.65282297 epoch total loss 1.62123382\n",
      "Trained batch 653 batch loss 1.60432744 epoch total loss 1.62120795\n",
      "Trained batch 654 batch loss 1.432616 epoch total loss 1.62091959\n",
      "Trained batch 655 batch loss 1.46999621 epoch total loss 1.62068915\n",
      "Trained batch 656 batch loss 1.51564789 epoch total loss 1.62052894\n",
      "Trained batch 657 batch loss 1.49332929 epoch total loss 1.62033534\n",
      "Trained batch 658 batch loss 1.42932904 epoch total loss 1.62004507\n",
      "Trained batch 659 batch loss 1.53830719 epoch total loss 1.61992097\n",
      "Trained batch 660 batch loss 1.5099175 epoch total loss 1.61975431\n",
      "Trained batch 661 batch loss 1.60738611 epoch total loss 1.6197356\n",
      "Trained batch 662 batch loss 1.65697181 epoch total loss 1.61979187\n",
      "Trained batch 663 batch loss 1.54879427 epoch total loss 1.61968493\n",
      "Trained batch 664 batch loss 1.57725739 epoch total loss 1.61962104\n",
      "Trained batch 665 batch loss 1.49599123 epoch total loss 1.61943507\n",
      "Trained batch 666 batch loss 1.5079639 epoch total loss 1.6192677\n",
      "Trained batch 667 batch loss 1.57670605 epoch total loss 1.61920381\n",
      "Trained batch 668 batch loss 1.53124702 epoch total loss 1.61907208\n",
      "Trained batch 669 batch loss 1.49792933 epoch total loss 1.618891\n",
      "Trained batch 670 batch loss 1.41796112 epoch total loss 1.61859107\n",
      "Trained batch 671 batch loss 1.4515996 epoch total loss 1.6183424\n",
      "Trained batch 672 batch loss 1.3839066 epoch total loss 1.61799347\n",
      "Trained batch 673 batch loss 1.61341953 epoch total loss 1.61798668\n",
      "Trained batch 674 batch loss 1.52753401 epoch total loss 1.61785257\n",
      "Trained batch 675 batch loss 1.56493616 epoch total loss 1.61777413\n",
      "Trained batch 676 batch loss 1.47606492 epoch total loss 1.61756456\n",
      "Trained batch 677 batch loss 1.49502754 epoch total loss 1.61738348\n",
      "Trained batch 678 batch loss 1.4110316 epoch total loss 1.61707914\n",
      "Trained batch 679 batch loss 1.46840858 epoch total loss 1.61686015\n",
      "Trained batch 680 batch loss 1.52603877 epoch total loss 1.61672652\n",
      "Trained batch 681 batch loss 1.50235593 epoch total loss 1.61655855\n",
      "Trained batch 682 batch loss 1.49756467 epoch total loss 1.61638403\n",
      "Trained batch 683 batch loss 1.53301311 epoch total loss 1.61626184\n",
      "Trained batch 684 batch loss 1.52158523 epoch total loss 1.61612344\n",
      "Trained batch 685 batch loss 1.45261741 epoch total loss 1.61588478\n",
      "Trained batch 686 batch loss 1.49836254 epoch total loss 1.6157136\n",
      "Trained batch 687 batch loss 1.57037044 epoch total loss 1.61564755\n",
      "Trained batch 688 batch loss 1.55226529 epoch total loss 1.61555529\n",
      "Trained batch 689 batch loss 1.5513382 epoch total loss 1.61546218\n",
      "Trained batch 690 batch loss 1.5565995 epoch total loss 1.61537695\n",
      "Trained batch 691 batch loss 1.5244596 epoch total loss 1.61524534\n",
      "Trained batch 692 batch loss 1.41147244 epoch total loss 1.6149509\n",
      "Trained batch 693 batch loss 1.45228 epoch total loss 1.61471617\n",
      "Trained batch 694 batch loss 1.70574903 epoch total loss 1.61484718\n",
      "Trained batch 695 batch loss 1.69065475 epoch total loss 1.61495638\n",
      "Trained batch 696 batch loss 1.60549915 epoch total loss 1.61494267\n",
      "Trained batch 697 batch loss 1.52026343 epoch total loss 1.61480689\n",
      "Trained batch 698 batch loss 1.56945968 epoch total loss 1.61474192\n",
      "Trained batch 699 batch loss 1.58203018 epoch total loss 1.61469507\n",
      "Trained batch 700 batch loss 1.58650708 epoch total loss 1.6146549\n",
      "Trained batch 701 batch loss 1.48147357 epoch total loss 1.61446488\n",
      "Trained batch 702 batch loss 1.48266077 epoch total loss 1.61427712\n",
      "Trained batch 703 batch loss 1.51681876 epoch total loss 1.61413848\n",
      "Trained batch 704 batch loss 1.55685472 epoch total loss 1.61405718\n",
      "Trained batch 705 batch loss 1.50204158 epoch total loss 1.6138984\n",
      "Trained batch 706 batch loss 1.61331129 epoch total loss 1.61389744\n",
      "Trained batch 707 batch loss 1.56703353 epoch total loss 1.61383116\n",
      "Trained batch 708 batch loss 1.59306896 epoch total loss 1.61380172\n",
      "Trained batch 709 batch loss 1.65532982 epoch total loss 1.61386025\n",
      "Trained batch 710 batch loss 1.50046647 epoch total loss 1.61370063\n",
      "Trained batch 711 batch loss 1.52971351 epoch total loss 1.61358237\n",
      "Trained batch 712 batch loss 1.53555226 epoch total loss 1.6134727\n",
      "Trained batch 713 batch loss 1.48488092 epoch total loss 1.61329234\n",
      "Trained batch 714 batch loss 1.42996788 epoch total loss 1.61303556\n",
      "Trained batch 715 batch loss 1.47494268 epoch total loss 1.61284244\n",
      "Trained batch 716 batch loss 1.60697365 epoch total loss 1.61283422\n",
      "Trained batch 717 batch loss 1.61953282 epoch total loss 1.61284351\n",
      "Trained batch 718 batch loss 1.53103483 epoch total loss 1.61272955\n",
      "Trained batch 719 batch loss 1.54172993 epoch total loss 1.61263084\n",
      "Trained batch 720 batch loss 1.56989324 epoch total loss 1.6125716\n",
      "Trained batch 721 batch loss 1.63170624 epoch total loss 1.61259806\n",
      "Trained batch 722 batch loss 1.5620662 epoch total loss 1.61252809\n",
      "Trained batch 723 batch loss 1.52085066 epoch total loss 1.61240125\n",
      "Trained batch 724 batch loss 1.47956324 epoch total loss 1.6122179\n",
      "Trained batch 725 batch loss 1.39039314 epoch total loss 1.61191189\n",
      "Trained batch 726 batch loss 1.36590707 epoch total loss 1.6115731\n",
      "Trained batch 727 batch loss 1.39335215 epoch total loss 1.61127293\n",
      "Trained batch 728 batch loss 1.38207698 epoch total loss 1.6109581\n",
      "Trained batch 729 batch loss 1.28164077 epoch total loss 1.6105063\n",
      "Trained batch 730 batch loss 1.29655445 epoch total loss 1.61007619\n",
      "Trained batch 731 batch loss 1.23953629 epoch total loss 1.60956919\n",
      "Trained batch 732 batch loss 1.24520481 epoch total loss 1.60907149\n",
      "Trained batch 733 batch loss 1.56884527 epoch total loss 1.60901666\n",
      "Trained batch 734 batch loss 1.55330122 epoch total loss 1.60894072\n",
      "Trained batch 735 batch loss 1.47773409 epoch total loss 1.60876226\n",
      "Trained batch 736 batch loss 1.48898959 epoch total loss 1.60859966\n",
      "Trained batch 737 batch loss 1.51577628 epoch total loss 1.60847366\n",
      "Trained batch 738 batch loss 1.48690963 epoch total loss 1.60830891\n",
      "Trained batch 739 batch loss 1.57338333 epoch total loss 1.6082617\n",
      "Trained batch 740 batch loss 1.5852313 epoch total loss 1.60823047\n",
      "Trained batch 741 batch loss 1.59862161 epoch total loss 1.6082176\n",
      "Trained batch 742 batch loss 1.48326135 epoch total loss 1.60804915\n",
      "Trained batch 743 batch loss 1.49342799 epoch total loss 1.6078949\n",
      "Trained batch 744 batch loss 1.4916532 epoch total loss 1.60773873\n",
      "Trained batch 745 batch loss 1.48462105 epoch total loss 1.60757339\n",
      "Trained batch 746 batch loss 1.59759021 epoch total loss 1.60756\n",
      "Trained batch 747 batch loss 1.46483374 epoch total loss 1.60736895\n",
      "Trained batch 748 batch loss 1.46385622 epoch total loss 1.60717714\n",
      "Trained batch 749 batch loss 1.48557425 epoch total loss 1.60701478\n",
      "Trained batch 750 batch loss 1.45422626 epoch total loss 1.60681105\n",
      "Trained batch 751 batch loss 1.47052729 epoch total loss 1.60662961\n",
      "Trained batch 752 batch loss 1.44892931 epoch total loss 1.60642\n",
      "Trained batch 753 batch loss 1.45207715 epoch total loss 1.606215\n",
      "Trained batch 754 batch loss 1.48352718 epoch total loss 1.60605228\n",
      "Trained batch 755 batch loss 1.40177941 epoch total loss 1.60578156\n",
      "Trained batch 756 batch loss 1.47541428 epoch total loss 1.60560918\n",
      "Trained batch 757 batch loss 1.4570092 epoch total loss 1.60541296\n",
      "Trained batch 758 batch loss 1.53591621 epoch total loss 1.60532129\n",
      "Trained batch 759 batch loss 1.50277829 epoch total loss 1.60518622\n",
      "Trained batch 760 batch loss 1.35297072 epoch total loss 1.60485435\n",
      "Trained batch 761 batch loss 1.50692534 epoch total loss 1.60472572\n",
      "Trained batch 762 batch loss 1.66555822 epoch total loss 1.60480559\n",
      "Trained batch 763 batch loss 1.47214699 epoch total loss 1.60463166\n",
      "Trained batch 764 batch loss 1.42153251 epoch total loss 1.60439205\n",
      "Trained batch 765 batch loss 1.50102079 epoch total loss 1.60425687\n",
      "Trained batch 766 batch loss 1.50021815 epoch total loss 1.60412109\n",
      "Trained batch 767 batch loss 1.52133274 epoch total loss 1.60401309\n",
      "Trained batch 768 batch loss 1.60863948 epoch total loss 1.60401917\n",
      "Trained batch 769 batch loss 1.6165204 epoch total loss 1.6040355\n",
      "Trained batch 770 batch loss 1.5710237 epoch total loss 1.6039927\n",
      "Trained batch 771 batch loss 1.48617792 epoch total loss 1.60383987\n",
      "Trained batch 772 batch loss 1.52106333 epoch total loss 1.60373271\n",
      "Trained batch 773 batch loss 1.44484735 epoch total loss 1.60352719\n",
      "Trained batch 774 batch loss 1.49189043 epoch total loss 1.60338295\n",
      "Trained batch 775 batch loss 1.58273101 epoch total loss 1.60335636\n",
      "Trained batch 776 batch loss 1.59772801 epoch total loss 1.60334921\n",
      "Trained batch 777 batch loss 1.5411694 epoch total loss 1.6032691\n",
      "Trained batch 778 batch loss 1.47906709 epoch total loss 1.6031096\n",
      "Trained batch 779 batch loss 1.417799 epoch total loss 1.60287178\n",
      "Trained batch 780 batch loss 1.56636238 epoch total loss 1.60282505\n",
      "Trained batch 781 batch loss 1.46293807 epoch total loss 1.60264587\n",
      "Trained batch 782 batch loss 1.53901386 epoch total loss 1.60256445\n",
      "Trained batch 783 batch loss 1.49913883 epoch total loss 1.60243237\n",
      "Trained batch 784 batch loss 1.47527277 epoch total loss 1.60227013\n",
      "Trained batch 785 batch loss 1.5379889 epoch total loss 1.60218823\n",
      "Trained batch 786 batch loss 1.55146289 epoch total loss 1.60212374\n",
      "Trained batch 787 batch loss 1.59865975 epoch total loss 1.60211933\n",
      "Trained batch 788 batch loss 1.55013096 epoch total loss 1.6020534\n",
      "Trained batch 789 batch loss 1.4340663 epoch total loss 1.6018405\n",
      "Trained batch 790 batch loss 1.4660213 epoch total loss 1.6016686\n",
      "Trained batch 791 batch loss 1.51658797 epoch total loss 1.60156107\n",
      "Trained batch 792 batch loss 1.52279 epoch total loss 1.60146165\n",
      "Trained batch 793 batch loss 1.48737133 epoch total loss 1.60131788\n",
      "Trained batch 794 batch loss 1.60461867 epoch total loss 1.60132205\n",
      "Trained batch 795 batch loss 1.47394049 epoch total loss 1.60116184\n",
      "Trained batch 796 batch loss 1.56694627 epoch total loss 1.6011188\n",
      "Trained batch 797 batch loss 1.56277943 epoch total loss 1.60107064\n",
      "Trained batch 798 batch loss 1.38836813 epoch total loss 1.60080421\n",
      "Trained batch 799 batch loss 1.4666276 epoch total loss 1.60063636\n",
      "Trained batch 800 batch loss 1.51688015 epoch total loss 1.60053158\n",
      "Trained batch 801 batch loss 1.55538499 epoch total loss 1.60047531\n",
      "Trained batch 802 batch loss 1.57561493 epoch total loss 1.6004442\n",
      "Trained batch 803 batch loss 1.56287885 epoch total loss 1.60039747\n",
      "Trained batch 804 batch loss 1.54769349 epoch total loss 1.6003319\n",
      "Trained batch 805 batch loss 1.51746511 epoch total loss 1.60022902\n",
      "Trained batch 806 batch loss 1.50137782 epoch total loss 1.60010624\n",
      "Trained batch 807 batch loss 1.50142431 epoch total loss 1.59998405\n",
      "Trained batch 808 batch loss 1.3967495 epoch total loss 1.59973252\n",
      "Trained batch 809 batch loss 1.41421986 epoch total loss 1.59950316\n",
      "Trained batch 810 batch loss 1.38213587 epoch total loss 1.5992347\n",
      "Trained batch 811 batch loss 1.40868032 epoch total loss 1.59899974\n",
      "Trained batch 812 batch loss 1.45481527 epoch total loss 1.59882224\n",
      "Trained batch 813 batch loss 1.46470582 epoch total loss 1.59865725\n",
      "Trained batch 814 batch loss 1.4610728 epoch total loss 1.59848821\n",
      "Trained batch 815 batch loss 1.51203656 epoch total loss 1.59838223\n",
      "Trained batch 816 batch loss 1.57230258 epoch total loss 1.59835017\n",
      "Trained batch 817 batch loss 1.56835616 epoch total loss 1.59831357\n",
      "Trained batch 818 batch loss 1.34040284 epoch total loss 1.59799826\n",
      "Trained batch 819 batch loss 1.46176744 epoch total loss 1.59783196\n",
      "Trained batch 820 batch loss 1.51226354 epoch total loss 1.59772754\n",
      "Trained batch 821 batch loss 1.54189563 epoch total loss 1.59765947\n",
      "Trained batch 822 batch loss 1.56849027 epoch total loss 1.59762406\n",
      "Trained batch 823 batch loss 1.49248528 epoch total loss 1.59749615\n",
      "Trained batch 824 batch loss 1.67314959 epoch total loss 1.59758794\n",
      "Trained batch 825 batch loss 1.57048368 epoch total loss 1.59755504\n",
      "Trained batch 826 batch loss 1.5557822 epoch total loss 1.5975045\n",
      "Trained batch 827 batch loss 1.49971259 epoch total loss 1.59738624\n",
      "Trained batch 828 batch loss 1.56892037 epoch total loss 1.59735191\n",
      "Trained batch 829 batch loss 1.55863357 epoch total loss 1.59730518\n",
      "Trained batch 830 batch loss 1.48221314 epoch total loss 1.59716654\n",
      "Trained batch 831 batch loss 1.48715234 epoch total loss 1.5970341\n",
      "Trained batch 832 batch loss 1.49576473 epoch total loss 1.59691238\n",
      "Trained batch 833 batch loss 1.45311904 epoch total loss 1.59673977\n",
      "Trained batch 834 batch loss 1.56952488 epoch total loss 1.59670722\n",
      "Trained batch 835 batch loss 1.52415013 epoch total loss 1.59662032\n",
      "Trained batch 836 batch loss 1.6025089 epoch total loss 1.59662735\n",
      "Trained batch 837 batch loss 1.58948481 epoch total loss 1.59661889\n",
      "Trained batch 838 batch loss 1.57387042 epoch total loss 1.59659171\n",
      "Trained batch 839 batch loss 1.55458736 epoch total loss 1.59654164\n",
      "Trained batch 840 batch loss 1.63605261 epoch total loss 1.59658873\n",
      "Trained batch 841 batch loss 1.50759339 epoch total loss 1.59648287\n",
      "Trained batch 842 batch loss 1.52866745 epoch total loss 1.59640241\n",
      "Trained batch 843 batch loss 1.54594314 epoch total loss 1.59634244\n",
      "Trained batch 844 batch loss 1.5715214 epoch total loss 1.596313\n",
      "Trained batch 845 batch loss 1.45130551 epoch total loss 1.59614146\n",
      "Trained batch 846 batch loss 1.52141225 epoch total loss 1.596053\n",
      "Trained batch 847 batch loss 1.45593667 epoch total loss 1.59588754\n",
      "Trained batch 848 batch loss 1.43896 epoch total loss 1.59570253\n",
      "Trained batch 849 batch loss 1.52968526 epoch total loss 1.5956248\n",
      "Trained batch 850 batch loss 1.51065993 epoch total loss 1.59552479\n",
      "Trained batch 851 batch loss 1.44356883 epoch total loss 1.59534621\n",
      "Trained batch 852 batch loss 1.32559657 epoch total loss 1.59502959\n",
      "Trained batch 853 batch loss 1.34086835 epoch total loss 1.59473157\n",
      "Trained batch 854 batch loss 1.35684752 epoch total loss 1.59445298\n",
      "Trained batch 855 batch loss 1.46279061 epoch total loss 1.59429896\n",
      "Trained batch 856 batch loss 1.5164783 epoch total loss 1.594208\n",
      "Trained batch 857 batch loss 1.6060617 epoch total loss 1.59422195\n",
      "Trained batch 858 batch loss 1.42521429 epoch total loss 1.5940249\n",
      "Trained batch 859 batch loss 1.39037037 epoch total loss 1.59378779\n",
      "Trained batch 860 batch loss 1.46074128 epoch total loss 1.59363306\n",
      "Trained batch 861 batch loss 1.56805384 epoch total loss 1.59360325\n",
      "Trained batch 862 batch loss 1.43686354 epoch total loss 1.59342146\n",
      "Trained batch 863 batch loss 1.53263569 epoch total loss 1.59335101\n",
      "Trained batch 864 batch loss 1.41697454 epoch total loss 1.5931468\n",
      "Trained batch 865 batch loss 1.44356871 epoch total loss 1.59297395\n",
      "Trained batch 866 batch loss 1.3694942 epoch total loss 1.59271598\n",
      "Trained batch 867 batch loss 1.4404943 epoch total loss 1.59254038\n",
      "Trained batch 868 batch loss 1.45507884 epoch total loss 1.59238207\n",
      "Trained batch 869 batch loss 1.42323637 epoch total loss 1.5921874\n",
      "Trained batch 870 batch loss 1.51141953 epoch total loss 1.59209466\n",
      "Trained batch 871 batch loss 1.44176602 epoch total loss 1.59192204\n",
      "Trained batch 872 batch loss 1.44278574 epoch total loss 1.59175098\n",
      "Trained batch 873 batch loss 1.51381302 epoch total loss 1.59166169\n",
      "Trained batch 874 batch loss 1.55108988 epoch total loss 1.59161532\n",
      "Trained batch 875 batch loss 1.56407511 epoch total loss 1.59158385\n",
      "Trained batch 876 batch loss 1.55040956 epoch total loss 1.59153688\n",
      "Trained batch 877 batch loss 1.48839879 epoch total loss 1.59141922\n",
      "Trained batch 878 batch loss 1.50059497 epoch total loss 1.59131587\n",
      "Trained batch 879 batch loss 1.45505357 epoch total loss 1.59116077\n",
      "Trained batch 880 batch loss 1.35064876 epoch total loss 1.59088755\n",
      "Trained batch 881 batch loss 1.39092159 epoch total loss 1.59066057\n",
      "Trained batch 882 batch loss 1.35705781 epoch total loss 1.59039569\n",
      "Trained batch 883 batch loss 1.57041454 epoch total loss 1.59037304\n",
      "Trained batch 884 batch loss 1.43796599 epoch total loss 1.59020066\n",
      "Trained batch 885 batch loss 1.44090295 epoch total loss 1.59003198\n",
      "Trained batch 886 batch loss 1.40886807 epoch total loss 1.58982754\n",
      "Trained batch 887 batch loss 1.4752425 epoch total loss 1.58969831\n",
      "Trained batch 888 batch loss 1.31505561 epoch total loss 1.58938897\n",
      "Trained batch 889 batch loss 1.35306025 epoch total loss 1.58912313\n",
      "Trained batch 890 batch loss 1.50078225 epoch total loss 1.58902383\n",
      "Trained batch 891 batch loss 1.45657563 epoch total loss 1.58887517\n",
      "Trained batch 892 batch loss 1.52719152 epoch total loss 1.58880603\n",
      "Trained batch 893 batch loss 1.3973093 epoch total loss 1.58859158\n",
      "Trained batch 894 batch loss 1.59710264 epoch total loss 1.58860111\n",
      "Trained batch 895 batch loss 1.65311849 epoch total loss 1.58867311\n",
      "Trained batch 896 batch loss 1.69988263 epoch total loss 1.58879721\n",
      "Trained batch 897 batch loss 1.71507704 epoch total loss 1.588938\n",
      "Trained batch 898 batch loss 1.56314075 epoch total loss 1.58890915\n",
      "Trained batch 899 batch loss 1.53439593 epoch total loss 1.58884859\n",
      "Trained batch 900 batch loss 1.62871337 epoch total loss 1.58889282\n",
      "Trained batch 901 batch loss 1.55879736 epoch total loss 1.58885944\n",
      "Trained batch 902 batch loss 1.48842931 epoch total loss 1.5887481\n",
      "Trained batch 903 batch loss 1.54591799 epoch total loss 1.58870065\n",
      "Trained batch 904 batch loss 1.46711063 epoch total loss 1.58856618\n",
      "Trained batch 905 batch loss 1.61120844 epoch total loss 1.58859122\n",
      "Trained batch 906 batch loss 1.64137912 epoch total loss 1.58864951\n",
      "Trained batch 907 batch loss 1.54302025 epoch total loss 1.58859909\n",
      "Trained batch 908 batch loss 1.46803033 epoch total loss 1.58846629\n",
      "Trained batch 909 batch loss 1.47964454 epoch total loss 1.5883466\n",
      "Trained batch 910 batch loss 1.50174308 epoch total loss 1.58825135\n",
      "Trained batch 911 batch loss 1.37259483 epoch total loss 1.5880146\n",
      "Trained batch 912 batch loss 1.31224048 epoch total loss 1.58771217\n",
      "Trained batch 913 batch loss 1.341151 epoch total loss 1.58744216\n",
      "Trained batch 914 batch loss 1.51483727 epoch total loss 1.58736277\n",
      "Trained batch 915 batch loss 1.59244561 epoch total loss 1.58736837\n",
      "Trained batch 916 batch loss 1.54127979 epoch total loss 1.58731794\n",
      "Trained batch 917 batch loss 1.49737561 epoch total loss 1.58722\n",
      "Trained batch 918 batch loss 1.39165711 epoch total loss 1.58700693\n",
      "Trained batch 919 batch loss 1.42106235 epoch total loss 1.58682621\n",
      "Trained batch 920 batch loss 1.50710225 epoch total loss 1.58673954\n",
      "Trained batch 921 batch loss 1.48894894 epoch total loss 1.58663332\n",
      "Trained batch 922 batch loss 1.49340475 epoch total loss 1.58653224\n",
      "Trained batch 923 batch loss 1.46224308 epoch total loss 1.58639765\n",
      "Trained batch 924 batch loss 1.38039088 epoch total loss 1.58617461\n",
      "Trained batch 925 batch loss 1.46257794 epoch total loss 1.58604097\n",
      "Trained batch 926 batch loss 1.49995971 epoch total loss 1.58594799\n",
      "Trained batch 927 batch loss 1.58819032 epoch total loss 1.58595037\n",
      "Trained batch 928 batch loss 1.52344108 epoch total loss 1.58588302\n",
      "Trained batch 929 batch loss 1.42722797 epoch total loss 1.58571231\n",
      "Trained batch 930 batch loss 1.40427279 epoch total loss 1.58551717\n",
      "Trained batch 931 batch loss 1.36356521 epoch total loss 1.58527875\n",
      "Trained batch 932 batch loss 1.44905305 epoch total loss 1.5851326\n",
      "Trained batch 933 batch loss 1.50920212 epoch total loss 1.58505118\n",
      "Trained batch 934 batch loss 1.57787 epoch total loss 1.58504355\n",
      "Trained batch 935 batch loss 1.36263299 epoch total loss 1.58480573\n",
      "Trained batch 936 batch loss 1.40133536 epoch total loss 1.58460975\n",
      "Trained batch 937 batch loss 1.39460349 epoch total loss 1.58440697\n",
      "Trained batch 938 batch loss 1.43990731 epoch total loss 1.58425295\n",
      "Trained batch 939 batch loss 1.47473288 epoch total loss 1.58413637\n",
      "Trained batch 940 batch loss 1.43692577 epoch total loss 1.58397973\n",
      "Trained batch 941 batch loss 1.46985817 epoch total loss 1.58385837\n",
      "Trained batch 942 batch loss 1.4062016 epoch total loss 1.5836699\n",
      "Trained batch 943 batch loss 1.49927676 epoch total loss 1.58358037\n",
      "Trained batch 944 batch loss 1.49905145 epoch total loss 1.58349085\n",
      "Trained batch 945 batch loss 1.53420401 epoch total loss 1.58343863\n",
      "Trained batch 946 batch loss 1.40265405 epoch total loss 1.58324754\n",
      "Trained batch 947 batch loss 1.36288548 epoch total loss 1.58301485\n",
      "Trained batch 948 batch loss 1.42730772 epoch total loss 1.58285069\n",
      "Trained batch 949 batch loss 1.4644537 epoch total loss 1.582726\n",
      "Trained batch 950 batch loss 1.57863498 epoch total loss 1.58272171\n",
      "Trained batch 951 batch loss 1.57756543 epoch total loss 1.58271623\n",
      "Trained batch 952 batch loss 1.54097855 epoch total loss 1.58267236\n",
      "Trained batch 953 batch loss 1.4392004 epoch total loss 1.5825218\n",
      "Trained batch 954 batch loss 1.35142744 epoch total loss 1.58227956\n",
      "Trained batch 955 batch loss 1.19520211 epoch total loss 1.58187425\n",
      "Trained batch 956 batch loss 1.34303415 epoch total loss 1.58162439\n",
      "Trained batch 957 batch loss 1.33781648 epoch total loss 1.58136964\n",
      "Trained batch 958 batch loss 1.33440256 epoch total loss 1.58111179\n",
      "Trained batch 959 batch loss 1.48714972 epoch total loss 1.5810138\n",
      "Trained batch 960 batch loss 1.41820526 epoch total loss 1.58084428\n",
      "Trained batch 961 batch loss 1.44502795 epoch total loss 1.5807029\n",
      "Trained batch 962 batch loss 1.52091122 epoch total loss 1.58064079\n",
      "Trained batch 963 batch loss 1.46555686 epoch total loss 1.58052123\n",
      "Trained batch 964 batch loss 1.43878949 epoch total loss 1.58037436\n",
      "Trained batch 965 batch loss 1.46035421 epoch total loss 1.58024991\n",
      "Trained batch 966 batch loss 1.45256698 epoch total loss 1.5801177\n",
      "Trained batch 967 batch loss 1.43875194 epoch total loss 1.57997143\n",
      "Trained batch 968 batch loss 1.39683342 epoch total loss 1.57978225\n",
      "Trained batch 969 batch loss 1.49440253 epoch total loss 1.57969415\n",
      "Trained batch 970 batch loss 1.49333763 epoch total loss 1.5796051\n",
      "Trained batch 971 batch loss 1.65895295 epoch total loss 1.57968676\n",
      "Trained batch 972 batch loss 1.60435617 epoch total loss 1.57971215\n",
      "Trained batch 973 batch loss 1.54503417 epoch total loss 1.57967651\n",
      "Trained batch 974 batch loss 1.32709467 epoch total loss 1.57941723\n",
      "Trained batch 975 batch loss 1.31409121 epoch total loss 1.57914507\n",
      "Trained batch 976 batch loss 1.30734026 epoch total loss 1.57886672\n",
      "Trained batch 977 batch loss 1.1797992 epoch total loss 1.57845819\n",
      "Trained batch 978 batch loss 1.24513435 epoch total loss 1.57811737\n",
      "Trained batch 979 batch loss 1.45258737 epoch total loss 1.57798922\n",
      "Trained batch 980 batch loss 1.56495035 epoch total loss 1.57797587\n",
      "Trained batch 981 batch loss 1.72492 epoch total loss 1.57812572\n",
      "Trained batch 982 batch loss 1.63575578 epoch total loss 1.57818437\n",
      "Trained batch 983 batch loss 1.49870622 epoch total loss 1.57810354\n",
      "Trained batch 984 batch loss 1.38433838 epoch total loss 1.57790661\n",
      "Trained batch 985 batch loss 1.47244477 epoch total loss 1.57779956\n",
      "Trained batch 986 batch loss 1.61253953 epoch total loss 1.57783484\n",
      "Trained batch 987 batch loss 1.60234499 epoch total loss 1.57785964\n",
      "Trained batch 988 batch loss 1.52071059 epoch total loss 1.57780182\n",
      "Trained batch 989 batch loss 1.54395723 epoch total loss 1.57776761\n",
      "Trained batch 990 batch loss 1.5642221 epoch total loss 1.5777539\n",
      "Trained batch 991 batch loss 1.48687804 epoch total loss 1.57766223\n",
      "Trained batch 992 batch loss 1.50005698 epoch total loss 1.57758391\n",
      "Trained batch 993 batch loss 1.45489347 epoch total loss 1.57746029\n",
      "Trained batch 994 batch loss 1.48348 epoch total loss 1.57736576\n",
      "Trained batch 995 batch loss 1.53258 epoch total loss 1.57732081\n",
      "Trained batch 996 batch loss 1.56332612 epoch total loss 1.57730675\n",
      "Trained batch 997 batch loss 1.25094712 epoch total loss 1.57697952\n",
      "Trained batch 998 batch loss 1.46038 epoch total loss 1.57686257\n",
      "Trained batch 999 batch loss 1.37474394 epoch total loss 1.57666028\n",
      "Trained batch 1000 batch loss 1.43197393 epoch total loss 1.57651567\n",
      "Trained batch 1001 batch loss 1.43515253 epoch total loss 1.57637441\n",
      "Trained batch 1002 batch loss 1.4075247 epoch total loss 1.57620585\n",
      "Trained batch 1003 batch loss 1.42735481 epoch total loss 1.57605743\n",
      "Trained batch 1004 batch loss 1.45752263 epoch total loss 1.57593942\n",
      "Trained batch 1005 batch loss 1.44296 epoch total loss 1.57580709\n",
      "Trained batch 1006 batch loss 1.27903318 epoch total loss 1.57551217\n",
      "Trained batch 1007 batch loss 1.26457334 epoch total loss 1.5752033\n",
      "Trained batch 1008 batch loss 1.14033914 epoch total loss 1.574772\n",
      "Trained batch 1009 batch loss 1.49616468 epoch total loss 1.57469404\n",
      "Trained batch 1010 batch loss 1.49193597 epoch total loss 1.57461214\n",
      "Trained batch 1011 batch loss 1.56392217 epoch total loss 1.57460165\n",
      "Trained batch 1012 batch loss 1.47934484 epoch total loss 1.57450747\n",
      "Trained batch 1013 batch loss 1.58932102 epoch total loss 1.57452214\n",
      "Trained batch 1014 batch loss 1.4815985 epoch total loss 1.57443047\n",
      "Trained batch 1015 batch loss 1.34353471 epoch total loss 1.57420301\n",
      "Trained batch 1016 batch loss 1.36415887 epoch total loss 1.57399619\n",
      "Trained batch 1017 batch loss 1.45532441 epoch total loss 1.5738796\n",
      "Trained batch 1018 batch loss 1.41458321 epoch total loss 1.57372308\n",
      "Trained batch 1019 batch loss 1.47293115 epoch total loss 1.57362413\n",
      "Trained batch 1020 batch loss 1.46085322 epoch total loss 1.57351351\n",
      "Trained batch 1021 batch loss 1.45774889 epoch total loss 1.57340014\n",
      "Trained batch 1022 batch loss 1.4286567 epoch total loss 1.57325852\n",
      "Trained batch 1023 batch loss 1.47565448 epoch total loss 1.57316315\n",
      "Trained batch 1024 batch loss 1.48276925 epoch total loss 1.57307494\n",
      "Trained batch 1025 batch loss 1.43382573 epoch total loss 1.57293904\n",
      "Trained batch 1026 batch loss 1.45182204 epoch total loss 1.57282102\n",
      "Trained batch 1027 batch loss 1.50854552 epoch total loss 1.57275844\n",
      "Trained batch 1028 batch loss 1.42982042 epoch total loss 1.57261932\n",
      "Trained batch 1029 batch loss 1.43386793 epoch total loss 1.57248449\n",
      "Trained batch 1030 batch loss 1.42708015 epoch total loss 1.57234335\n",
      "Trained batch 1031 batch loss 1.47292614 epoch total loss 1.57224691\n",
      "Trained batch 1032 batch loss 1.41101396 epoch total loss 1.57209063\n",
      "Trained batch 1033 batch loss 1.57361436 epoch total loss 1.57209218\n",
      "Trained batch 1034 batch loss 1.5149672 epoch total loss 1.57203698\n",
      "Trained batch 1035 batch loss 1.53556871 epoch total loss 1.5720017\n",
      "Trained batch 1036 batch loss 1.67404723 epoch total loss 1.57210016\n",
      "Trained batch 1037 batch loss 1.71968472 epoch total loss 1.5722425\n",
      "Trained batch 1038 batch loss 1.54268098 epoch total loss 1.57221413\n",
      "Trained batch 1039 batch loss 1.63776231 epoch total loss 1.57227731\n",
      "Trained batch 1040 batch loss 1.51853526 epoch total loss 1.57222557\n",
      "Trained batch 1041 batch loss 1.49723208 epoch total loss 1.57215357\n",
      "Trained batch 1042 batch loss 1.47418678 epoch total loss 1.57205951\n",
      "Trained batch 1043 batch loss 1.52167177 epoch total loss 1.57201135\n",
      "Trained batch 1044 batch loss 1.39065862 epoch total loss 1.57183754\n",
      "Trained batch 1045 batch loss 1.3602221 epoch total loss 1.57163501\n",
      "Trained batch 1046 batch loss 1.32896733 epoch total loss 1.57140303\n",
      "Trained batch 1047 batch loss 1.56837296 epoch total loss 1.57140017\n",
      "Trained batch 1048 batch loss 1.36237776 epoch total loss 1.57120073\n",
      "Trained batch 1049 batch loss 1.45171654 epoch total loss 1.57108676\n",
      "Trained batch 1050 batch loss 1.40408421 epoch total loss 1.57092774\n",
      "Trained batch 1051 batch loss 1.3332721 epoch total loss 1.5707016\n",
      "Trained batch 1052 batch loss 1.42013776 epoch total loss 1.57055855\n",
      "Trained batch 1053 batch loss 1.22481704 epoch total loss 1.57023025\n",
      "Trained batch 1054 batch loss 1.2688936 epoch total loss 1.56994426\n",
      "Trained batch 1055 batch loss 1.41957641 epoch total loss 1.56980181\n",
      "Trained batch 1056 batch loss 1.44933629 epoch total loss 1.56968772\n",
      "Trained batch 1057 batch loss 1.38916612 epoch total loss 1.5695169\n",
      "Trained batch 1058 batch loss 1.52096474 epoch total loss 1.569471\n",
      "Trained batch 1059 batch loss 1.57404304 epoch total loss 1.56947541\n",
      "Trained batch 1060 batch loss 1.5368228 epoch total loss 1.56944466\n",
      "Trained batch 1061 batch loss 1.53597486 epoch total loss 1.56941319\n",
      "Trained batch 1062 batch loss 1.51203585 epoch total loss 1.56935918\n",
      "Trained batch 1063 batch loss 1.50145054 epoch total loss 1.56929529\n",
      "Trained batch 1064 batch loss 1.44081521 epoch total loss 1.56917453\n",
      "Trained batch 1065 batch loss 1.48785365 epoch total loss 1.56909811\n",
      "Trained batch 1066 batch loss 1.35161114 epoch total loss 1.56889403\n",
      "Trained batch 1067 batch loss 1.31612551 epoch total loss 1.56865716\n",
      "Trained batch 1068 batch loss 1.38774359 epoch total loss 1.56848776\n",
      "Trained batch 1069 batch loss 1.40592802 epoch total loss 1.56833565\n",
      "Trained batch 1070 batch loss 1.45405817 epoch total loss 1.56822884\n",
      "Trained batch 1071 batch loss 1.45783103 epoch total loss 1.56812584\n",
      "Trained batch 1072 batch loss 1.45918298 epoch total loss 1.56802428\n",
      "Trained batch 1073 batch loss 1.38741541 epoch total loss 1.56785595\n",
      "Trained batch 1074 batch loss 1.53855193 epoch total loss 1.56782866\n",
      "Trained batch 1075 batch loss 1.51111937 epoch total loss 1.56777596\n",
      "Trained batch 1076 batch loss 1.53994393 epoch total loss 1.5677501\n",
      "Trained batch 1077 batch loss 1.48129344 epoch total loss 1.56766975\n",
      "Trained batch 1078 batch loss 1.41409028 epoch total loss 1.56752729\n",
      "Trained batch 1079 batch loss 1.43861341 epoch total loss 1.56740785\n",
      "Trained batch 1080 batch loss 1.38291 epoch total loss 1.56723702\n",
      "Trained batch 1081 batch loss 1.33700132 epoch total loss 1.56702411\n",
      "Trained batch 1082 batch loss 1.49217713 epoch total loss 1.56695485\n",
      "Trained batch 1083 batch loss 1.41590965 epoch total loss 1.56681538\n",
      "Trained batch 1084 batch loss 1.44542098 epoch total loss 1.56670344\n",
      "Trained batch 1085 batch loss 1.43329096 epoch total loss 1.56658053\n",
      "Trained batch 1086 batch loss 1.46905029 epoch total loss 1.56649065\n",
      "Trained batch 1087 batch loss 1.67280173 epoch total loss 1.56658852\n",
      "Trained batch 1088 batch loss 1.54954338 epoch total loss 1.5665729\n",
      "Trained batch 1089 batch loss 1.7167716 epoch total loss 1.56671083\n",
      "Trained batch 1090 batch loss 1.52777719 epoch total loss 1.56667519\n",
      "Trained batch 1091 batch loss 1.40824008 epoch total loss 1.56652987\n",
      "Trained batch 1092 batch loss 1.42814946 epoch total loss 1.56640315\n",
      "Trained batch 1093 batch loss 1.52074826 epoch total loss 1.56636131\n",
      "Trained batch 1094 batch loss 1.55417347 epoch total loss 1.56635022\n",
      "Trained batch 1095 batch loss 1.53209019 epoch total loss 1.56631899\n",
      "Trained batch 1096 batch loss 1.43580723 epoch total loss 1.5661999\n",
      "Trained batch 1097 batch loss 1.42456949 epoch total loss 1.5660708\n",
      "Trained batch 1098 batch loss 1.43083417 epoch total loss 1.56594753\n",
      "Trained batch 1099 batch loss 1.34471345 epoch total loss 1.56574631\n",
      "Trained batch 1100 batch loss 1.55686307 epoch total loss 1.5657382\n",
      "Trained batch 1101 batch loss 1.63533688 epoch total loss 1.5658015\n",
      "Trained batch 1102 batch loss 1.70865154 epoch total loss 1.56593108\n",
      "Trained batch 1103 batch loss 1.62680113 epoch total loss 1.56598628\n",
      "Trained batch 1104 batch loss 1.57521272 epoch total loss 1.56599462\n",
      "Trained batch 1105 batch loss 1.58859062 epoch total loss 1.56601512\n",
      "Trained batch 1106 batch loss 1.58973205 epoch total loss 1.56603646\n",
      "Trained batch 1107 batch loss 1.494349 epoch total loss 1.56597173\n",
      "Trained batch 1108 batch loss 1.48114383 epoch total loss 1.56589532\n",
      "Trained batch 1109 batch loss 1.45570028 epoch total loss 1.5657959\n",
      "Trained batch 1110 batch loss 1.52002835 epoch total loss 1.56575465\n",
      "Trained batch 1111 batch loss 1.57889497 epoch total loss 1.56576645\n",
      "Trained batch 1112 batch loss 1.59530163 epoch total loss 1.56579304\n",
      "Trained batch 1113 batch loss 1.44939864 epoch total loss 1.56568837\n",
      "Trained batch 1114 batch loss 1.47386265 epoch total loss 1.565606\n",
      "Trained batch 1115 batch loss 1.4891901 epoch total loss 1.56553745\n",
      "Trained batch 1116 batch loss 1.49033701 epoch total loss 1.5654701\n",
      "Trained batch 1117 batch loss 1.51961958 epoch total loss 1.56542897\n",
      "Trained batch 1118 batch loss 1.57937193 epoch total loss 1.56544149\n",
      "Trained batch 1119 batch loss 1.6138382 epoch total loss 1.56548476\n",
      "Trained batch 1120 batch loss 1.68924236 epoch total loss 1.56559527\n",
      "Trained batch 1121 batch loss 1.58259785 epoch total loss 1.56561041\n",
      "Trained batch 1122 batch loss 1.62227726 epoch total loss 1.56566095\n",
      "Trained batch 1123 batch loss 1.56860018 epoch total loss 1.56566358\n",
      "Trained batch 1124 batch loss 1.58855009 epoch total loss 1.56568396\n",
      "Trained batch 1125 batch loss 1.61143827 epoch total loss 1.56572461\n",
      "Trained batch 1126 batch loss 1.55800748 epoch total loss 1.5657177\n",
      "Trained batch 1127 batch loss 1.45181704 epoch total loss 1.56561661\n",
      "Trained batch 1128 batch loss 1.47608542 epoch total loss 1.56553721\n",
      "Trained batch 1129 batch loss 1.59349418 epoch total loss 1.56556201\n",
      "Trained batch 1130 batch loss 1.34323573 epoch total loss 1.56536531\n",
      "Trained batch 1131 batch loss 1.26502621 epoch total loss 1.56509972\n",
      "Trained batch 1132 batch loss 1.36061597 epoch total loss 1.56491911\n",
      "Trained batch 1133 batch loss 1.57541549 epoch total loss 1.56492841\n",
      "Trained batch 1134 batch loss 1.47196341 epoch total loss 1.5648464\n",
      "Trained batch 1135 batch loss 1.49901652 epoch total loss 1.56478834\n",
      "Trained batch 1136 batch loss 1.53510773 epoch total loss 1.56476223\n",
      "Trained batch 1137 batch loss 1.43599796 epoch total loss 1.56464911\n",
      "Trained batch 1138 batch loss 1.46587408 epoch total loss 1.5645622\n",
      "Trained batch 1139 batch loss 1.44255257 epoch total loss 1.56445503\n",
      "Trained batch 1140 batch loss 1.48847175 epoch total loss 1.56438839\n",
      "Trained batch 1141 batch loss 1.34243727 epoch total loss 1.56419384\n",
      "Trained batch 1142 batch loss 1.38970697 epoch total loss 1.56404102\n",
      "Trained batch 1143 batch loss 1.44672775 epoch total loss 1.5639385\n",
      "Trained batch 1144 batch loss 1.48411322 epoch total loss 1.56386876\n",
      "Trained batch 1145 batch loss 1.46228218 epoch total loss 1.56378\n",
      "Trained batch 1146 batch loss 1.46359611 epoch total loss 1.56369257\n",
      "Trained batch 1147 batch loss 1.47193861 epoch total loss 1.56361258\n",
      "Trained batch 1148 batch loss 1.44857836 epoch total loss 1.56351244\n",
      "Trained batch 1149 batch loss 1.45906138 epoch total loss 1.56342149\n",
      "Trained batch 1150 batch loss 1.44611573 epoch total loss 1.56331956\n",
      "Trained batch 1151 batch loss 1.45499814 epoch total loss 1.56322539\n",
      "Trained batch 1152 batch loss 1.53325856 epoch total loss 1.5631994\n",
      "Trained batch 1153 batch loss 1.51733613 epoch total loss 1.56315958\n",
      "Trained batch 1154 batch loss 1.45432496 epoch total loss 1.56306529\n",
      "Trained batch 1155 batch loss 1.42115903 epoch total loss 1.56294239\n",
      "Trained batch 1156 batch loss 1.49701858 epoch total loss 1.5628854\n",
      "Trained batch 1157 batch loss 1.37449527 epoch total loss 1.56272256\n",
      "Trained batch 1158 batch loss 1.48606098 epoch total loss 1.5626564\n",
      "Trained batch 1159 batch loss 1.45939684 epoch total loss 1.56256735\n",
      "Trained batch 1160 batch loss 1.4087137 epoch total loss 1.56243467\n",
      "Trained batch 1161 batch loss 1.56775451 epoch total loss 1.5624392\n",
      "Trained batch 1162 batch loss 1.42419219 epoch total loss 1.56232023\n",
      "Trained batch 1163 batch loss 1.42960584 epoch total loss 1.56220615\n",
      "Trained batch 1164 batch loss 1.47600746 epoch total loss 1.562132\n",
      "Trained batch 1165 batch loss 1.32881 epoch total loss 1.56193173\n",
      "Trained batch 1166 batch loss 1.42991865 epoch total loss 1.5618186\n",
      "Trained batch 1167 batch loss 1.39644361 epoch total loss 1.56167686\n",
      "Trained batch 1168 batch loss 1.44557595 epoch total loss 1.56157744\n",
      "Trained batch 1169 batch loss 1.45774889 epoch total loss 1.56148863\n",
      "Trained batch 1170 batch loss 1.46610522 epoch total loss 1.56140709\n",
      "Trained batch 1171 batch loss 1.57889569 epoch total loss 1.56142199\n",
      "Trained batch 1172 batch loss 1.47486174 epoch total loss 1.5613482\n",
      "Trained batch 1173 batch loss 1.52503181 epoch total loss 1.56131721\n",
      "Trained batch 1174 batch loss 1.46209502 epoch total loss 1.56123257\n",
      "Trained batch 1175 batch loss 1.41125882 epoch total loss 1.56110501\n",
      "Trained batch 1176 batch loss 1.3476398 epoch total loss 1.56092346\n",
      "Trained batch 1177 batch loss 1.41813636 epoch total loss 1.5608021\n",
      "Trained batch 1178 batch loss 1.43219519 epoch total loss 1.56069303\n",
      "Trained batch 1179 batch loss 1.51086175 epoch total loss 1.56065071\n",
      "Trained batch 1180 batch loss 1.55904722 epoch total loss 1.56064939\n",
      "Trained batch 1181 batch loss 1.67948198 epoch total loss 1.56075\n",
      "Trained batch 1182 batch loss 1.55356956 epoch total loss 1.56074393\n",
      "Trained batch 1183 batch loss 1.56911981 epoch total loss 1.56075096\n",
      "Trained batch 1184 batch loss 1.48622656 epoch total loss 1.56068802\n",
      "Trained batch 1185 batch loss 1.36931586 epoch total loss 1.56052649\n",
      "Trained batch 1186 batch loss 1.37263465 epoch total loss 1.56036806\n",
      "Trained batch 1187 batch loss 1.52299249 epoch total loss 1.56033659\n",
      "Trained batch 1188 batch loss 1.520311 epoch total loss 1.56030285\n",
      "Trained batch 1189 batch loss 1.42550957 epoch total loss 1.56018949\n",
      "Trained batch 1190 batch loss 1.50692105 epoch total loss 1.56014478\n",
      "Trained batch 1191 batch loss 1.42399776 epoch total loss 1.56003046\n",
      "Trained batch 1192 batch loss 1.50462592 epoch total loss 1.55998397\n",
      "Trained batch 1193 batch loss 1.45658457 epoch total loss 1.55989718\n",
      "Trained batch 1194 batch loss 1.47908831 epoch total loss 1.55982959\n",
      "Trained batch 1195 batch loss 1.4326179 epoch total loss 1.55972314\n",
      "Trained batch 1196 batch loss 1.44558954 epoch total loss 1.55962765\n",
      "Trained batch 1197 batch loss 1.40243673 epoch total loss 1.5594964\n",
      "Trained batch 1198 batch loss 1.46370018 epoch total loss 1.55941641\n",
      "Trained batch 1199 batch loss 1.43642652 epoch total loss 1.55931389\n",
      "Trained batch 1200 batch loss 1.49271417 epoch total loss 1.55925834\n",
      "Trained batch 1201 batch loss 1.41964483 epoch total loss 1.55914211\n",
      "Trained batch 1202 batch loss 1.4367007 epoch total loss 1.55904019\n",
      "Trained batch 1203 batch loss 1.46758258 epoch total loss 1.55896413\n",
      "Trained batch 1204 batch loss 1.53911924 epoch total loss 1.55894756\n",
      "Trained batch 1205 batch loss 1.5274291 epoch total loss 1.55892146\n",
      "Trained batch 1206 batch loss 1.55154061 epoch total loss 1.55891538\n",
      "Trained batch 1207 batch loss 1.53029096 epoch total loss 1.55889153\n",
      "Trained batch 1208 batch loss 1.40787578 epoch total loss 1.5587666\n",
      "Trained batch 1209 batch loss 1.45801377 epoch total loss 1.55868316\n",
      "Trained batch 1210 batch loss 1.46383154 epoch total loss 1.55860484\n",
      "Trained batch 1211 batch loss 1.40575075 epoch total loss 1.55847859\n",
      "Trained batch 1212 batch loss 1.36383843 epoch total loss 1.55831814\n",
      "Trained batch 1213 batch loss 1.40127122 epoch total loss 1.55818856\n",
      "Trained batch 1214 batch loss 1.50875366 epoch total loss 1.55814791\n",
      "Trained batch 1215 batch loss 1.51883793 epoch total loss 1.55811548\n",
      "Trained batch 1216 batch loss 1.50794768 epoch total loss 1.55807424\n",
      "Trained batch 1217 batch loss 1.39772534 epoch total loss 1.55794251\n",
      "Trained batch 1218 batch loss 1.31972086 epoch total loss 1.55774689\n",
      "Trained batch 1219 batch loss 1.42087245 epoch total loss 1.55763459\n",
      "Trained batch 1220 batch loss 1.40584636 epoch total loss 1.55751026\n",
      "Trained batch 1221 batch loss 1.36648142 epoch total loss 1.55735373\n",
      "Trained batch 1222 batch loss 1.40575159 epoch total loss 1.55722976\n",
      "Trained batch 1223 batch loss 1.46296763 epoch total loss 1.55715263\n",
      "Trained batch 1224 batch loss 1.4953028 epoch total loss 1.5571022\n",
      "Trained batch 1225 batch loss 1.45119619 epoch total loss 1.55701566\n",
      "Trained batch 1226 batch loss 1.36342371 epoch total loss 1.55685782\n",
      "Trained batch 1227 batch loss 1.51425469 epoch total loss 1.55682313\n",
      "Trained batch 1228 batch loss 1.59367263 epoch total loss 1.55685306\n",
      "Trained batch 1229 batch loss 1.56866336 epoch total loss 1.55686259\n",
      "Trained batch 1230 batch loss 1.66895032 epoch total loss 1.55695379\n",
      "Trained batch 1231 batch loss 1.37705708 epoch total loss 1.55680764\n",
      "Trained batch 1232 batch loss 1.38097024 epoch total loss 1.55666494\n",
      "Trained batch 1233 batch loss 1.50614667 epoch total loss 1.55662394\n",
      "Trained batch 1234 batch loss 1.57145834 epoch total loss 1.55663586\n",
      "Trained batch 1235 batch loss 1.55112147 epoch total loss 1.55663145\n",
      "Trained batch 1236 batch loss 1.51209 epoch total loss 1.55659544\n",
      "Trained batch 1237 batch loss 1.40847397 epoch total loss 1.55647564\n",
      "Trained batch 1238 batch loss 1.36130428 epoch total loss 1.55631804\n",
      "Trained batch 1239 batch loss 1.4142642 epoch total loss 1.55620337\n",
      "Trained batch 1240 batch loss 1.41372 epoch total loss 1.55608845\n",
      "Trained batch 1241 batch loss 1.49594498 epoch total loss 1.55604\n",
      "Trained batch 1242 batch loss 1.35591674 epoch total loss 1.55587888\n",
      "Trained batch 1243 batch loss 1.36924827 epoch total loss 1.55572879\n",
      "Trained batch 1244 batch loss 1.42366099 epoch total loss 1.5556227\n",
      "Trained batch 1245 batch loss 1.33979988 epoch total loss 1.55544937\n",
      "Trained batch 1246 batch loss 1.52185452 epoch total loss 1.55542243\n",
      "Trained batch 1247 batch loss 1.43799543 epoch total loss 1.55532825\n",
      "Trained batch 1248 batch loss 1.38458323 epoch total loss 1.5551914\n",
      "Trained batch 1249 batch loss 1.38199198 epoch total loss 1.55505276\n",
      "Trained batch 1250 batch loss 1.39358008 epoch total loss 1.55492353\n",
      "Trained batch 1251 batch loss 1.4902494 epoch total loss 1.5548718\n",
      "Trained batch 1252 batch loss 1.44412565 epoch total loss 1.55478334\n",
      "Trained batch 1253 batch loss 1.50613618 epoch total loss 1.55474448\n",
      "Trained batch 1254 batch loss 1.33715594 epoch total loss 1.55457103\n",
      "Trained batch 1255 batch loss 1.36127138 epoch total loss 1.55441701\n",
      "Trained batch 1256 batch loss 1.40253949 epoch total loss 1.55429614\n",
      "Trained batch 1257 batch loss 1.31548071 epoch total loss 1.55410612\n",
      "Trained batch 1258 batch loss 1.40466762 epoch total loss 1.55398726\n",
      "Trained batch 1259 batch loss 1.39502192 epoch total loss 1.55386102\n",
      "Trained batch 1260 batch loss 1.51965177 epoch total loss 1.55383384\n",
      "Trained batch 1261 batch loss 1.54796124 epoch total loss 1.55382919\n",
      "Trained batch 1262 batch loss 1.74354172 epoch total loss 1.55397952\n",
      "Trained batch 1263 batch loss 1.67237639 epoch total loss 1.55407333\n",
      "Trained batch 1264 batch loss 1.52726924 epoch total loss 1.554052\n",
      "Trained batch 1265 batch loss 1.62422097 epoch total loss 1.55410755\n",
      "Trained batch 1266 batch loss 1.51883221 epoch total loss 1.55407965\n",
      "Trained batch 1267 batch loss 1.34820819 epoch total loss 1.55391717\n",
      "Trained batch 1268 batch loss 1.44488049 epoch total loss 1.55383122\n",
      "Trained batch 1269 batch loss 1.48653364 epoch total loss 1.55377817\n",
      "Trained batch 1270 batch loss 1.52005422 epoch total loss 1.55375159\n",
      "Trained batch 1271 batch loss 1.47284698 epoch total loss 1.55368793\n",
      "Trained batch 1272 batch loss 1.38078308 epoch total loss 1.55355203\n",
      "Trained batch 1273 batch loss 1.4537282 epoch total loss 1.55347359\n",
      "Trained batch 1274 batch loss 1.33483231 epoch total loss 1.55330205\n",
      "Trained batch 1275 batch loss 1.36948633 epoch total loss 1.55315781\n",
      "Trained batch 1276 batch loss 1.35922873 epoch total loss 1.55300593\n",
      "Trained batch 1277 batch loss 1.49748945 epoch total loss 1.5529623\n",
      "Trained batch 1278 batch loss 1.41759229 epoch total loss 1.55285645\n",
      "Trained batch 1279 batch loss 1.41875935 epoch total loss 1.55275154\n",
      "Trained batch 1280 batch loss 1.31327915 epoch total loss 1.55256438\n",
      "Trained batch 1281 batch loss 1.32486296 epoch total loss 1.55238664\n",
      "Trained batch 1282 batch loss 1.47101402 epoch total loss 1.55232322\n",
      "Trained batch 1283 batch loss 1.48227453 epoch total loss 1.55226862\n",
      "Trained batch 1284 batch loss 1.44175732 epoch total loss 1.55218256\n",
      "Trained batch 1285 batch loss 1.52218723 epoch total loss 1.55215931\n",
      "Trained batch 1286 batch loss 1.52453244 epoch total loss 1.55213773\n",
      "Trained batch 1287 batch loss 1.55273676 epoch total loss 1.55213821\n",
      "Trained batch 1288 batch loss 1.5318197 epoch total loss 1.55212247\n",
      "Trained batch 1289 batch loss 1.59720349 epoch total loss 1.5521574\n",
      "Trained batch 1290 batch loss 1.52635264 epoch total loss 1.55213749\n",
      "Trained batch 1291 batch loss 1.43381751 epoch total loss 1.55204582\n",
      "Trained batch 1292 batch loss 1.40083504 epoch total loss 1.55192888\n",
      "Trained batch 1293 batch loss 1.56411767 epoch total loss 1.55193818\n",
      "Trained batch 1294 batch loss 1.50748992 epoch total loss 1.55190384\n",
      "Trained batch 1295 batch loss 1.47236943 epoch total loss 1.55184245\n",
      "Trained batch 1296 batch loss 1.3945024 epoch total loss 1.5517211\n",
      "Trained batch 1297 batch loss 1.41759038 epoch total loss 1.55161762\n",
      "Trained batch 1298 batch loss 1.38860393 epoch total loss 1.55149209\n",
      "Trained batch 1299 batch loss 1.3688612 epoch total loss 1.55135143\n",
      "Trained batch 1300 batch loss 1.49406016 epoch total loss 1.55130732\n",
      "Trained batch 1301 batch loss 1.42659235 epoch total loss 1.5512116\n",
      "Trained batch 1302 batch loss 1.5125829 epoch total loss 1.55118191\n",
      "Trained batch 1303 batch loss 1.32819974 epoch total loss 1.55101073\n",
      "Trained batch 1304 batch loss 1.49462295 epoch total loss 1.55096757\n",
      "Trained batch 1305 batch loss 1.40232 epoch total loss 1.55085361\n",
      "Trained batch 1306 batch loss 1.38871205 epoch total loss 1.55072951\n",
      "Trained batch 1307 batch loss 1.27023423 epoch total loss 1.55051494\n",
      "Trained batch 1308 batch loss 1.50529122 epoch total loss 1.55048025\n",
      "Trained batch 1309 batch loss 1.52002716 epoch total loss 1.550457\n",
      "Trained batch 1310 batch loss 1.52171659 epoch total loss 1.55043507\n",
      "Trained batch 1311 batch loss 1.39225888 epoch total loss 1.55031443\n",
      "Trained batch 1312 batch loss 1.4028399 epoch total loss 1.55020201\n",
      "Trained batch 1313 batch loss 1.34930682 epoch total loss 1.55004907\n",
      "Trained batch 1314 batch loss 1.35497594 epoch total loss 1.54990053\n",
      "Trained batch 1315 batch loss 1.33385372 epoch total loss 1.54973626\n",
      "Trained batch 1316 batch loss 1.23460317 epoch total loss 1.54949677\n",
      "Trained batch 1317 batch loss 1.34330034 epoch total loss 1.54934025\n",
      "Trained batch 1318 batch loss 1.4911387 epoch total loss 1.54929602\n",
      "Trained batch 1319 batch loss 1.43221688 epoch total loss 1.54920733\n",
      "Trained batch 1320 batch loss 1.49591279 epoch total loss 1.54916692\n",
      "Trained batch 1321 batch loss 1.47643459 epoch total loss 1.54911196\n",
      "Trained batch 1322 batch loss 1.51771593 epoch total loss 1.54908812\n",
      "Trained batch 1323 batch loss 1.44594359 epoch total loss 1.54901028\n",
      "Trained batch 1324 batch loss 1.49361372 epoch total loss 1.54896843\n",
      "Trained batch 1325 batch loss 1.35829008 epoch total loss 1.54882467\n",
      "Trained batch 1326 batch loss 1.32452679 epoch total loss 1.54865539\n",
      "Trained batch 1327 batch loss 1.21620488 epoch total loss 1.54840493\n",
      "Trained batch 1328 batch loss 1.29711628 epoch total loss 1.54821575\n",
      "Trained batch 1329 batch loss 1.67799377 epoch total loss 1.54831338\n",
      "Trained batch 1330 batch loss 1.63400984 epoch total loss 1.54837787\n",
      "Trained batch 1331 batch loss 1.52591896 epoch total loss 1.54836094\n",
      "Trained batch 1332 batch loss 1.4434725 epoch total loss 1.54828215\n",
      "Trained batch 1333 batch loss 1.45659852 epoch total loss 1.54821324\n",
      "Trained batch 1334 batch loss 1.5157299 epoch total loss 1.54818881\n",
      "Trained batch 1335 batch loss 1.44286144 epoch total loss 1.54811\n",
      "Trained batch 1336 batch loss 1.38789129 epoch total loss 1.54799008\n",
      "Trained batch 1337 batch loss 1.39563715 epoch total loss 1.54787624\n",
      "Trained batch 1338 batch loss 1.44120598 epoch total loss 1.54779649\n",
      "Trained batch 1339 batch loss 1.49144852 epoch total loss 1.54775441\n",
      "Trained batch 1340 batch loss 1.44136453 epoch total loss 1.54767501\n",
      "Trained batch 1341 batch loss 1.47538149 epoch total loss 1.54762113\n",
      "Trained batch 1342 batch loss 1.39308488 epoch total loss 1.54750586\n",
      "Trained batch 1343 batch loss 1.37521923 epoch total loss 1.54737759\n",
      "Trained batch 1344 batch loss 1.44230962 epoch total loss 1.5472995\n",
      "Trained batch 1345 batch loss 1.39549613 epoch total loss 1.54718661\n",
      "Trained batch 1346 batch loss 1.44971395 epoch total loss 1.54711425\n",
      "Trained batch 1347 batch loss 1.48722041 epoch total loss 1.54706979\n",
      "Trained batch 1348 batch loss 1.47091508 epoch total loss 1.5470134\n",
      "Trained batch 1349 batch loss 1.48166859 epoch total loss 1.546965\n",
      "Trained batch 1350 batch loss 1.56708813 epoch total loss 1.5469799\n",
      "Trained batch 1351 batch loss 1.41532803 epoch total loss 1.54688239\n",
      "Trained batch 1352 batch loss 1.41288567 epoch total loss 1.54678321\n",
      "Trained batch 1353 batch loss 1.57349539 epoch total loss 1.546803\n",
      "Trained batch 1354 batch loss 1.52451313 epoch total loss 1.54678643\n",
      "Trained batch 1355 batch loss 1.44505858 epoch total loss 1.54671144\n",
      "Trained batch 1356 batch loss 1.45114112 epoch total loss 1.54664099\n",
      "Trained batch 1357 batch loss 1.37019551 epoch total loss 1.54651082\n",
      "Trained batch 1358 batch loss 1.43175232 epoch total loss 1.5464263\n",
      "Trained batch 1359 batch loss 1.41879475 epoch total loss 1.54633224\n",
      "Trained batch 1360 batch loss 1.43963456 epoch total loss 1.54625392\n",
      "Trained batch 1361 batch loss 1.49227476 epoch total loss 1.5462141\n",
      "Trained batch 1362 batch loss 1.38284135 epoch total loss 1.54609418\n",
      "Trained batch 1363 batch loss 1.35887802 epoch total loss 1.54595685\n",
      "Trained batch 1364 batch loss 1.38089287 epoch total loss 1.54583573\n",
      "Trained batch 1365 batch loss 1.36257768 epoch total loss 1.5457015\n",
      "Trained batch 1366 batch loss 1.37146592 epoch total loss 1.54557407\n",
      "Trained batch 1367 batch loss 1.41442144 epoch total loss 1.54547799\n",
      "Trained batch 1368 batch loss 1.42530882 epoch total loss 1.54539013\n",
      "Trained batch 1369 batch loss 1.49316847 epoch total loss 1.54535198\n",
      "Trained batch 1370 batch loss 1.42328095 epoch total loss 1.54526293\n",
      "Trained batch 1371 batch loss 1.48129821 epoch total loss 1.5452162\n",
      "Trained batch 1372 batch loss 1.43343794 epoch total loss 1.54513466\n",
      "Trained batch 1373 batch loss 1.47932625 epoch total loss 1.54508674\n",
      "Trained batch 1374 batch loss 1.38533914 epoch total loss 1.54497039\n",
      "Trained batch 1375 batch loss 1.54097331 epoch total loss 1.54496753\n",
      "Trained batch 1376 batch loss 1.52055275 epoch total loss 1.54494977\n",
      "Trained batch 1377 batch loss 1.30961502 epoch total loss 1.54477882\n",
      "Trained batch 1378 batch loss 1.35833704 epoch total loss 1.54464352\n",
      "Trained batch 1379 batch loss 1.54900718 epoch total loss 1.54464674\n",
      "Trained batch 1380 batch loss 1.41814351 epoch total loss 1.54455519\n",
      "Trained batch 1381 batch loss 1.46489227 epoch total loss 1.54449737\n",
      "Trained batch 1382 batch loss 1.41447783 epoch total loss 1.54440343\n",
      "Trained batch 1383 batch loss 1.46901536 epoch total loss 1.54434884\n",
      "Trained batch 1384 batch loss 1.40893185 epoch total loss 1.54425097\n",
      "Trained batch 1385 batch loss 1.4508214 epoch total loss 1.54418361\n",
      "Trained batch 1386 batch loss 1.51956177 epoch total loss 1.54416585\n",
      "Trained batch 1387 batch loss 1.48823 epoch total loss 1.54412556\n",
      "Trained batch 1388 batch loss 1.40729988 epoch total loss 1.54402697\n",
      "Epoch 1 train loss 1.5440269708633423\n",
      "Validated batch 1 batch loss 1.39306331\n",
      "Validated batch 2 batch loss 1.38354897\n",
      "Validated batch 3 batch loss 1.38475275\n",
      "Validated batch 4 batch loss 1.39004469\n",
      "Validated batch 5 batch loss 1.42597842\n",
      "Validated batch 6 batch loss 1.43680549\n",
      "Validated batch 7 batch loss 1.4361397\n",
      "Validated batch 8 batch loss 1.42273092\n",
      "Validated batch 9 batch loss 1.42669761\n",
      "Validated batch 10 batch loss 1.50008464\n",
      "Validated batch 11 batch loss 1.45927203\n",
      "Validated batch 12 batch loss 1.4286027\n",
      "Validated batch 13 batch loss 1.42480969\n",
      "Validated batch 14 batch loss 1.37040794\n",
      "Validated batch 15 batch loss 1.44788671\n",
      "Validated batch 16 batch loss 1.46324086\n",
      "Validated batch 17 batch loss 1.51172245\n",
      "Validated batch 18 batch loss 1.45129514\n",
      "Validated batch 19 batch loss 1.37324917\n",
      "Validated batch 20 batch loss 1.48803294\n",
      "Validated batch 21 batch loss 1.37848771\n",
      "Validated batch 22 batch loss 1.40773678\n",
      "Validated batch 23 batch loss 1.43096399\n",
      "Validated batch 24 batch loss 1.38249469\n",
      "Validated batch 25 batch loss 1.42784464\n",
      "Validated batch 26 batch loss 1.37187147\n",
      "Validated batch 27 batch loss 1.32566166\n",
      "Validated batch 28 batch loss 1.33798599\n",
      "Validated batch 29 batch loss 1.44874692\n",
      "Validated batch 30 batch loss 1.39785993\n",
      "Validated batch 31 batch loss 1.35887432\n",
      "Validated batch 32 batch loss 1.40632725\n",
      "Validated batch 33 batch loss 1.42075706\n",
      "Validated batch 34 batch loss 1.37484097\n",
      "Validated batch 35 batch loss 1.36938763\n",
      "Validated batch 36 batch loss 1.46784544\n",
      "Validated batch 37 batch loss 1.40760601\n",
      "Validated batch 38 batch loss 1.47034705\n",
      "Validated batch 39 batch loss 1.49629736\n",
      "Validated batch 40 batch loss 1.42143583\n",
      "Validated batch 41 batch loss 1.47621655\n",
      "Validated batch 42 batch loss 1.35288334\n",
      "Validated batch 43 batch loss 1.39175224\n",
      "Validated batch 44 batch loss 1.37618637\n",
      "Validated batch 45 batch loss 1.4433856\n",
      "Validated batch 46 batch loss 1.56052887\n",
      "Validated batch 47 batch loss 1.37846613\n",
      "Validated batch 48 batch loss 1.35554075\n",
      "Validated batch 49 batch loss 1.43755722\n",
      "Validated batch 50 batch loss 1.335549\n",
      "Validated batch 51 batch loss 1.43858576\n",
      "Validated batch 52 batch loss 1.4702065\n",
      "Validated batch 53 batch loss 1.46367979\n",
      "Validated batch 54 batch loss 1.52245808\n",
      "Validated batch 55 batch loss 1.47009027\n",
      "Validated batch 56 batch loss 1.43380785\n",
      "Validated batch 57 batch loss 1.41022372\n",
      "Validated batch 58 batch loss 1.49139583\n",
      "Validated batch 59 batch loss 1.47834706\n",
      "Validated batch 60 batch loss 1.50256526\n",
      "Validated batch 61 batch loss 1.46547389\n",
      "Validated batch 62 batch loss 1.46661949\n",
      "Validated batch 63 batch loss 1.50537658\n",
      "Validated batch 64 batch loss 1.28824139\n",
      "Validated batch 65 batch loss 1.3901577\n",
      "Validated batch 66 batch loss 1.45545852\n",
      "Validated batch 67 batch loss 1.46472192\n",
      "Validated batch 68 batch loss 1.45421147\n",
      "Validated batch 69 batch loss 1.40352368\n",
      "Validated batch 70 batch loss 1.30186439\n",
      "Validated batch 71 batch loss 1.50368333\n",
      "Validated batch 72 batch loss 1.48957717\n",
      "Validated batch 73 batch loss 1.40096259\n",
      "Validated batch 74 batch loss 1.46139514\n",
      "Validated batch 75 batch loss 1.54715967\n",
      "Validated batch 76 batch loss 1.30245018\n",
      "Validated batch 77 batch loss 1.46612632\n",
      "Validated batch 78 batch loss 1.40925765\n",
      "Validated batch 79 batch loss 1.45941043\n",
      "Validated batch 80 batch loss 1.45526612\n",
      "Validated batch 81 batch loss 1.34869456\n",
      "Validated batch 82 batch loss 1.25458133\n",
      "Validated batch 83 batch loss 1.46611214\n",
      "Validated batch 84 batch loss 1.4207052\n",
      "Validated batch 85 batch loss 1.40374851\n",
      "Validated batch 86 batch loss 1.44856834\n",
      "Validated batch 87 batch loss 1.41418588\n",
      "Validated batch 88 batch loss 1.44720531\n",
      "Validated batch 89 batch loss 1.47834873\n",
      "Validated batch 90 batch loss 1.44498992\n",
      "Validated batch 91 batch loss 1.45976365\n",
      "Validated batch 92 batch loss 1.35039639\n",
      "Validated batch 93 batch loss 1.42581153\n",
      "Validated batch 94 batch loss 1.50942135\n",
      "Validated batch 95 batch loss 1.36651826\n",
      "Validated batch 96 batch loss 1.34005332\n",
      "Validated batch 97 batch loss 1.47604334\n",
      "Validated batch 98 batch loss 1.37133813\n",
      "Validated batch 99 batch loss 1.33115697\n",
      "Validated batch 100 batch loss 1.39707279\n",
      "Validated batch 101 batch loss 1.33152175\n",
      "Validated batch 102 batch loss 1.51557541\n",
      "Validated batch 103 batch loss 1.34573197\n",
      "Validated batch 104 batch loss 1.32568324\n",
      "Validated batch 105 batch loss 1.3791225\n",
      "Validated batch 106 batch loss 1.53448594\n",
      "Validated batch 107 batch loss 1.46792245\n",
      "Validated batch 108 batch loss 1.53338671\n",
      "Validated batch 109 batch loss 1.37114906\n",
      "Validated batch 110 batch loss 1.53129125\n",
      "Validated batch 111 batch loss 1.40795648\n",
      "Validated batch 112 batch loss 1.47524226\n",
      "Validated batch 113 batch loss 1.50511217\n",
      "Validated batch 114 batch loss 1.21644509\n",
      "Validated batch 115 batch loss 1.4707824\n",
      "Validated batch 116 batch loss 1.52082777\n",
      "Validated batch 117 batch loss 1.41741395\n",
      "Validated batch 118 batch loss 1.42665923\n",
      "Validated batch 119 batch loss 1.40741897\n",
      "Validated batch 120 batch loss 1.36645567\n",
      "Validated batch 121 batch loss 1.47645354\n",
      "Validated batch 122 batch loss 1.4127655\n",
      "Validated batch 123 batch loss 1.35377574\n",
      "Validated batch 124 batch loss 1.36316502\n",
      "Validated batch 125 batch loss 1.40127218\n",
      "Validated batch 126 batch loss 1.43848693\n",
      "Validated batch 127 batch loss 1.44973671\n",
      "Validated batch 128 batch loss 1.42677379\n",
      "Validated batch 129 batch loss 1.34019303\n",
      "Validated batch 130 batch loss 1.40032542\n",
      "Validated batch 131 batch loss 1.4476794\n",
      "Validated batch 132 batch loss 1.43977642\n",
      "Validated batch 133 batch loss 1.43883383\n",
      "Validated batch 134 batch loss 1.52582169\n",
      "Validated batch 135 batch loss 1.63766384\n",
      "Validated batch 136 batch loss 1.56113446\n",
      "Validated batch 137 batch loss 1.41063142\n",
      "Validated batch 138 batch loss 1.34707701\n",
      "Validated batch 139 batch loss 1.39588654\n",
      "Validated batch 140 batch loss 1.41028416\n",
      "Validated batch 141 batch loss 1.41294587\n",
      "Validated batch 142 batch loss 1.3677417\n",
      "Validated batch 143 batch loss 1.38453054\n",
      "Validated batch 144 batch loss 1.51918554\n",
      "Validated batch 145 batch loss 1.35560811\n",
      "Validated batch 146 batch loss 1.44261587\n",
      "Validated batch 147 batch loss 1.38963866\n",
      "Validated batch 148 batch loss 1.46837389\n",
      "Validated batch 149 batch loss 1.39222014\n",
      "Validated batch 150 batch loss 1.3505975\n",
      "Validated batch 151 batch loss 1.42566478\n",
      "Validated batch 152 batch loss 1.47016621\n",
      "Validated batch 153 batch loss 1.46437883\n",
      "Validated batch 154 batch loss 1.39872503\n",
      "Validated batch 155 batch loss 1.45151901\n",
      "Validated batch 156 batch loss 1.38940692\n",
      "Validated batch 157 batch loss 1.31552076\n",
      "Validated batch 158 batch loss 1.4237442\n",
      "Validated batch 159 batch loss 1.37843919\n",
      "Validated batch 160 batch loss 1.38242197\n",
      "Validated batch 161 batch loss 1.43791056\n",
      "Validated batch 162 batch loss 1.46172357\n",
      "Validated batch 163 batch loss 1.46101713\n",
      "Validated batch 164 batch loss 1.37311053\n",
      "Validated batch 165 batch loss 1.37486553\n",
      "Validated batch 166 batch loss 1.45341206\n",
      "Validated batch 167 batch loss 1.41733873\n",
      "Validated batch 168 batch loss 1.46281886\n",
      "Validated batch 169 batch loss 1.51447928\n",
      "Validated batch 170 batch loss 1.49132323\n",
      "Validated batch 171 batch loss 1.45477796\n",
      "Validated batch 172 batch loss 1.46470797\n",
      "Validated batch 173 batch loss 1.49838817\n",
      "Validated batch 174 batch loss 1.35768259\n",
      "Validated batch 175 batch loss 1.4867146\n",
      "Validated batch 176 batch loss 1.51561975\n",
      "Validated batch 177 batch loss 1.40824175\n",
      "Validated batch 178 batch loss 1.4692049\n",
      "Validated batch 179 batch loss 1.37605691\n",
      "Validated batch 180 batch loss 1.31415617\n",
      "Validated batch 181 batch loss 1.44122577\n",
      "Validated batch 182 batch loss 1.3592068\n",
      "Validated batch 183 batch loss 1.53980982\n",
      "Validated batch 184 batch loss 1.38825321\n",
      "Validated batch 185 batch loss 1.44612336\n",
      "Epoch 1 val loss 1.4256019592285156\n",
      "Model /aiffel/aiffel/mpii/models/model-epoch-1-loss-1.4256.h5 saved.\n",
      "Start epoch 2 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.40563869 epoch total loss 1.40563869\n",
      "Trained batch 2 batch loss 1.43685341 epoch total loss 1.42124605\n",
      "Trained batch 3 batch loss 1.45901966 epoch total loss 1.43383729\n",
      "Trained batch 4 batch loss 1.43738246 epoch total loss 1.43472362\n",
      "Trained batch 5 batch loss 1.41600418 epoch total loss 1.43097973\n",
      "Trained batch 6 batch loss 1.41020441 epoch total loss 1.42751729\n",
      "Trained batch 7 batch loss 1.39330649 epoch total loss 1.42263007\n",
      "Trained batch 8 batch loss 1.4233321 epoch total loss 1.42271781\n",
      "Trained batch 9 batch loss 1.46333861 epoch total loss 1.42723131\n",
      "Trained batch 10 batch loss 1.35192013 epoch total loss 1.41970015\n",
      "Trained batch 11 batch loss 1.46767366 epoch total loss 1.4240613\n",
      "Trained batch 12 batch loss 1.39899194 epoch total loss 1.42197227\n",
      "Trained batch 13 batch loss 1.35885131 epoch total loss 1.41711676\n",
      "Trained batch 14 batch loss 1.36102045 epoch total loss 1.4131099\n",
      "Trained batch 15 batch loss 1.34094489 epoch total loss 1.40829885\n",
      "Trained batch 16 batch loss 1.33539259 epoch total loss 1.40374219\n",
      "Trained batch 17 batch loss 1.38463557 epoch total loss 1.40261829\n",
      "Trained batch 18 batch loss 1.41071057 epoch total loss 1.40306795\n",
      "Trained batch 19 batch loss 1.34888577 epoch total loss 1.40021622\n",
      "Trained batch 20 batch loss 1.41829491 epoch total loss 1.40112019\n",
      "Trained batch 21 batch loss 1.46261859 epoch total loss 1.40404868\n",
      "Trained batch 22 batch loss 1.50392246 epoch total loss 1.40858841\n",
      "Trained batch 23 batch loss 1.47158527 epoch total loss 1.41132736\n",
      "Trained batch 24 batch loss 1.4070189 epoch total loss 1.41114795\n",
      "Trained batch 25 batch loss 1.2971735 epoch total loss 1.40658891\n",
      "Trained batch 26 batch loss 1.37997139 epoch total loss 1.40556514\n",
      "Trained batch 27 batch loss 1.3697077 epoch total loss 1.40423715\n",
      "Trained batch 28 batch loss 1.37016881 epoch total loss 1.4030205\n",
      "Trained batch 29 batch loss 1.41378784 epoch total loss 1.40339172\n",
      "Trained batch 30 batch loss 1.43202734 epoch total loss 1.40434623\n",
      "Trained batch 31 batch loss 1.51918912 epoch total loss 1.40805078\n",
      "Trained batch 32 batch loss 1.29454267 epoch total loss 1.4045037\n",
      "Trained batch 33 batch loss 1.47094238 epoch total loss 1.40651703\n",
      "Trained batch 34 batch loss 1.42637682 epoch total loss 1.40710115\n",
      "Trained batch 35 batch loss 1.52045441 epoch total loss 1.41033983\n",
      "Trained batch 36 batch loss 1.51581931 epoch total loss 1.41326976\n",
      "Trained batch 37 batch loss 1.72243619 epoch total loss 1.42162561\n",
      "Trained batch 38 batch loss 1.4718399 epoch total loss 1.42294705\n",
      "Trained batch 39 batch loss 1.25238824 epoch total loss 1.41857374\n",
      "Trained batch 40 batch loss 1.39595151 epoch total loss 1.41800809\n",
      "Trained batch 41 batch loss 1.520293 epoch total loss 1.4205029\n",
      "Trained batch 42 batch loss 1.54640865 epoch total loss 1.42350066\n",
      "Trained batch 43 batch loss 1.48589659 epoch total loss 1.42495179\n",
      "Trained batch 44 batch loss 1.43338585 epoch total loss 1.42514348\n",
      "Trained batch 45 batch loss 1.49037731 epoch total loss 1.42659318\n",
      "Trained batch 46 batch loss 1.46740103 epoch total loss 1.42748034\n",
      "Trained batch 47 batch loss 1.39740026 epoch total loss 1.42684031\n",
      "Trained batch 48 batch loss 1.35121369 epoch total loss 1.42526472\n",
      "Trained batch 49 batch loss 1.30209112 epoch total loss 1.42275095\n",
      "Trained batch 50 batch loss 1.44042492 epoch total loss 1.42310441\n",
      "Trained batch 51 batch loss 1.44610262 epoch total loss 1.42355537\n",
      "Trained batch 52 batch loss 1.54696286 epoch total loss 1.42592871\n",
      "Trained batch 53 batch loss 1.64147711 epoch total loss 1.42999566\n",
      "Trained batch 54 batch loss 1.47989416 epoch total loss 1.43091977\n",
      "Trained batch 55 batch loss 1.55197155 epoch total loss 1.43312073\n",
      "Trained batch 56 batch loss 1.456532 epoch total loss 1.43353879\n",
      "Trained batch 57 batch loss 1.35662055 epoch total loss 1.43218946\n",
      "Trained batch 58 batch loss 1.31698084 epoch total loss 1.43020296\n",
      "Trained batch 59 batch loss 1.41055536 epoch total loss 1.42987\n",
      "Trained batch 60 batch loss 1.45673537 epoch total loss 1.43031764\n",
      "Trained batch 61 batch loss 1.4836607 epoch total loss 1.43119216\n",
      "Trained batch 62 batch loss 1.47390056 epoch total loss 1.43188095\n",
      "Trained batch 63 batch loss 1.4161427 epoch total loss 1.43163121\n",
      "Trained batch 64 batch loss 1.42594373 epoch total loss 1.43154228\n",
      "Trained batch 65 batch loss 1.38458991 epoch total loss 1.43082\n",
      "Trained batch 66 batch loss 1.46148658 epoch total loss 1.43128455\n",
      "Trained batch 67 batch loss 1.40016472 epoch total loss 1.43082011\n",
      "Trained batch 68 batch loss 1.35357285 epoch total loss 1.42968416\n",
      "Trained batch 69 batch loss 1.40963387 epoch total loss 1.42939365\n",
      "Trained batch 70 batch loss 1.33578098 epoch total loss 1.42805624\n",
      "Trained batch 71 batch loss 1.32127464 epoch total loss 1.4265523\n",
      "Trained batch 72 batch loss 1.40898216 epoch total loss 1.42630816\n",
      "Trained batch 73 batch loss 1.25991726 epoch total loss 1.42402887\n",
      "Trained batch 74 batch loss 1.29513979 epoch total loss 1.42228723\n",
      "Trained batch 75 batch loss 1.03000653 epoch total loss 1.4170568\n",
      "Trained batch 76 batch loss 1.24803901 epoch total loss 1.41483283\n",
      "Trained batch 77 batch loss 1.5309968 epoch total loss 1.41634154\n",
      "Trained batch 78 batch loss 1.5060029 epoch total loss 1.41749108\n",
      "Trained batch 79 batch loss 1.54032636 epoch total loss 1.41904593\n",
      "Trained batch 80 batch loss 1.454988 epoch total loss 1.41949522\n",
      "Trained batch 81 batch loss 1.483024 epoch total loss 1.4202795\n",
      "Trained batch 82 batch loss 1.27716088 epoch total loss 1.41853416\n",
      "Trained batch 83 batch loss 1.26965606 epoch total loss 1.41674042\n",
      "Trained batch 84 batch loss 1.37530053 epoch total loss 1.41624701\n",
      "Trained batch 85 batch loss 1.34306693 epoch total loss 1.41538608\n",
      "Trained batch 86 batch loss 1.42747259 epoch total loss 1.41552663\n",
      "Trained batch 87 batch loss 1.3317616 epoch total loss 1.41456389\n",
      "Trained batch 88 batch loss 1.36360109 epoch total loss 1.41398478\n",
      "Trained batch 89 batch loss 1.42696857 epoch total loss 1.41413069\n",
      "Trained batch 90 batch loss 1.34992886 epoch total loss 1.41341734\n",
      "Trained batch 91 batch loss 1.36725533 epoch total loss 1.41291\n",
      "Trained batch 92 batch loss 1.38803744 epoch total loss 1.41263962\n",
      "Trained batch 93 batch loss 1.3376019 epoch total loss 1.41183269\n",
      "Trained batch 94 batch loss 1.38373208 epoch total loss 1.41153371\n",
      "Trained batch 95 batch loss 1.37755358 epoch total loss 1.41117597\n",
      "Trained batch 96 batch loss 1.3899343 epoch total loss 1.41095483\n",
      "Trained batch 97 batch loss 1.36980903 epoch total loss 1.41053069\n",
      "Trained batch 98 batch loss 1.39292324 epoch total loss 1.41035104\n",
      "Trained batch 99 batch loss 1.27130532 epoch total loss 1.40894651\n",
      "Trained batch 100 batch loss 1.41083312 epoch total loss 1.40896535\n",
      "Trained batch 101 batch loss 1.63639736 epoch total loss 1.41121709\n",
      "Trained batch 102 batch loss 1.41432357 epoch total loss 1.41124761\n",
      "Trained batch 103 batch loss 1.42346907 epoch total loss 1.41136611\n",
      "Trained batch 104 batch loss 1.64705372 epoch total loss 1.41363227\n",
      "Trained batch 105 batch loss 1.69116426 epoch total loss 1.4162755\n",
      "Trained batch 106 batch loss 1.41333103 epoch total loss 1.41624773\n",
      "Trained batch 107 batch loss 1.56516039 epoch total loss 1.41763937\n",
      "Trained batch 108 batch loss 1.46556735 epoch total loss 1.41808307\n",
      "Trained batch 109 batch loss 1.51872289 epoch total loss 1.41900635\n",
      "Trained batch 110 batch loss 1.60799456 epoch total loss 1.42072451\n",
      "Trained batch 111 batch loss 1.44651508 epoch total loss 1.42095685\n",
      "Trained batch 112 batch loss 1.29859793 epoch total loss 1.41986442\n",
      "Trained batch 113 batch loss 1.37563705 epoch total loss 1.41947305\n",
      "Trained batch 114 batch loss 1.35149133 epoch total loss 1.41887665\n",
      "Trained batch 115 batch loss 1.461285 epoch total loss 1.41924548\n",
      "Trained batch 116 batch loss 1.41814637 epoch total loss 1.41923606\n",
      "Trained batch 117 batch loss 1.5158844 epoch total loss 1.42006207\n",
      "Trained batch 118 batch loss 1.38360357 epoch total loss 1.41975307\n",
      "Trained batch 119 batch loss 1.51603842 epoch total loss 1.42056227\n",
      "Trained batch 120 batch loss 1.60322666 epoch total loss 1.42208445\n",
      "Trained batch 121 batch loss 1.44525671 epoch total loss 1.4222759\n",
      "Trained batch 122 batch loss 1.42118442 epoch total loss 1.42226696\n",
      "Trained batch 123 batch loss 1.37223458 epoch total loss 1.42186022\n",
      "Trained batch 124 batch loss 1.3652612 epoch total loss 1.42140377\n",
      "Trained batch 125 batch loss 1.46575439 epoch total loss 1.42175865\n",
      "Trained batch 126 batch loss 1.42307591 epoch total loss 1.42176914\n",
      "Trained batch 127 batch loss 1.37741375 epoch total loss 1.42141986\n",
      "Trained batch 128 batch loss 1.39555383 epoch total loss 1.4212178\n",
      "Trained batch 129 batch loss 1.31423414 epoch total loss 1.42038846\n",
      "Trained batch 130 batch loss 1.36835849 epoch total loss 1.41998827\n",
      "Trained batch 131 batch loss 1.51649809 epoch total loss 1.42072499\n",
      "Trained batch 132 batch loss 1.54444611 epoch total loss 1.42166233\n",
      "Trained batch 133 batch loss 1.43140411 epoch total loss 1.42173553\n",
      "Trained batch 134 batch loss 1.43176341 epoch total loss 1.42181027\n",
      "Trained batch 135 batch loss 1.50215912 epoch total loss 1.4224056\n",
      "Trained batch 136 batch loss 1.48398 epoch total loss 1.42285824\n",
      "Trained batch 137 batch loss 1.43379426 epoch total loss 1.42293811\n",
      "Trained batch 138 batch loss 1.35999382 epoch total loss 1.42248201\n",
      "Trained batch 139 batch loss 1.29546583 epoch total loss 1.42156827\n",
      "Trained batch 140 batch loss 1.31526685 epoch total loss 1.42080891\n",
      "Trained batch 141 batch loss 1.25140405 epoch total loss 1.41960752\n",
      "Trained batch 142 batch loss 1.31121063 epoch total loss 1.4188441\n",
      "Trained batch 143 batch loss 1.23507226 epoch total loss 1.41755903\n",
      "Trained batch 144 batch loss 1.13922489 epoch total loss 1.41562605\n",
      "Trained batch 145 batch loss 1.19171512 epoch total loss 1.41408181\n",
      "Trained batch 146 batch loss 1.09444165 epoch total loss 1.41189253\n",
      "Trained batch 147 batch loss 1.33604097 epoch total loss 1.4113766\n",
      "Trained batch 148 batch loss 1.53190637 epoch total loss 1.41219091\n",
      "Trained batch 149 batch loss 1.39607716 epoch total loss 1.41208279\n",
      "Trained batch 150 batch loss 1.47154522 epoch total loss 1.41247916\n",
      "Trained batch 151 batch loss 1.46558535 epoch total loss 1.41283083\n",
      "Trained batch 152 batch loss 1.43116128 epoch total loss 1.41295147\n",
      "Trained batch 153 batch loss 1.43492234 epoch total loss 1.41309512\n",
      "Trained batch 154 batch loss 1.45472944 epoch total loss 1.41336548\n",
      "Trained batch 155 batch loss 1.45186722 epoch total loss 1.41361392\n",
      "Trained batch 156 batch loss 1.41810787 epoch total loss 1.41364264\n",
      "Trained batch 157 batch loss 1.59816408 epoch total loss 1.41481793\n",
      "Trained batch 158 batch loss 1.46582556 epoch total loss 1.41514075\n",
      "Trained batch 159 batch loss 1.37624919 epoch total loss 1.41489613\n",
      "Trained batch 160 batch loss 1.49856067 epoch total loss 1.4154191\n",
      "Trained batch 161 batch loss 1.51666379 epoch total loss 1.41604793\n",
      "Trained batch 162 batch loss 1.4883101 epoch total loss 1.41649401\n",
      "Trained batch 163 batch loss 1.45364451 epoch total loss 1.41672194\n",
      "Trained batch 164 batch loss 1.30235267 epoch total loss 1.41602457\n",
      "Trained batch 165 batch loss 1.377 epoch total loss 1.41578805\n",
      "Trained batch 166 batch loss 1.40408313 epoch total loss 1.41571748\n",
      "Trained batch 167 batch loss 1.41383708 epoch total loss 1.41570628\n",
      "Trained batch 168 batch loss 1.44237721 epoch total loss 1.41586506\n",
      "Trained batch 169 batch loss 1.30662167 epoch total loss 1.41521859\n",
      "Trained batch 170 batch loss 1.33929086 epoch total loss 1.41477203\n",
      "Trained batch 171 batch loss 1.38445878 epoch total loss 1.41459477\n",
      "Trained batch 172 batch loss 1.32750165 epoch total loss 1.41408837\n",
      "Trained batch 173 batch loss 1.35832119 epoch total loss 1.41376603\n",
      "Trained batch 174 batch loss 1.42089558 epoch total loss 1.41380703\n",
      "Trained batch 175 batch loss 1.39259577 epoch total loss 1.4136858\n",
      "Trained batch 176 batch loss 1.47741652 epoch total loss 1.41404796\n",
      "Trained batch 177 batch loss 1.54726827 epoch total loss 1.41480064\n",
      "Trained batch 178 batch loss 1.40364814 epoch total loss 1.41473794\n",
      "Trained batch 179 batch loss 1.38985133 epoch total loss 1.41459882\n",
      "Trained batch 180 batch loss 1.49911261 epoch total loss 1.41506839\n",
      "Trained batch 181 batch loss 1.49711573 epoch total loss 1.41552162\n",
      "Trained batch 182 batch loss 1.44970024 epoch total loss 1.4157095\n",
      "Trained batch 183 batch loss 1.45599771 epoch total loss 1.41592956\n",
      "Trained batch 184 batch loss 1.48296475 epoch total loss 1.41629398\n",
      "Trained batch 185 batch loss 1.44882178 epoch total loss 1.41646981\n",
      "Trained batch 186 batch loss 1.47784388 epoch total loss 1.41679978\n",
      "Trained batch 187 batch loss 1.51604 epoch total loss 1.4173305\n",
      "Trained batch 188 batch loss 1.45381379 epoch total loss 1.41752458\n",
      "Trained batch 189 batch loss 1.39670181 epoch total loss 1.41741443\n",
      "Trained batch 190 batch loss 1.38218963 epoch total loss 1.41722906\n",
      "Trained batch 191 batch loss 1.44275224 epoch total loss 1.41736269\n",
      "Trained batch 192 batch loss 1.46459591 epoch total loss 1.41760874\n",
      "Trained batch 193 batch loss 1.39370418 epoch total loss 1.41748488\n",
      "Trained batch 194 batch loss 1.26418376 epoch total loss 1.41669476\n",
      "Trained batch 195 batch loss 1.34839773 epoch total loss 1.4163444\n",
      "Trained batch 196 batch loss 1.40185142 epoch total loss 1.41627049\n",
      "Trained batch 197 batch loss 1.38555741 epoch total loss 1.41611457\n",
      "Trained batch 198 batch loss 1.25945175 epoch total loss 1.41532338\n",
      "Trained batch 199 batch loss 1.24270868 epoch total loss 1.41445601\n",
      "Trained batch 200 batch loss 1.24158657 epoch total loss 1.41359162\n",
      "Trained batch 201 batch loss 1.4345305 epoch total loss 1.41369581\n",
      "Trained batch 202 batch loss 1.7237699 epoch total loss 1.41523075\n",
      "Trained batch 203 batch loss 1.44918156 epoch total loss 1.415398\n",
      "Trained batch 204 batch loss 1.36330593 epoch total loss 1.41514277\n",
      "Trained batch 205 batch loss 1.48184788 epoch total loss 1.4154681\n",
      "Trained batch 206 batch loss 1.39042521 epoch total loss 1.4153465\n",
      "Trained batch 207 batch loss 1.41881 epoch total loss 1.41536331\n",
      "Trained batch 208 batch loss 1.29541683 epoch total loss 1.41478658\n",
      "Trained batch 209 batch loss 1.42687821 epoch total loss 1.41484439\n",
      "Trained batch 210 batch loss 1.36199868 epoch total loss 1.41459274\n",
      "Trained batch 211 batch loss 1.45458019 epoch total loss 1.41478229\n",
      "Trained batch 212 batch loss 1.38071918 epoch total loss 1.41462159\n",
      "Trained batch 213 batch loss 1.45891702 epoch total loss 1.41482961\n",
      "Trained batch 214 batch loss 1.2913897 epoch total loss 1.41425276\n",
      "Trained batch 215 batch loss 1.40301204 epoch total loss 1.41420043\n",
      "Trained batch 216 batch loss 1.32954681 epoch total loss 1.41380858\n",
      "Trained batch 217 batch loss 1.36028922 epoch total loss 1.41356194\n",
      "Trained batch 218 batch loss 1.38534 epoch total loss 1.4134326\n",
      "Trained batch 219 batch loss 1.39873147 epoch total loss 1.41336548\n",
      "Trained batch 220 batch loss 1.3808713 epoch total loss 1.41321766\n",
      "Trained batch 221 batch loss 1.3215642 epoch total loss 1.41280293\n",
      "Trained batch 222 batch loss 1.60542083 epoch total loss 1.41367054\n",
      "Trained batch 223 batch loss 1.54490423 epoch total loss 1.41425908\n",
      "Trained batch 224 batch loss 1.5814333 epoch total loss 1.41500533\n",
      "Trained batch 225 batch loss 1.57673061 epoch total loss 1.41572404\n",
      "Trained batch 226 batch loss 1.48039544 epoch total loss 1.41601026\n",
      "Trained batch 227 batch loss 1.52305818 epoch total loss 1.41648185\n",
      "Trained batch 228 batch loss 1.50589681 epoch total loss 1.41687405\n",
      "Trained batch 229 batch loss 1.43171823 epoch total loss 1.4169389\n",
      "Trained batch 230 batch loss 1.43620801 epoch total loss 1.41702271\n",
      "Trained batch 231 batch loss 1.43037319 epoch total loss 1.4170804\n",
      "Trained batch 232 batch loss 1.35331333 epoch total loss 1.41680551\n",
      "Trained batch 233 batch loss 1.48145044 epoch total loss 1.41708291\n",
      "Trained batch 234 batch loss 1.51192355 epoch total loss 1.41748834\n",
      "Trained batch 235 batch loss 1.43994069 epoch total loss 1.41758382\n",
      "Trained batch 236 batch loss 1.35919595 epoch total loss 1.41733646\n",
      "Trained batch 237 batch loss 1.38850749 epoch total loss 1.41721487\n",
      "Trained batch 238 batch loss 1.35615754 epoch total loss 1.41695833\n",
      "Trained batch 239 batch loss 1.32716715 epoch total loss 1.4165827\n",
      "Trained batch 240 batch loss 1.37428474 epoch total loss 1.41640651\n",
      "Trained batch 241 batch loss 1.5183351 epoch total loss 1.41682947\n",
      "Trained batch 242 batch loss 1.46199203 epoch total loss 1.41701615\n",
      "Trained batch 243 batch loss 1.35669219 epoch total loss 1.41676784\n",
      "Trained batch 244 batch loss 1.37900126 epoch total loss 1.4166131\n",
      "Trained batch 245 batch loss 1.45112789 epoch total loss 1.41675401\n",
      "Trained batch 246 batch loss 1.40045488 epoch total loss 1.41668773\n",
      "Trained batch 247 batch loss 1.44749689 epoch total loss 1.41681254\n",
      "Trained batch 248 batch loss 1.45995593 epoch total loss 1.41698658\n",
      "Trained batch 249 batch loss 1.30363119 epoch total loss 1.41653121\n",
      "Trained batch 250 batch loss 1.49154043 epoch total loss 1.41683125\n",
      "Trained batch 251 batch loss 1.54855978 epoch total loss 1.41735613\n",
      "Trained batch 252 batch loss 1.32292795 epoch total loss 1.41698146\n",
      "Trained batch 253 batch loss 1.44570208 epoch total loss 1.41709495\n",
      "Trained batch 254 batch loss 1.26572788 epoch total loss 1.41649902\n",
      "Trained batch 255 batch loss 1.33468461 epoch total loss 1.41617811\n",
      "Trained batch 256 batch loss 1.40326905 epoch total loss 1.41612768\n",
      "Trained batch 257 batch loss 1.35339332 epoch total loss 1.41588354\n",
      "Trained batch 258 batch loss 1.37496591 epoch total loss 1.41572499\n",
      "Trained batch 259 batch loss 1.40537906 epoch total loss 1.41568506\n",
      "Trained batch 260 batch loss 1.4397316 epoch total loss 1.41577744\n",
      "Trained batch 261 batch loss 1.45012641 epoch total loss 1.41590905\n",
      "Trained batch 262 batch loss 1.43875146 epoch total loss 1.41599631\n",
      "Trained batch 263 batch loss 1.38637495 epoch total loss 1.41588366\n",
      "Trained batch 264 batch loss 1.43503034 epoch total loss 1.41595626\n",
      "Trained batch 265 batch loss 1.34955835 epoch total loss 1.41570556\n",
      "Trained batch 266 batch loss 1.35594261 epoch total loss 1.41548097\n",
      "Trained batch 267 batch loss 1.4139905 epoch total loss 1.41547549\n",
      "Trained batch 268 batch loss 1.44582236 epoch total loss 1.41558874\n",
      "Trained batch 269 batch loss 1.38985491 epoch total loss 1.41549313\n",
      "Trained batch 270 batch loss 1.40248895 epoch total loss 1.41544497\n",
      "Trained batch 271 batch loss 1.41213632 epoch total loss 1.41543269\n",
      "Trained batch 272 batch loss 1.49643588 epoch total loss 1.41573048\n",
      "Trained batch 273 batch loss 1.50998187 epoch total loss 1.41607571\n",
      "Trained batch 274 batch loss 1.48242939 epoch total loss 1.41631794\n",
      "Trained batch 275 batch loss 1.4433068 epoch total loss 1.41641605\n",
      "Trained batch 276 batch loss 1.40428078 epoch total loss 1.41637194\n",
      "Trained batch 277 batch loss 1.36062682 epoch total loss 1.41617072\n",
      "Trained batch 278 batch loss 1.39516389 epoch total loss 1.41609526\n",
      "Trained batch 279 batch loss 1.28537476 epoch total loss 1.41562665\n",
      "Trained batch 280 batch loss 1.33122694 epoch total loss 1.41532528\n",
      "Trained batch 281 batch loss 1.37805295 epoch total loss 1.4151926\n",
      "Trained batch 282 batch loss 1.34895611 epoch total loss 1.41495776\n",
      "Trained batch 283 batch loss 1.39575386 epoch total loss 1.41488993\n",
      "Trained batch 284 batch loss 1.37192857 epoch total loss 1.41473866\n",
      "Trained batch 285 batch loss 1.43633807 epoch total loss 1.41481435\n",
      "Trained batch 286 batch loss 1.36438322 epoch total loss 1.41463804\n",
      "Trained batch 287 batch loss 1.388273 epoch total loss 1.41454625\n",
      "Trained batch 288 batch loss 1.29499698 epoch total loss 1.41413105\n",
      "Trained batch 289 batch loss 1.33320391 epoch total loss 1.41385102\n",
      "Trained batch 290 batch loss 1.18007624 epoch total loss 1.41304493\n",
      "Trained batch 291 batch loss 1.36970794 epoch total loss 1.41289604\n",
      "Trained batch 292 batch loss 1.39365935 epoch total loss 1.41283011\n",
      "Trained batch 293 batch loss 1.43472838 epoch total loss 1.41290486\n",
      "Trained batch 294 batch loss 1.31450438 epoch total loss 1.41257012\n",
      "Trained batch 295 batch loss 1.37992978 epoch total loss 1.41245949\n",
      "Trained batch 296 batch loss 1.34029579 epoch total loss 1.41221583\n",
      "Trained batch 297 batch loss 1.42692196 epoch total loss 1.4122653\n",
      "Trained batch 298 batch loss 1.46364045 epoch total loss 1.41243768\n",
      "Trained batch 299 batch loss 1.44737959 epoch total loss 1.41255462\n",
      "Trained batch 300 batch loss 1.33234608 epoch total loss 1.41228724\n",
      "Trained batch 301 batch loss 1.23489523 epoch total loss 1.41169786\n",
      "Trained batch 302 batch loss 1.25755858 epoch total loss 1.41118753\n",
      "Trained batch 303 batch loss 1.1530869 epoch total loss 1.41033566\n",
      "Trained batch 304 batch loss 1.24352241 epoch total loss 1.40978694\n",
      "Trained batch 305 batch loss 1.28312778 epoch total loss 1.40937173\n",
      "Trained batch 306 batch loss 1.33890033 epoch total loss 1.40914142\n",
      "Trained batch 307 batch loss 1.44955587 epoch total loss 1.40927303\n",
      "Trained batch 308 batch loss 1.44207227 epoch total loss 1.40937948\n",
      "Trained batch 309 batch loss 1.37948823 epoch total loss 1.4092828\n",
      "Trained batch 310 batch loss 1.38032627 epoch total loss 1.40918946\n",
      "Trained batch 311 batch loss 1.29867733 epoch total loss 1.4088341\n",
      "Trained batch 312 batch loss 1.29005432 epoch total loss 1.40845335\n",
      "Trained batch 313 batch loss 1.40617299 epoch total loss 1.40844595\n",
      "Trained batch 314 batch loss 1.39776278 epoch total loss 1.40841198\n",
      "Trained batch 315 batch loss 1.33503532 epoch total loss 1.40817904\n",
      "Trained batch 316 batch loss 1.24652731 epoch total loss 1.4076674\n",
      "Trained batch 317 batch loss 1.38906097 epoch total loss 1.40760875\n",
      "Trained batch 318 batch loss 1.50657904 epoch total loss 1.40792\n",
      "Trained batch 319 batch loss 1.51276445 epoch total loss 1.40824866\n",
      "Trained batch 320 batch loss 1.38667774 epoch total loss 1.40818131\n",
      "Trained batch 321 batch loss 1.28882277 epoch total loss 1.4078095\n",
      "Trained batch 322 batch loss 1.31189978 epoch total loss 1.40751159\n",
      "Trained batch 323 batch loss 1.27549815 epoch total loss 1.40710294\n",
      "Trained batch 324 batch loss 1.26953328 epoch total loss 1.40667832\n",
      "Trained batch 325 batch loss 1.34950972 epoch total loss 1.40650237\n",
      "Trained batch 326 batch loss 1.35190392 epoch total loss 1.40633488\n",
      "Trained batch 327 batch loss 1.32577455 epoch total loss 1.40608859\n",
      "Trained batch 328 batch loss 1.46656275 epoch total loss 1.40627289\n",
      "Trained batch 329 batch loss 1.40948415 epoch total loss 1.40628266\n",
      "Trained batch 330 batch loss 1.46843028 epoch total loss 1.40647101\n",
      "Trained batch 331 batch loss 1.61512136 epoch total loss 1.40710139\n",
      "Trained batch 332 batch loss 1.51656294 epoch total loss 1.40743113\n",
      "Trained batch 333 batch loss 1.43167043 epoch total loss 1.40750384\n",
      "Trained batch 334 batch loss 1.30032873 epoch total loss 1.40718293\n",
      "Trained batch 335 batch loss 1.36856246 epoch total loss 1.40706766\n",
      "Trained batch 336 batch loss 1.42801106 epoch total loss 1.40713\n",
      "Trained batch 337 batch loss 1.42627168 epoch total loss 1.40718687\n",
      "Trained batch 338 batch loss 1.4957068 epoch total loss 1.40744865\n",
      "Trained batch 339 batch loss 1.48185587 epoch total loss 1.40766811\n",
      "Trained batch 340 batch loss 1.49374342 epoch total loss 1.40792131\n",
      "Trained batch 341 batch loss 1.54893219 epoch total loss 1.40833473\n",
      "Trained batch 342 batch loss 1.55100799 epoch total loss 1.40875196\n",
      "Trained batch 343 batch loss 1.48353112 epoch total loss 1.40896988\n",
      "Trained batch 344 batch loss 1.54440188 epoch total loss 1.40936363\n",
      "Trained batch 345 batch loss 1.52416015 epoch total loss 1.40969634\n",
      "Trained batch 346 batch loss 1.51561189 epoch total loss 1.41000247\n",
      "Trained batch 347 batch loss 1.35315216 epoch total loss 1.40983868\n",
      "Trained batch 348 batch loss 1.41223335 epoch total loss 1.40984559\n",
      "Trained batch 349 batch loss 1.36015236 epoch total loss 1.40970314\n",
      "Trained batch 350 batch loss 1.48654699 epoch total loss 1.40992272\n",
      "Trained batch 351 batch loss 1.422683 epoch total loss 1.40995896\n",
      "Trained batch 352 batch loss 1.35141182 epoch total loss 1.40979266\n",
      "Trained batch 353 batch loss 1.33546913 epoch total loss 1.40958214\n",
      "Trained batch 354 batch loss 1.42290759 epoch total loss 1.40961981\n",
      "Trained batch 355 batch loss 1.52995372 epoch total loss 1.40995884\n",
      "Trained batch 356 batch loss 1.44835162 epoch total loss 1.41006672\n",
      "Trained batch 357 batch loss 1.18936527 epoch total loss 1.4094485\n",
      "Trained batch 358 batch loss 1.14125943 epoch total loss 1.40869939\n",
      "Trained batch 359 batch loss 1.41837764 epoch total loss 1.40872622\n",
      "Trained batch 360 batch loss 1.28330088 epoch total loss 1.40837789\n",
      "Trained batch 361 batch loss 1.34398651 epoch total loss 1.40819955\n",
      "Trained batch 362 batch loss 1.32676423 epoch total loss 1.40797448\n",
      "Trained batch 363 batch loss 1.43326604 epoch total loss 1.4080441\n",
      "Trained batch 364 batch loss 1.40393543 epoch total loss 1.40803289\n",
      "Trained batch 365 batch loss 1.49973881 epoch total loss 1.40828419\n",
      "Trained batch 366 batch loss 1.4398545 epoch total loss 1.40837061\n",
      "Trained batch 367 batch loss 1.6648742 epoch total loss 1.40906942\n",
      "Trained batch 368 batch loss 1.59930384 epoch total loss 1.40958631\n",
      "Trained batch 369 batch loss 1.56927991 epoch total loss 1.41001916\n",
      "Trained batch 370 batch loss 1.46418929 epoch total loss 1.41016543\n",
      "Trained batch 371 batch loss 1.44233239 epoch total loss 1.41025221\n",
      "Trained batch 372 batch loss 1.42543697 epoch total loss 1.41029286\n",
      "Trained batch 373 batch loss 1.50376844 epoch total loss 1.41054356\n",
      "Trained batch 374 batch loss 1.41366053 epoch total loss 1.41055179\n",
      "Trained batch 375 batch loss 1.43674827 epoch total loss 1.41062176\n",
      "Trained batch 376 batch loss 1.31776595 epoch total loss 1.41037476\n",
      "Trained batch 377 batch loss 1.38032985 epoch total loss 1.41029501\n",
      "Trained batch 378 batch loss 1.389961 epoch total loss 1.41024113\n",
      "Trained batch 379 batch loss 1.45939159 epoch total loss 1.41037095\n",
      "Trained batch 380 batch loss 1.45401764 epoch total loss 1.41048586\n",
      "Trained batch 381 batch loss 1.50670743 epoch total loss 1.41073835\n",
      "Trained batch 382 batch loss 1.45021391 epoch total loss 1.4108417\n",
      "Trained batch 383 batch loss 1.47007358 epoch total loss 1.41099644\n",
      "Trained batch 384 batch loss 1.51395023 epoch total loss 1.41126454\n",
      "Trained batch 385 batch loss 1.42865634 epoch total loss 1.41130972\n",
      "Trained batch 386 batch loss 1.45317566 epoch total loss 1.4114182\n",
      "Trained batch 387 batch loss 1.34196 epoch total loss 1.41123879\n",
      "Trained batch 388 batch loss 1.36952555 epoch total loss 1.41113126\n",
      "Trained batch 389 batch loss 1.36681914 epoch total loss 1.4110173\n",
      "Trained batch 390 batch loss 1.44572175 epoch total loss 1.41110635\n",
      "Trained batch 391 batch loss 1.38328075 epoch total loss 1.4110353\n",
      "Trained batch 392 batch loss 1.48923731 epoch total loss 1.41123486\n",
      "Trained batch 393 batch loss 1.41675091 epoch total loss 1.4112488\n",
      "Trained batch 394 batch loss 1.44937038 epoch total loss 1.41134548\n",
      "Trained batch 395 batch loss 1.5804472 epoch total loss 1.41177356\n",
      "Trained batch 396 batch loss 1.52526617 epoch total loss 1.41206026\n",
      "Trained batch 397 batch loss 1.49064827 epoch total loss 1.41225815\n",
      "Trained batch 398 batch loss 1.40857184 epoch total loss 1.41224897\n",
      "Trained batch 399 batch loss 1.25976455 epoch total loss 1.41186678\n",
      "Trained batch 400 batch loss 1.15330362 epoch total loss 1.41122043\n",
      "Trained batch 401 batch loss 1.27170491 epoch total loss 1.41087258\n",
      "Trained batch 402 batch loss 1.36160183 epoch total loss 1.41074991\n",
      "Trained batch 403 batch loss 1.20676613 epoch total loss 1.41024375\n",
      "Trained batch 404 batch loss 1.09845948 epoch total loss 1.40947199\n",
      "Trained batch 405 batch loss 1.17650247 epoch total loss 1.4088968\n",
      "Trained batch 406 batch loss 1.22349608 epoch total loss 1.40844023\n",
      "Trained batch 407 batch loss 1.34991848 epoch total loss 1.40829635\n",
      "Trained batch 408 batch loss 1.35012674 epoch total loss 1.40815377\n",
      "Trained batch 409 batch loss 1.41038406 epoch total loss 1.40815926\n",
      "Trained batch 410 batch loss 1.49246502 epoch total loss 1.40836489\n",
      "Trained batch 411 batch loss 1.4815805 epoch total loss 1.40854311\n",
      "Trained batch 412 batch loss 1.46772909 epoch total loss 1.40868664\n",
      "Trained batch 413 batch loss 1.43590975 epoch total loss 1.40875256\n",
      "Trained batch 414 batch loss 1.34864795 epoch total loss 1.40860736\n",
      "Trained batch 415 batch loss 1.33809626 epoch total loss 1.40843737\n",
      "Trained batch 416 batch loss 1.39694786 epoch total loss 1.40840983\n",
      "Trained batch 417 batch loss 1.30928254 epoch total loss 1.40817213\n",
      "Trained batch 418 batch loss 1.26379347 epoch total loss 1.40782666\n",
      "Trained batch 419 batch loss 1.31475401 epoch total loss 1.40760458\n",
      "Trained batch 420 batch loss 1.35907078 epoch total loss 1.40748906\n",
      "Trained batch 421 batch loss 1.37828338 epoch total loss 1.40741968\n",
      "Trained batch 422 batch loss 1.42139757 epoch total loss 1.40745282\n",
      "Trained batch 423 batch loss 1.37949133 epoch total loss 1.40738678\n",
      "Trained batch 424 batch loss 1.28404665 epoch total loss 1.40709591\n",
      "Trained batch 425 batch loss 1.3580209 epoch total loss 1.4069804\n",
      "Trained batch 426 batch loss 1.37520802 epoch total loss 1.40690577\n",
      "Trained batch 427 batch loss 1.42949402 epoch total loss 1.4069587\n",
      "Trained batch 428 batch loss 1.40596175 epoch total loss 1.40695632\n",
      "Trained batch 429 batch loss 1.39827323 epoch total loss 1.40693605\n",
      "Trained batch 430 batch loss 1.32618415 epoch total loss 1.40674818\n",
      "Trained batch 431 batch loss 1.34769964 epoch total loss 1.4066112\n",
      "Trained batch 432 batch loss 1.33447218 epoch total loss 1.40644431\n",
      "Trained batch 433 batch loss 1.28196239 epoch total loss 1.40615678\n",
      "Trained batch 434 batch loss 1.37128353 epoch total loss 1.40607643\n",
      "Trained batch 435 batch loss 1.44348383 epoch total loss 1.4061625\n",
      "Trained batch 436 batch loss 1.43499899 epoch total loss 1.40622854\n",
      "Trained batch 437 batch loss 1.37399089 epoch total loss 1.40615475\n",
      "Trained batch 438 batch loss 1.48699 epoch total loss 1.40633929\n",
      "Trained batch 439 batch loss 1.43673754 epoch total loss 1.40640867\n",
      "Trained batch 440 batch loss 1.44954801 epoch total loss 1.40650666\n",
      "Trained batch 441 batch loss 1.44883275 epoch total loss 1.40660262\n",
      "Trained batch 442 batch loss 1.38568163 epoch total loss 1.40655529\n",
      "Trained batch 443 batch loss 1.47659457 epoch total loss 1.40671349\n",
      "Trained batch 444 batch loss 1.3821311 epoch total loss 1.40665817\n",
      "Trained batch 445 batch loss 1.38422346 epoch total loss 1.40660775\n",
      "Trained batch 446 batch loss 1.3952589 epoch total loss 1.40658224\n",
      "Trained batch 447 batch loss 1.33431804 epoch total loss 1.40642059\n",
      "Trained batch 448 batch loss 1.33929682 epoch total loss 1.40627074\n",
      "Trained batch 449 batch loss 1.3956511 epoch total loss 1.40624702\n",
      "Trained batch 450 batch loss 1.32666802 epoch total loss 1.40607011\n",
      "Trained batch 451 batch loss 1.36698174 epoch total loss 1.40598357\n",
      "Trained batch 452 batch loss 1.45270872 epoch total loss 1.40608692\n",
      "Trained batch 453 batch loss 1.32893705 epoch total loss 1.40591657\n",
      "Trained batch 454 batch loss 1.5442872 epoch total loss 1.40622139\n",
      "Trained batch 455 batch loss 1.37593532 epoch total loss 1.40615475\n",
      "Trained batch 456 batch loss 1.39512646 epoch total loss 1.40613055\n",
      "Trained batch 457 batch loss 1.31288767 epoch total loss 1.40592659\n",
      "Trained batch 458 batch loss 1.31702769 epoch total loss 1.40573239\n",
      "Trained batch 459 batch loss 1.37291443 epoch total loss 1.40566087\n",
      "Trained batch 460 batch loss 1.34990048 epoch total loss 1.40553975\n",
      "Trained batch 461 batch loss 1.36745 epoch total loss 1.40545702\n",
      "Trained batch 462 batch loss 1.41197276 epoch total loss 1.40547121\n",
      "Trained batch 463 batch loss 1.4357543 epoch total loss 1.40553653\n",
      "Trained batch 464 batch loss 1.39887404 epoch total loss 1.40552223\n",
      "Trained batch 465 batch loss 1.49576473 epoch total loss 1.4057163\n",
      "Trained batch 466 batch loss 1.36798286 epoch total loss 1.40563536\n",
      "Trained batch 467 batch loss 1.32183516 epoch total loss 1.40545595\n",
      "Trained batch 468 batch loss 1.30201578 epoch total loss 1.40523481\n",
      "Trained batch 469 batch loss 1.2421037 epoch total loss 1.40488708\n",
      "Trained batch 470 batch loss 1.31668913 epoch total loss 1.40469944\n",
      "Trained batch 471 batch loss 1.46045196 epoch total loss 1.40481782\n",
      "Trained batch 472 batch loss 1.42712069 epoch total loss 1.40486503\n",
      "Trained batch 473 batch loss 1.34801042 epoch total loss 1.40474486\n",
      "Trained batch 474 batch loss 1.33306265 epoch total loss 1.40459371\n",
      "Trained batch 475 batch loss 1.42160022 epoch total loss 1.40462947\n",
      "Trained batch 476 batch loss 1.29931533 epoch total loss 1.40440822\n",
      "Trained batch 477 batch loss 1.26531315 epoch total loss 1.40411663\n",
      "Trained batch 478 batch loss 1.33403862 epoch total loss 1.40397\n",
      "Trained batch 479 batch loss 1.36275506 epoch total loss 1.40388393\n",
      "Trained batch 480 batch loss 1.47942865 epoch total loss 1.40404129\n",
      "Trained batch 481 batch loss 1.35811186 epoch total loss 1.4039458\n",
      "Trained batch 482 batch loss 1.43132436 epoch total loss 1.40400255\n",
      "Trained batch 483 batch loss 1.34007668 epoch total loss 1.40387022\n",
      "Trained batch 484 batch loss 1.39604938 epoch total loss 1.40385413\n",
      "Trained batch 485 batch loss 1.39319026 epoch total loss 1.40383208\n",
      "Trained batch 486 batch loss 1.47743177 epoch total loss 1.40398359\n",
      "Trained batch 487 batch loss 1.47196126 epoch total loss 1.40412319\n",
      "Trained batch 488 batch loss 1.41963661 epoch total loss 1.4041549\n",
      "Trained batch 489 batch loss 1.31468463 epoch total loss 1.40397203\n",
      "Trained batch 490 batch loss 1.3463707 epoch total loss 1.40385449\n",
      "Trained batch 491 batch loss 1.40646374 epoch total loss 1.40385985\n",
      "Trained batch 492 batch loss 1.42319822 epoch total loss 1.40389919\n",
      "Trained batch 493 batch loss 1.35778785 epoch total loss 1.40380561\n",
      "Trained batch 494 batch loss 1.51756489 epoch total loss 1.40403593\n",
      "Trained batch 495 batch loss 1.50022483 epoch total loss 1.40423024\n",
      "Trained batch 496 batch loss 1.43806803 epoch total loss 1.40429842\n",
      "Trained batch 497 batch loss 1.43709493 epoch total loss 1.40436447\n",
      "Trained batch 498 batch loss 1.39348555 epoch total loss 1.40434253\n",
      "Trained batch 499 batch loss 1.36686468 epoch total loss 1.40426755\n",
      "Trained batch 500 batch loss 1.57627213 epoch total loss 1.40461159\n",
      "Trained batch 501 batch loss 1.54070556 epoch total loss 1.40488327\n",
      "Trained batch 502 batch loss 1.52419114 epoch total loss 1.40512085\n",
      "Trained batch 503 batch loss 1.29251671 epoch total loss 1.40489697\n",
      "Trained batch 504 batch loss 1.40382957 epoch total loss 1.40489483\n",
      "Trained batch 505 batch loss 1.40209603 epoch total loss 1.40488935\n",
      "Trained batch 506 batch loss 1.33628726 epoch total loss 1.4047538\n",
      "Trained batch 507 batch loss 1.39526081 epoch total loss 1.40473509\n",
      "Trained batch 508 batch loss 1.41204822 epoch total loss 1.40474951\n",
      "Trained batch 509 batch loss 1.3628639 epoch total loss 1.40466714\n",
      "Trained batch 510 batch loss 1.4472599 epoch total loss 1.4047507\n",
      "Trained batch 511 batch loss 1.40628088 epoch total loss 1.40475368\n",
      "Trained batch 512 batch loss 1.37470484 epoch total loss 1.40469503\n",
      "Trained batch 513 batch loss 1.33622038 epoch total loss 1.40456164\n",
      "Trained batch 514 batch loss 1.30583894 epoch total loss 1.40436959\n",
      "Trained batch 515 batch loss 1.42032695 epoch total loss 1.40440059\n",
      "Trained batch 516 batch loss 1.31339967 epoch total loss 1.40422428\n",
      "Trained batch 517 batch loss 1.28864443 epoch total loss 1.40400064\n",
      "Trained batch 518 batch loss 1.3384788 epoch total loss 1.40387428\n",
      "Trained batch 519 batch loss 1.35199702 epoch total loss 1.40377426\n",
      "Trained batch 520 batch loss 1.47161508 epoch total loss 1.40390468\n",
      "Trained batch 521 batch loss 1.32579303 epoch total loss 1.40375483\n",
      "Trained batch 522 batch loss 1.35315466 epoch total loss 1.40365791\n",
      "Trained batch 523 batch loss 1.47341895 epoch total loss 1.40379119\n",
      "Trained batch 524 batch loss 1.43857217 epoch total loss 1.40385759\n",
      "Trained batch 525 batch loss 1.30929244 epoch total loss 1.40367746\n",
      "Trained batch 526 batch loss 1.30524492 epoch total loss 1.4034903\n",
      "Trained batch 527 batch loss 1.30405068 epoch total loss 1.40330172\n",
      "Trained batch 528 batch loss 1.40734577 epoch total loss 1.40330935\n",
      "Trained batch 529 batch loss 1.50800216 epoch total loss 1.40350723\n",
      "Trained batch 530 batch loss 1.41986489 epoch total loss 1.40353811\n",
      "Trained batch 531 batch loss 1.33840466 epoch total loss 1.40341532\n",
      "Trained batch 532 batch loss 1.44409418 epoch total loss 1.40349185\n",
      "Trained batch 533 batch loss 1.29315078 epoch total loss 1.40328479\n",
      "Trained batch 534 batch loss 1.34989667 epoch total loss 1.40318489\n",
      "Trained batch 535 batch loss 1.41224027 epoch total loss 1.40320182\n",
      "Trained batch 536 batch loss 1.45475078 epoch total loss 1.40329802\n",
      "Trained batch 537 batch loss 1.43477309 epoch total loss 1.40335655\n",
      "Trained batch 538 batch loss 1.51715195 epoch total loss 1.40356803\n",
      "Trained batch 539 batch loss 1.28886068 epoch total loss 1.40335536\n",
      "Trained batch 540 batch loss 1.36508274 epoch total loss 1.40328443\n",
      "Trained batch 541 batch loss 1.3181622 epoch total loss 1.40312719\n",
      "Trained batch 542 batch loss 1.49186575 epoch total loss 1.40329087\n",
      "Trained batch 543 batch loss 1.42049062 epoch total loss 1.40332258\n",
      "Trained batch 544 batch loss 1.40344703 epoch total loss 1.40332282\n",
      "Trained batch 545 batch loss 1.34228873 epoch total loss 1.40321076\n",
      "Trained batch 546 batch loss 1.40296948 epoch total loss 1.40321028\n",
      "Trained batch 547 batch loss 1.40063739 epoch total loss 1.40320563\n",
      "Trained batch 548 batch loss 1.38669991 epoch total loss 1.40317547\n",
      "Trained batch 549 batch loss 1.36099243 epoch total loss 1.40309858\n",
      "Trained batch 550 batch loss 1.37859654 epoch total loss 1.40305412\n",
      "Trained batch 551 batch loss 1.41616774 epoch total loss 1.40307784\n",
      "Trained batch 552 batch loss 1.45536041 epoch total loss 1.40317261\n",
      "Trained batch 553 batch loss 1.44482327 epoch total loss 1.40324795\n",
      "Trained batch 554 batch loss 1.46540654 epoch total loss 1.40336013\n",
      "Trained batch 555 batch loss 1.41939223 epoch total loss 1.40338898\n",
      "Trained batch 556 batch loss 1.3539803 epoch total loss 1.40330017\n",
      "Trained batch 557 batch loss 1.31091833 epoch total loss 1.40313423\n",
      "Trained batch 558 batch loss 1.41595531 epoch total loss 1.40315723\n",
      "Trained batch 559 batch loss 1.3502754 epoch total loss 1.40306258\n",
      "Trained batch 560 batch loss 1.35681772 epoch total loss 1.40298009\n",
      "Trained batch 561 batch loss 1.38657761 epoch total loss 1.40295088\n",
      "Trained batch 562 batch loss 1.37457943 epoch total loss 1.40290034\n",
      "Trained batch 563 batch loss 1.36014986 epoch total loss 1.4028244\n",
      "Trained batch 564 batch loss 1.51992321 epoch total loss 1.40303206\n",
      "Trained batch 565 batch loss 1.4663415 epoch total loss 1.40314412\n",
      "Trained batch 566 batch loss 1.47299945 epoch total loss 1.40326762\n",
      "Trained batch 567 batch loss 1.42941546 epoch total loss 1.40331376\n",
      "Trained batch 568 batch loss 1.48458779 epoch total loss 1.40345681\n",
      "Trained batch 569 batch loss 1.48530626 epoch total loss 1.40360057\n",
      "Trained batch 570 batch loss 1.36873651 epoch total loss 1.40353942\n",
      "Trained batch 571 batch loss 1.37065983 epoch total loss 1.40348184\n",
      "Trained batch 572 batch loss 1.41618323 epoch total loss 1.40350401\n",
      "Trained batch 573 batch loss 1.3065331 epoch total loss 1.40333486\n",
      "Trained batch 574 batch loss 1.35672593 epoch total loss 1.40325367\n",
      "Trained batch 575 batch loss 1.42522883 epoch total loss 1.40329182\n",
      "Trained batch 576 batch loss 1.47846472 epoch total loss 1.40342236\n",
      "Trained batch 577 batch loss 1.45967245 epoch total loss 1.40351987\n",
      "Trained batch 578 batch loss 1.45038426 epoch total loss 1.40360093\n",
      "Trained batch 579 batch loss 1.51688647 epoch total loss 1.40379655\n",
      "Trained batch 580 batch loss 1.33877623 epoch total loss 1.4036845\n",
      "Trained batch 581 batch loss 1.33086967 epoch total loss 1.40355921\n",
      "Trained batch 582 batch loss 1.35539174 epoch total loss 1.40347648\n",
      "Trained batch 583 batch loss 1.36115634 epoch total loss 1.40340388\n",
      "Trained batch 584 batch loss 1.47152805 epoch total loss 1.40352058\n",
      "Trained batch 585 batch loss 1.32314575 epoch total loss 1.40338314\n",
      "Trained batch 586 batch loss 1.30630064 epoch total loss 1.40321743\n",
      "Trained batch 587 batch loss 1.27175164 epoch total loss 1.40299344\n",
      "Trained batch 588 batch loss 1.34234631 epoch total loss 1.40289021\n",
      "Trained batch 589 batch loss 1.25849903 epoch total loss 1.40264511\n",
      "Trained batch 590 batch loss 1.41562462 epoch total loss 1.40266716\n",
      "Trained batch 591 batch loss 1.40745687 epoch total loss 1.40267527\n",
      "Trained batch 592 batch loss 1.41746938 epoch total loss 1.4027003\n",
      "Trained batch 593 batch loss 1.33373284 epoch total loss 1.40258396\n",
      "Trained batch 594 batch loss 1.44644451 epoch total loss 1.40265787\n",
      "Trained batch 595 batch loss 1.31983018 epoch total loss 1.40251863\n",
      "Trained batch 596 batch loss 1.50177741 epoch total loss 1.40268517\n",
      "Trained batch 597 batch loss 1.41316986 epoch total loss 1.40270269\n",
      "Trained batch 598 batch loss 1.32932627 epoch total loss 1.40258\n",
      "Trained batch 599 batch loss 1.28527379 epoch total loss 1.40238416\n",
      "Trained batch 600 batch loss 1.31734395 epoch total loss 1.40224242\n",
      "Trained batch 601 batch loss 1.42107439 epoch total loss 1.40227377\n",
      "Trained batch 602 batch loss 1.40686417 epoch total loss 1.4022814\n",
      "Trained batch 603 batch loss 1.45761096 epoch total loss 1.40237308\n",
      "Trained batch 604 batch loss 1.42945457 epoch total loss 1.4024179\n",
      "Trained batch 605 batch loss 1.33459425 epoch total loss 1.40230584\n",
      "Trained batch 606 batch loss 1.36694431 epoch total loss 1.40224743\n",
      "Trained batch 607 batch loss 1.38362885 epoch total loss 1.40221679\n",
      "Trained batch 608 batch loss 1.32697248 epoch total loss 1.40209293\n",
      "Trained batch 609 batch loss 1.33013046 epoch total loss 1.4019748\n",
      "Trained batch 610 batch loss 1.43433726 epoch total loss 1.40202785\n",
      "Trained batch 611 batch loss 1.40587938 epoch total loss 1.40203416\n",
      "Trained batch 612 batch loss 1.45372248 epoch total loss 1.40211868\n",
      "Trained batch 613 batch loss 1.33016658 epoch total loss 1.40200126\n",
      "Trained batch 614 batch loss 1.29885077 epoch total loss 1.40183318\n",
      "Trained batch 615 batch loss 1.35284281 epoch total loss 1.40175354\n",
      "Trained batch 616 batch loss 1.25555301 epoch total loss 1.4015162\n",
      "Trained batch 617 batch loss 1.26523328 epoch total loss 1.40129542\n",
      "Trained batch 618 batch loss 1.2657392 epoch total loss 1.40107608\n",
      "Trained batch 619 batch loss 1.26611876 epoch total loss 1.40085804\n",
      "Trained batch 620 batch loss 1.38719559 epoch total loss 1.40083599\n",
      "Trained batch 621 batch loss 1.16693807 epoch total loss 1.40045929\n",
      "Trained batch 622 batch loss 1.35279465 epoch total loss 1.40038264\n",
      "Trained batch 623 batch loss 1.36332917 epoch total loss 1.40032327\n",
      "Trained batch 624 batch loss 1.2877878 epoch total loss 1.40014291\n",
      "Trained batch 625 batch loss 1.3394562 epoch total loss 1.40004575\n",
      "Trained batch 626 batch loss 1.38216233 epoch total loss 1.40001714\n",
      "Trained batch 627 batch loss 1.43297935 epoch total loss 1.40006983\n",
      "Trained batch 628 batch loss 1.35851717 epoch total loss 1.40000367\n",
      "Trained batch 629 batch loss 1.32799482 epoch total loss 1.39988911\n",
      "Trained batch 630 batch loss 1.43945217 epoch total loss 1.39995193\n",
      "Trained batch 631 batch loss 1.44183874 epoch total loss 1.40001833\n",
      "Trained batch 632 batch loss 1.27655828 epoch total loss 1.39982295\n",
      "Trained batch 633 batch loss 1.29009509 epoch total loss 1.39964962\n",
      "Trained batch 634 batch loss 1.3130455 epoch total loss 1.39951301\n",
      "Trained batch 635 batch loss 1.28952813 epoch total loss 1.39933991\n",
      "Trained batch 636 batch loss 1.22463548 epoch total loss 1.39906514\n",
      "Trained batch 637 batch loss 1.31238544 epoch total loss 1.398929\n",
      "Trained batch 638 batch loss 1.28902948 epoch total loss 1.39875674\n",
      "Trained batch 639 batch loss 1.33367085 epoch total loss 1.39865494\n",
      "Trained batch 640 batch loss 1.29807055 epoch total loss 1.39849782\n",
      "Trained batch 641 batch loss 1.29901195 epoch total loss 1.39834261\n",
      "Trained batch 642 batch loss 1.29973006 epoch total loss 1.39818895\n",
      "Trained batch 643 batch loss 1.43829656 epoch total loss 1.39825141\n",
      "Trained batch 644 batch loss 1.41357565 epoch total loss 1.39827514\n",
      "Trained batch 645 batch loss 1.47372174 epoch total loss 1.39839208\n",
      "Trained batch 646 batch loss 1.40461588 epoch total loss 1.39840174\n",
      "Trained batch 647 batch loss 1.45486665 epoch total loss 1.398489\n",
      "Trained batch 648 batch loss 1.4017837 epoch total loss 1.39849412\n",
      "Trained batch 649 batch loss 1.31971824 epoch total loss 1.39837265\n",
      "Trained batch 650 batch loss 1.40092695 epoch total loss 1.3983767\n",
      "Trained batch 651 batch loss 1.33302855 epoch total loss 1.39827621\n",
      "Trained batch 652 batch loss 1.43092644 epoch total loss 1.39832628\n",
      "Trained batch 653 batch loss 1.39697 epoch total loss 1.39832425\n",
      "Trained batch 654 batch loss 1.42627692 epoch total loss 1.39836693\n",
      "Trained batch 655 batch loss 1.38763857 epoch total loss 1.3983506\n",
      "Trained batch 656 batch loss 1.48152328 epoch total loss 1.39847732\n",
      "Trained batch 657 batch loss 1.36423886 epoch total loss 1.39842522\n",
      "Trained batch 658 batch loss 1.4199729 epoch total loss 1.398458\n",
      "Trained batch 659 batch loss 1.33761024 epoch total loss 1.39836562\n",
      "Trained batch 660 batch loss 1.42180705 epoch total loss 1.39840114\n",
      "Trained batch 661 batch loss 1.35383677 epoch total loss 1.39833367\n",
      "Trained batch 662 batch loss 1.32175696 epoch total loss 1.39821804\n",
      "Trained batch 663 batch loss 1.39970791 epoch total loss 1.3982203\n",
      "Trained batch 664 batch loss 1.37773979 epoch total loss 1.39818954\n",
      "Trained batch 665 batch loss 1.19584727 epoch total loss 1.3978852\n",
      "Trained batch 666 batch loss 1.2759732 epoch total loss 1.39770222\n",
      "Trained batch 667 batch loss 1.20434785 epoch total loss 1.3974123\n",
      "Trained batch 668 batch loss 1.22352302 epoch total loss 1.39715207\n",
      "Trained batch 669 batch loss 1.41686809 epoch total loss 1.39718151\n",
      "Trained batch 670 batch loss 1.42558038 epoch total loss 1.39722395\n",
      "Trained batch 671 batch loss 1.6614449 epoch total loss 1.3976177\n",
      "Trained batch 672 batch loss 1.56099176 epoch total loss 1.39786077\n",
      "Trained batch 673 batch loss 1.33447027 epoch total loss 1.39776659\n",
      "Trained batch 674 batch loss 1.4512701 epoch total loss 1.39784598\n",
      "Trained batch 675 batch loss 1.38536656 epoch total loss 1.39782751\n",
      "Trained batch 676 batch loss 1.44569254 epoch total loss 1.39789832\n",
      "Trained batch 677 batch loss 1.5088321 epoch total loss 1.39806223\n",
      "Trained batch 678 batch loss 1.55739915 epoch total loss 1.39829719\n",
      "Trained batch 679 batch loss 1.58744287 epoch total loss 1.39857578\n",
      "Trained batch 680 batch loss 1.39869201 epoch total loss 1.3985759\n",
      "Trained batch 681 batch loss 1.31920969 epoch total loss 1.39845932\n",
      "Trained batch 682 batch loss 1.34598434 epoch total loss 1.39838243\n",
      "Trained batch 683 batch loss 1.36551762 epoch total loss 1.39833438\n",
      "Trained batch 684 batch loss 1.26516891 epoch total loss 1.39813972\n",
      "Trained batch 685 batch loss 1.29089522 epoch total loss 1.39798319\n",
      "Trained batch 686 batch loss 1.3240639 epoch total loss 1.39787543\n",
      "Trained batch 687 batch loss 1.35420561 epoch total loss 1.39781177\n",
      "Trained batch 688 batch loss 1.27539027 epoch total loss 1.39763379\n",
      "Trained batch 689 batch loss 1.26280379 epoch total loss 1.39743817\n",
      "Trained batch 690 batch loss 1.30599046 epoch total loss 1.39730561\n",
      "Trained batch 691 batch loss 1.37960148 epoch total loss 1.39728\n",
      "Trained batch 692 batch loss 1.23638749 epoch total loss 1.3970474\n",
      "Trained batch 693 batch loss 1.3094511 epoch total loss 1.39692104\n",
      "Trained batch 694 batch loss 1.24604011 epoch total loss 1.3967036\n",
      "Trained batch 695 batch loss 1.50292897 epoch total loss 1.39685643\n",
      "Trained batch 696 batch loss 1.24368834 epoch total loss 1.39663649\n",
      "Trained batch 697 batch loss 1.46573102 epoch total loss 1.39673567\n",
      "Trained batch 698 batch loss 1.39418983 epoch total loss 1.39673197\n",
      "Trained batch 699 batch loss 1.40462923 epoch total loss 1.39674318\n",
      "Trained batch 700 batch loss 1.42480063 epoch total loss 1.39678323\n",
      "Trained batch 701 batch loss 1.40983331 epoch total loss 1.39680195\n",
      "Trained batch 702 batch loss 1.26985371 epoch total loss 1.39662111\n",
      "Trained batch 703 batch loss 1.29245067 epoch total loss 1.39647293\n",
      "Trained batch 704 batch loss 1.34767306 epoch total loss 1.39640355\n",
      "Trained batch 705 batch loss 1.53905094 epoch total loss 1.39660597\n",
      "Trained batch 706 batch loss 1.37384403 epoch total loss 1.39657366\n",
      "Trained batch 707 batch loss 1.43518603 epoch total loss 1.39662826\n",
      "Trained batch 708 batch loss 1.3802017 epoch total loss 1.39660501\n",
      "Trained batch 709 batch loss 1.38690507 epoch total loss 1.39659142\n",
      "Trained batch 710 batch loss 1.42737305 epoch total loss 1.3966347\n",
      "Trained batch 711 batch loss 1.25660229 epoch total loss 1.39643776\n",
      "Trained batch 712 batch loss 1.31158626 epoch total loss 1.39631855\n",
      "Trained batch 713 batch loss 1.34894145 epoch total loss 1.39625216\n",
      "Trained batch 714 batch loss 1.46977139 epoch total loss 1.39635515\n",
      "Trained batch 715 batch loss 1.43220496 epoch total loss 1.39640522\n",
      "Trained batch 716 batch loss 1.38554108 epoch total loss 1.39639008\n",
      "Trained batch 717 batch loss 1.36381102 epoch total loss 1.39634466\n",
      "Trained batch 718 batch loss 1.34621155 epoch total loss 1.39627481\n",
      "Trained batch 719 batch loss 1.33832932 epoch total loss 1.39619422\n",
      "Trained batch 720 batch loss 1.48246217 epoch total loss 1.39631402\n",
      "Trained batch 721 batch loss 1.44954062 epoch total loss 1.39638782\n",
      "Trained batch 722 batch loss 1.5222621 epoch total loss 1.39656222\n",
      "Trained batch 723 batch loss 1.36002755 epoch total loss 1.39651167\n",
      "Trained batch 724 batch loss 1.3470037 epoch total loss 1.39644337\n",
      "Trained batch 725 batch loss 1.34814584 epoch total loss 1.39637673\n",
      "Trained batch 726 batch loss 1.37922215 epoch total loss 1.39635301\n",
      "Trained batch 727 batch loss 1.39158964 epoch total loss 1.39634657\n",
      "Trained batch 728 batch loss 1.46400392 epoch total loss 1.39643943\n",
      "Trained batch 729 batch loss 1.58701873 epoch total loss 1.39670086\n",
      "Trained batch 730 batch loss 1.55466878 epoch total loss 1.39691734\n",
      "Trained batch 731 batch loss 1.40086329 epoch total loss 1.39692271\n",
      "Trained batch 732 batch loss 1.51824927 epoch total loss 1.39708853\n",
      "Trained batch 733 batch loss 1.50393462 epoch total loss 1.39723432\n",
      "Trained batch 734 batch loss 1.51434207 epoch total loss 1.3973937\n",
      "Trained batch 735 batch loss 1.44214261 epoch total loss 1.39745462\n",
      "Trained batch 736 batch loss 1.4638598 epoch total loss 1.39754486\n",
      "Trained batch 737 batch loss 1.25953007 epoch total loss 1.39735758\n",
      "Trained batch 738 batch loss 1.36240435 epoch total loss 1.39731026\n",
      "Trained batch 739 batch loss 1.31929874 epoch total loss 1.39720476\n",
      "Trained batch 740 batch loss 1.28380895 epoch total loss 1.39705145\n",
      "Trained batch 741 batch loss 1.2756443 epoch total loss 1.39688766\n",
      "Trained batch 742 batch loss 1.19217932 epoch total loss 1.39661169\n",
      "Trained batch 743 batch loss 1.26755404 epoch total loss 1.396438\n",
      "Trained batch 744 batch loss 1.4587723 epoch total loss 1.39652181\n",
      "Trained batch 745 batch loss 1.56881595 epoch total loss 1.39675307\n",
      "Trained batch 746 batch loss 1.53484392 epoch total loss 1.39693809\n",
      "Trained batch 747 batch loss 1.38028646 epoch total loss 1.39691579\n",
      "Trained batch 748 batch loss 1.30444455 epoch total loss 1.39679217\n",
      "Trained batch 749 batch loss 1.4328835 epoch total loss 1.39684033\n",
      "Trained batch 750 batch loss 1.39816821 epoch total loss 1.39684212\n",
      "Trained batch 751 batch loss 1.45435071 epoch total loss 1.39691865\n",
      "Trained batch 752 batch loss 1.45889688 epoch total loss 1.39700103\n",
      "Trained batch 753 batch loss 1.43031299 epoch total loss 1.39704525\n",
      "Trained batch 754 batch loss 1.63738191 epoch total loss 1.39736402\n",
      "Trained batch 755 batch loss 1.45401609 epoch total loss 1.397439\n",
      "Trained batch 756 batch loss 1.38400412 epoch total loss 1.39742124\n",
      "Trained batch 757 batch loss 1.35364783 epoch total loss 1.39736342\n",
      "Trained batch 758 batch loss 1.24328232 epoch total loss 1.39716\n",
      "Trained batch 759 batch loss 1.31711781 epoch total loss 1.39705467\n",
      "Trained batch 760 batch loss 1.41261053 epoch total loss 1.39707518\n",
      "Trained batch 761 batch loss 1.37947679 epoch total loss 1.39705205\n",
      "Trained batch 762 batch loss 1.20064735 epoch total loss 1.39679432\n",
      "Trained batch 763 batch loss 1.22721279 epoch total loss 1.39657199\n",
      "Trained batch 764 batch loss 1.28406644 epoch total loss 1.39642477\n",
      "Trained batch 765 batch loss 1.31379378 epoch total loss 1.39631689\n",
      "Trained batch 766 batch loss 1.3247894 epoch total loss 1.39622355\n",
      "Trained batch 767 batch loss 1.36638343 epoch total loss 1.39618456\n",
      "Trained batch 768 batch loss 1.22002685 epoch total loss 1.39595509\n",
      "Trained batch 769 batch loss 1.37672162 epoch total loss 1.39593\n",
      "Trained batch 770 batch loss 1.31097794 epoch total loss 1.39581978\n",
      "Trained batch 771 batch loss 1.45928097 epoch total loss 1.39590204\n",
      "Trained batch 772 batch loss 1.38412285 epoch total loss 1.39588678\n",
      "Trained batch 773 batch loss 1.26361907 epoch total loss 1.39571583\n",
      "Trained batch 774 batch loss 1.27715421 epoch total loss 1.39556253\n",
      "Trained batch 775 batch loss 1.37787402 epoch total loss 1.39553976\n",
      "Trained batch 776 batch loss 1.47558081 epoch total loss 1.395643\n",
      "Trained batch 777 batch loss 1.51732016 epoch total loss 1.39579952\n",
      "Trained batch 778 batch loss 1.39031816 epoch total loss 1.39579248\n",
      "Trained batch 779 batch loss 1.37036431 epoch total loss 1.39575982\n",
      "Trained batch 780 batch loss 1.26427698 epoch total loss 1.39559126\n",
      "Trained batch 781 batch loss 1.28361535 epoch total loss 1.39544785\n",
      "Trained batch 782 batch loss 1.42063498 epoch total loss 1.39548\n",
      "Trained batch 783 batch loss 1.3660512 epoch total loss 1.39544249\n",
      "Trained batch 784 batch loss 1.45917523 epoch total loss 1.39552391\n",
      "Trained batch 785 batch loss 1.35054 epoch total loss 1.39546657\n",
      "Trained batch 786 batch loss 1.17784 epoch total loss 1.39518976\n",
      "Trained batch 787 batch loss 1.1718061 epoch total loss 1.39490581\n",
      "Trained batch 788 batch loss 1.26472235 epoch total loss 1.3947407\n",
      "Trained batch 789 batch loss 1.44109988 epoch total loss 1.39479935\n",
      "Trained batch 790 batch loss 1.45099521 epoch total loss 1.39487052\n",
      "Trained batch 791 batch loss 1.47149479 epoch total loss 1.39496732\n",
      "Trained batch 792 batch loss 1.46604884 epoch total loss 1.39505708\n",
      "Trained batch 793 batch loss 1.50004375 epoch total loss 1.39518952\n",
      "Trained batch 794 batch loss 1.43057919 epoch total loss 1.39523399\n",
      "Trained batch 795 batch loss 1.43113494 epoch total loss 1.39527917\n",
      "Trained batch 796 batch loss 1.37876296 epoch total loss 1.39525843\n",
      "Trained batch 797 batch loss 1.41306329 epoch total loss 1.39528084\n",
      "Trained batch 798 batch loss 1.43200064 epoch total loss 1.39532685\n",
      "Trained batch 799 batch loss 1.40119457 epoch total loss 1.39533424\n",
      "Trained batch 800 batch loss 1.41173971 epoch total loss 1.39535475\n",
      "Trained batch 801 batch loss 1.42870498 epoch total loss 1.39539635\n",
      "Trained batch 802 batch loss 1.26408553 epoch total loss 1.39523268\n",
      "Trained batch 803 batch loss 1.20273376 epoch total loss 1.39499295\n",
      "Trained batch 804 batch loss 1.11579859 epoch total loss 1.39464569\n",
      "Trained batch 805 batch loss 1.20945 epoch total loss 1.39441574\n",
      "Trained batch 806 batch loss 1.37091124 epoch total loss 1.39438665\n",
      "Trained batch 807 batch loss 1.53318834 epoch total loss 1.39455867\n",
      "Trained batch 808 batch loss 1.73067021 epoch total loss 1.39497471\n",
      "Trained batch 809 batch loss 1.329 epoch total loss 1.39489305\n",
      "Trained batch 810 batch loss 1.33083439 epoch total loss 1.39481401\n",
      "Trained batch 811 batch loss 1.33089209 epoch total loss 1.39473522\n",
      "Trained batch 812 batch loss 1.46946132 epoch total loss 1.39482725\n",
      "Trained batch 813 batch loss 1.43639922 epoch total loss 1.39487839\n",
      "Trained batch 814 batch loss 1.4140594 epoch total loss 1.39490199\n",
      "Trained batch 815 batch loss 1.38824606 epoch total loss 1.39489388\n",
      "Trained batch 816 batch loss 1.43355799 epoch total loss 1.39494133\n",
      "Trained batch 817 batch loss 1.38517642 epoch total loss 1.39492929\n",
      "Trained batch 818 batch loss 1.32460868 epoch total loss 1.39484334\n",
      "Trained batch 819 batch loss 1.31946 epoch total loss 1.39475119\n",
      "Trained batch 820 batch loss 1.25805855 epoch total loss 1.39458454\n",
      "Trained batch 821 batch loss 1.40076649 epoch total loss 1.39459205\n",
      "Trained batch 822 batch loss 1.44861865 epoch total loss 1.39465773\n",
      "Trained batch 823 batch loss 1.25715709 epoch total loss 1.39449072\n",
      "Trained batch 824 batch loss 1.11249864 epoch total loss 1.39414859\n",
      "Trained batch 825 batch loss 1.07725859 epoch total loss 1.3937645\n",
      "Trained batch 826 batch loss 1.17590165 epoch total loss 1.3935008\n",
      "Trained batch 827 batch loss 1.29720235 epoch total loss 1.39338434\n",
      "Trained batch 828 batch loss 1.22587371 epoch total loss 1.39318204\n",
      "Trained batch 829 batch loss 1.32295203 epoch total loss 1.39309728\n",
      "Trained batch 830 batch loss 1.25009406 epoch total loss 1.39292502\n",
      "Trained batch 831 batch loss 1.26471758 epoch total loss 1.39277089\n",
      "Trained batch 832 batch loss 1.39479101 epoch total loss 1.39277327\n",
      "Trained batch 833 batch loss 1.3518188 epoch total loss 1.39272404\n",
      "Trained batch 834 batch loss 1.38271809 epoch total loss 1.392712\n",
      "Trained batch 835 batch loss 1.37181258 epoch total loss 1.39268708\n",
      "Trained batch 836 batch loss 1.24594319 epoch total loss 1.39251149\n",
      "Trained batch 837 batch loss 1.25387788 epoch total loss 1.39234591\n",
      "Trained batch 838 batch loss 1.26879573 epoch total loss 1.39219856\n",
      "Trained batch 839 batch loss 1.40791464 epoch total loss 1.39221728\n",
      "Trained batch 840 batch loss 1.35251117 epoch total loss 1.39217007\n",
      "Trained batch 841 batch loss 1.51371062 epoch total loss 1.39231455\n",
      "Trained batch 842 batch loss 1.44202745 epoch total loss 1.39237356\n",
      "Trained batch 843 batch loss 1.44239128 epoch total loss 1.39243293\n",
      "Trained batch 844 batch loss 1.36255634 epoch total loss 1.39239752\n",
      "Trained batch 845 batch loss 1.21888137 epoch total loss 1.39219213\n",
      "Trained batch 846 batch loss 1.37618756 epoch total loss 1.39217329\n",
      "Trained batch 847 batch loss 1.50053835 epoch total loss 1.39230108\n",
      "Trained batch 848 batch loss 1.37538576 epoch total loss 1.39228117\n",
      "Trained batch 849 batch loss 1.34979773 epoch total loss 1.39223123\n",
      "Trained batch 850 batch loss 1.29149175 epoch total loss 1.39211273\n",
      "Trained batch 851 batch loss 1.30963874 epoch total loss 1.39201581\n",
      "Trained batch 852 batch loss 1.29503489 epoch total loss 1.39190197\n",
      "Trained batch 853 batch loss 1.32931828 epoch total loss 1.39182866\n",
      "Trained batch 854 batch loss 1.15044665 epoch total loss 1.39154601\n",
      "Trained batch 855 batch loss 1.19819844 epoch total loss 1.39131987\n",
      "Trained batch 856 batch loss 1.24178612 epoch total loss 1.39114523\n",
      "Trained batch 857 batch loss 1.21718836 epoch total loss 1.39094222\n",
      "Trained batch 858 batch loss 1.17694259 epoch total loss 1.39069283\n",
      "Trained batch 859 batch loss 1.28700471 epoch total loss 1.39057219\n",
      "Trained batch 860 batch loss 1.30136728 epoch total loss 1.39046848\n",
      "Trained batch 861 batch loss 1.35733724 epoch total loss 1.39043\n",
      "Trained batch 862 batch loss 1.31426859 epoch total loss 1.39034152\n",
      "Trained batch 863 batch loss 1.18685114 epoch total loss 1.39010572\n",
      "Trained batch 864 batch loss 1.33631015 epoch total loss 1.3900435\n",
      "Trained batch 865 batch loss 1.40164351 epoch total loss 1.39005685\n",
      "Trained batch 866 batch loss 1.40643859 epoch total loss 1.3900758\n",
      "Trained batch 867 batch loss 1.43742454 epoch total loss 1.3901304\n",
      "Trained batch 868 batch loss 1.4253962 epoch total loss 1.39017105\n",
      "Trained batch 869 batch loss 1.42320871 epoch total loss 1.39020908\n",
      "Trained batch 870 batch loss 1.29068208 epoch total loss 1.39009464\n",
      "Trained batch 871 batch loss 1.32247508 epoch total loss 1.39001703\n",
      "Trained batch 872 batch loss 1.33981729 epoch total loss 1.38995945\n",
      "Trained batch 873 batch loss 1.2493751 epoch total loss 1.38979852\n",
      "Trained batch 874 batch loss 1.2213906 epoch total loss 1.38960588\n",
      "Trained batch 875 batch loss 1.18359828 epoch total loss 1.38937044\n",
      "Trained batch 876 batch loss 1.30368924 epoch total loss 1.38927257\n",
      "Trained batch 877 batch loss 1.27917099 epoch total loss 1.38914704\n",
      "Trained batch 878 batch loss 1.28996682 epoch total loss 1.38903403\n",
      "Trained batch 879 batch loss 1.19223309 epoch total loss 1.38881016\n",
      "Trained batch 880 batch loss 1.28057492 epoch total loss 1.38868713\n",
      "Trained batch 881 batch loss 1.34098649 epoch total loss 1.38863289\n",
      "Trained batch 882 batch loss 1.30864 epoch total loss 1.38854218\n",
      "Trained batch 883 batch loss 1.32770824 epoch total loss 1.38847339\n",
      "Trained batch 884 batch loss 1.41957676 epoch total loss 1.38850856\n",
      "Trained batch 885 batch loss 1.37119961 epoch total loss 1.38848901\n",
      "Trained batch 886 batch loss 1.41509449 epoch total loss 1.38851893\n",
      "Trained batch 887 batch loss 1.30883229 epoch total loss 1.38842916\n",
      "Trained batch 888 batch loss 1.53681052 epoch total loss 1.3885963\n",
      "Trained batch 889 batch loss 1.40219593 epoch total loss 1.38861156\n",
      "Trained batch 890 batch loss 1.10880935 epoch total loss 1.3882972\n",
      "Trained batch 891 batch loss 1.16365635 epoch total loss 1.38804507\n",
      "Trained batch 892 batch loss 1.41297328 epoch total loss 1.38807297\n",
      "Trained batch 893 batch loss 1.44446445 epoch total loss 1.38813615\n",
      "Trained batch 894 batch loss 1.46456468 epoch total loss 1.38822174\n",
      "Trained batch 895 batch loss 1.4072541 epoch total loss 1.38824296\n",
      "Trained batch 896 batch loss 1.45290899 epoch total loss 1.38831508\n",
      "Trained batch 897 batch loss 1.45651615 epoch total loss 1.38839114\n",
      "Trained batch 898 batch loss 1.37635219 epoch total loss 1.38837767\n",
      "Trained batch 899 batch loss 1.39820063 epoch total loss 1.38838863\n",
      "Trained batch 900 batch loss 1.4456588 epoch total loss 1.38845229\n",
      "Trained batch 901 batch loss 1.3968482 epoch total loss 1.38846159\n",
      "Trained batch 902 batch loss 1.32012177 epoch total loss 1.38838577\n",
      "Trained batch 903 batch loss 1.32823586 epoch total loss 1.38831913\n",
      "Trained batch 904 batch loss 1.22197926 epoch total loss 1.38813508\n",
      "Trained batch 905 batch loss 1.25137889 epoch total loss 1.38798392\n",
      "Trained batch 906 batch loss 1.40643775 epoch total loss 1.38800442\n",
      "Trained batch 907 batch loss 1.28175163 epoch total loss 1.38788724\n",
      "Trained batch 908 batch loss 1.4504323 epoch total loss 1.38795614\n",
      "Trained batch 909 batch loss 1.43195605 epoch total loss 1.38800454\n",
      "Trained batch 910 batch loss 1.40413725 epoch total loss 1.3880223\n",
      "Trained batch 911 batch loss 1.28238869 epoch total loss 1.38790631\n",
      "Trained batch 912 batch loss 1.18794942 epoch total loss 1.38768721\n",
      "Trained batch 913 batch loss 1.49357224 epoch total loss 1.38780308\n",
      "Trained batch 914 batch loss 1.4293685 epoch total loss 1.3878485\n",
      "Trained batch 915 batch loss 1.46578944 epoch total loss 1.38793373\n",
      "Trained batch 916 batch loss 1.4643476 epoch total loss 1.38801718\n",
      "Trained batch 917 batch loss 1.46680522 epoch total loss 1.38810301\n",
      "Trained batch 918 batch loss 1.39620805 epoch total loss 1.38811195\n",
      "Trained batch 919 batch loss 1.2560004 epoch total loss 1.38796818\n",
      "Trained batch 920 batch loss 1.27665782 epoch total loss 1.38784707\n",
      "Trained batch 921 batch loss 1.33414876 epoch total loss 1.38778877\n",
      "Trained batch 922 batch loss 1.31388569 epoch total loss 1.38770854\n",
      "Trained batch 923 batch loss 1.36628127 epoch total loss 1.38768542\n",
      "Trained batch 924 batch loss 1.49882627 epoch total loss 1.38780558\n",
      "Trained batch 925 batch loss 1.41373241 epoch total loss 1.3878336\n",
      "Trained batch 926 batch loss 1.3907814 epoch total loss 1.38783681\n",
      "Trained batch 927 batch loss 1.3969481 epoch total loss 1.38784659\n",
      "Trained batch 928 batch loss 1.48456967 epoch total loss 1.3879509\n",
      "Trained batch 929 batch loss 1.48168349 epoch total loss 1.38805175\n",
      "Trained batch 930 batch loss 1.42019439 epoch total loss 1.38808632\n",
      "Trained batch 931 batch loss 1.44310963 epoch total loss 1.38814545\n",
      "Trained batch 932 batch loss 1.4797864 epoch total loss 1.38824368\n",
      "Trained batch 933 batch loss 1.31211 epoch total loss 1.38816214\n",
      "Trained batch 934 batch loss 1.29343045 epoch total loss 1.38806069\n",
      "Trained batch 935 batch loss 1.24346101 epoch total loss 1.38790607\n",
      "Trained batch 936 batch loss 1.15737581 epoch total loss 1.38765967\n",
      "Trained batch 937 batch loss 1.277161 epoch total loss 1.38754189\n",
      "Trained batch 938 batch loss 1.33721852 epoch total loss 1.38748813\n",
      "Trained batch 939 batch loss 1.24407411 epoch total loss 1.3873353\n",
      "Trained batch 940 batch loss 1.30782282 epoch total loss 1.38725078\n",
      "Trained batch 941 batch loss 1.37250674 epoch total loss 1.38723516\n",
      "Trained batch 942 batch loss 1.39922428 epoch total loss 1.3872478\n",
      "Trained batch 943 batch loss 1.38098907 epoch total loss 1.38724124\n",
      "Trained batch 944 batch loss 1.28399944 epoch total loss 1.38713193\n",
      "Trained batch 945 batch loss 1.28888619 epoch total loss 1.38702798\n",
      "Trained batch 946 batch loss 1.35074615 epoch total loss 1.38698959\n",
      "Trained batch 947 batch loss 1.3886261 epoch total loss 1.38699138\n",
      "Trained batch 948 batch loss 1.00062561 epoch total loss 1.38658381\n",
      "Trained batch 949 batch loss 1.13419139 epoch total loss 1.38631785\n",
      "Trained batch 950 batch loss 1.20726728 epoch total loss 1.38612938\n",
      "Trained batch 951 batch loss 1.42170119 epoch total loss 1.38616681\n",
      "Trained batch 952 batch loss 1.47979581 epoch total loss 1.38626504\n",
      "Trained batch 953 batch loss 1.44982994 epoch total loss 1.3863318\n",
      "Trained batch 954 batch loss 1.50421596 epoch total loss 1.38645542\n",
      "Trained batch 955 batch loss 1.46327817 epoch total loss 1.38653588\n",
      "Trained batch 956 batch loss 1.39242363 epoch total loss 1.38654208\n",
      "Trained batch 957 batch loss 1.27991855 epoch total loss 1.38643062\n",
      "Trained batch 958 batch loss 1.15328848 epoch total loss 1.38618731\n",
      "Trained batch 959 batch loss 1.3396275 epoch total loss 1.38613868\n",
      "Trained batch 960 batch loss 1.47593307 epoch total loss 1.38623226\n",
      "Trained batch 961 batch loss 1.41402352 epoch total loss 1.38626122\n",
      "Trained batch 962 batch loss 1.34100497 epoch total loss 1.38621426\n",
      "Trained batch 963 batch loss 1.38629127 epoch total loss 1.38621426\n",
      "Trained batch 964 batch loss 1.38738346 epoch total loss 1.38621545\n",
      "Trained batch 965 batch loss 1.40010107 epoch total loss 1.38622987\n",
      "Trained batch 966 batch loss 1.36453438 epoch total loss 1.38620734\n",
      "Trained batch 967 batch loss 1.35554981 epoch total loss 1.38617563\n",
      "Trained batch 968 batch loss 1.20082986 epoch total loss 1.38598418\n",
      "Trained batch 969 batch loss 1.34469521 epoch total loss 1.38594162\n",
      "Trained batch 970 batch loss 1.34906054 epoch total loss 1.3859036\n",
      "Trained batch 971 batch loss 1.38091731 epoch total loss 1.38589847\n",
      "Trained batch 972 batch loss 1.34865844 epoch total loss 1.38586009\n",
      "Trained batch 973 batch loss 1.34245539 epoch total loss 1.3858155\n",
      "Trained batch 974 batch loss 1.37635279 epoch total loss 1.38580573\n",
      "Trained batch 975 batch loss 1.40881014 epoch total loss 1.38582933\n",
      "Trained batch 976 batch loss 1.35613585 epoch total loss 1.38579881\n",
      "Trained batch 977 batch loss 1.32277977 epoch total loss 1.38573432\n",
      "Trained batch 978 batch loss 1.36446941 epoch total loss 1.38571262\n",
      "Trained batch 979 batch loss 1.35227299 epoch total loss 1.38567853\n",
      "Trained batch 980 batch loss 1.27278364 epoch total loss 1.38556337\n",
      "Trained batch 981 batch loss 1.35275793 epoch total loss 1.38552988\n",
      "Trained batch 982 batch loss 1.41068566 epoch total loss 1.38555551\n",
      "Trained batch 983 batch loss 1.36207259 epoch total loss 1.38553154\n",
      "Trained batch 984 batch loss 1.37415171 epoch total loss 1.38552\n",
      "Trained batch 985 batch loss 1.3472476 epoch total loss 1.38548124\n",
      "Trained batch 986 batch loss 1.27100098 epoch total loss 1.38536513\n",
      "Trained batch 987 batch loss 1.33090746 epoch total loss 1.38530993\n",
      "Trained batch 988 batch loss 1.31116152 epoch total loss 1.38523483\n",
      "Trained batch 989 batch loss 1.30234766 epoch total loss 1.38515115\n",
      "Trained batch 990 batch loss 1.24047947 epoch total loss 1.385005\n",
      "Trained batch 991 batch loss 1.39551115 epoch total loss 1.38501561\n",
      "Trained batch 992 batch loss 1.40511954 epoch total loss 1.38503587\n",
      "Trained batch 993 batch loss 1.38999903 epoch total loss 1.38504088\n",
      "Trained batch 994 batch loss 1.37184751 epoch total loss 1.38502753\n",
      "Trained batch 995 batch loss 1.40993702 epoch total loss 1.38505256\n",
      "Trained batch 996 batch loss 1.50477731 epoch total loss 1.38517272\n",
      "Trained batch 997 batch loss 1.34772813 epoch total loss 1.38513529\n",
      "Trained batch 998 batch loss 1.29220486 epoch total loss 1.38504219\n",
      "Trained batch 999 batch loss 1.41239476 epoch total loss 1.38506949\n",
      "Trained batch 1000 batch loss 1.50763035 epoch total loss 1.38519216\n",
      "Trained batch 1001 batch loss 1.39058864 epoch total loss 1.38519752\n",
      "Trained batch 1002 batch loss 1.35168254 epoch total loss 1.38516414\n",
      "Trained batch 1003 batch loss 1.47788954 epoch total loss 1.38525653\n",
      "Trained batch 1004 batch loss 1.36572456 epoch total loss 1.3852371\n",
      "Trained batch 1005 batch loss 1.35895681 epoch total loss 1.38521099\n",
      "Trained batch 1006 batch loss 1.33667 epoch total loss 1.38516283\n",
      "Trained batch 1007 batch loss 1.42338133 epoch total loss 1.38520074\n",
      "Trained batch 1008 batch loss 1.53991854 epoch total loss 1.38535416\n",
      "Trained batch 1009 batch loss 1.43409848 epoch total loss 1.38540244\n",
      "Trained batch 1010 batch loss 1.39905667 epoch total loss 1.38541603\n",
      "Trained batch 1011 batch loss 1.29819894 epoch total loss 1.38532972\n",
      "Trained batch 1012 batch loss 1.43931639 epoch total loss 1.38538313\n",
      "Trained batch 1013 batch loss 1.3202213 epoch total loss 1.38531876\n",
      "Trained batch 1014 batch loss 1.386127 epoch total loss 1.38531947\n",
      "Trained batch 1015 batch loss 1.44304323 epoch total loss 1.38537633\n",
      "Trained batch 1016 batch loss 1.45768523 epoch total loss 1.3854475\n",
      "Trained batch 1017 batch loss 1.35730386 epoch total loss 1.38541985\n",
      "Trained batch 1018 batch loss 1.22438383 epoch total loss 1.38526154\n",
      "Trained batch 1019 batch loss 1.3016932 epoch total loss 1.38517952\n",
      "Trained batch 1020 batch loss 1.28669834 epoch total loss 1.38508296\n",
      "Trained batch 1021 batch loss 1.36319411 epoch total loss 1.3850615\n",
      "Trained batch 1022 batch loss 1.1835773 epoch total loss 1.38486445\n",
      "Trained batch 1023 batch loss 1.31281543 epoch total loss 1.384794\n",
      "Trained batch 1024 batch loss 1.3051784 epoch total loss 1.38471627\n",
      "Trained batch 1025 batch loss 1.24218798 epoch total loss 1.38457727\n",
      "Trained batch 1026 batch loss 1.41283488 epoch total loss 1.38460481\n",
      "Trained batch 1027 batch loss 1.39785969 epoch total loss 1.38461769\n",
      "Trained batch 1028 batch loss 1.33984351 epoch total loss 1.38457406\n",
      "Trained batch 1029 batch loss 1.29541683 epoch total loss 1.38448739\n",
      "Trained batch 1030 batch loss 1.23168087 epoch total loss 1.38433909\n",
      "Trained batch 1031 batch loss 1.18142092 epoch total loss 1.38414228\n",
      "Trained batch 1032 batch loss 1.23029113 epoch total loss 1.38399327\n",
      "Trained batch 1033 batch loss 1.12273669 epoch total loss 1.38374031\n",
      "Trained batch 1034 batch loss 1.19382167 epoch total loss 1.3835566\n",
      "Trained batch 1035 batch loss 1.26780355 epoch total loss 1.38344479\n",
      "Trained batch 1036 batch loss 1.28558052 epoch total loss 1.38335025\n",
      "Trained batch 1037 batch loss 1.26564264 epoch total loss 1.38323677\n",
      "Trained batch 1038 batch loss 1.34329808 epoch total loss 1.38319826\n",
      "Trained batch 1039 batch loss 1.38894296 epoch total loss 1.38320374\n",
      "Trained batch 1040 batch loss 1.39468503 epoch total loss 1.38321471\n",
      "Trained batch 1041 batch loss 1.31669962 epoch total loss 1.38315082\n",
      "Trained batch 1042 batch loss 1.37817645 epoch total loss 1.38314605\n",
      "Trained batch 1043 batch loss 1.32882595 epoch total loss 1.38309395\n",
      "Trained batch 1044 batch loss 1.40261447 epoch total loss 1.38311267\n",
      "Trained batch 1045 batch loss 1.31671047 epoch total loss 1.38304901\n",
      "Trained batch 1046 batch loss 1.36310053 epoch total loss 1.38303\n",
      "Trained batch 1047 batch loss 1.3868463 epoch total loss 1.38303363\n",
      "Trained batch 1048 batch loss 1.22928214 epoch total loss 1.38288689\n",
      "Trained batch 1049 batch loss 1.28426158 epoch total loss 1.38279295\n",
      "Trained batch 1050 batch loss 1.42911732 epoch total loss 1.38283706\n",
      "Trained batch 1051 batch loss 1.33997941 epoch total loss 1.38279629\n",
      "Trained batch 1052 batch loss 1.4376353 epoch total loss 1.38284838\n",
      "Trained batch 1053 batch loss 1.52576208 epoch total loss 1.38298404\n",
      "Trained batch 1054 batch loss 1.37510633 epoch total loss 1.38297665\n",
      "Trained batch 1055 batch loss 1.38178849 epoch total loss 1.38297558\n",
      "Trained batch 1056 batch loss 1.41153872 epoch total loss 1.38300252\n",
      "Trained batch 1057 batch loss 1.31945181 epoch total loss 1.38294244\n",
      "Trained batch 1058 batch loss 1.4564116 epoch total loss 1.38301182\n",
      "Trained batch 1059 batch loss 1.37024593 epoch total loss 1.38299978\n",
      "Trained batch 1060 batch loss 1.27169609 epoch total loss 1.38289487\n",
      "Trained batch 1061 batch loss 1.22695732 epoch total loss 1.38274789\n",
      "Trained batch 1062 batch loss 1.20786488 epoch total loss 1.38258314\n",
      "Trained batch 1063 batch loss 1.31319308 epoch total loss 1.38251793\n",
      "Trained batch 1064 batch loss 1.29453707 epoch total loss 1.38243532\n",
      "Trained batch 1065 batch loss 1.4018929 epoch total loss 1.38245356\n",
      "Trained batch 1066 batch loss 1.58638632 epoch total loss 1.38264489\n",
      "Trained batch 1067 batch loss 1.64339614 epoch total loss 1.38288927\n",
      "Trained batch 1068 batch loss 1.49587858 epoch total loss 1.38299501\n",
      "Trained batch 1069 batch loss 1.52668202 epoch total loss 1.38312948\n",
      "Trained batch 1070 batch loss 1.45273685 epoch total loss 1.38319457\n",
      "Trained batch 1071 batch loss 1.37552226 epoch total loss 1.38318741\n",
      "Trained batch 1072 batch loss 1.15182424 epoch total loss 1.38297153\n",
      "Trained batch 1073 batch loss 1.36093497 epoch total loss 1.38295102\n",
      "Trained batch 1074 batch loss 1.33493233 epoch total loss 1.38290644\n",
      "Trained batch 1075 batch loss 1.45978618 epoch total loss 1.38297796\n",
      "Trained batch 1076 batch loss 1.40596187 epoch total loss 1.3829993\n",
      "Trained batch 1077 batch loss 1.31414962 epoch total loss 1.38293552\n",
      "Trained batch 1078 batch loss 1.19609261 epoch total loss 1.38276207\n",
      "Trained batch 1079 batch loss 1.20069599 epoch total loss 1.38259339\n",
      "Trained batch 1080 batch loss 1.22932172 epoch total loss 1.38245153\n",
      "Trained batch 1081 batch loss 1.28424263 epoch total loss 1.3823607\n",
      "Trained batch 1082 batch loss 1.2527523 epoch total loss 1.38224101\n",
      "Trained batch 1083 batch loss 1.2435807 epoch total loss 1.38211286\n",
      "Trained batch 1084 batch loss 1.30917835 epoch total loss 1.38204563\n",
      "Trained batch 1085 batch loss 1.43132663 epoch total loss 1.38209105\n",
      "Trained batch 1086 batch loss 1.35459101 epoch total loss 1.38206565\n",
      "Trained batch 1087 batch loss 1.40267515 epoch total loss 1.38208473\n",
      "Trained batch 1088 batch loss 1.36060309 epoch total loss 1.38206494\n",
      "Trained batch 1089 batch loss 1.32123709 epoch total loss 1.38200915\n",
      "Trained batch 1090 batch loss 1.32441986 epoch total loss 1.38195634\n",
      "Trained batch 1091 batch loss 1.23116314 epoch total loss 1.38181818\n",
      "Trained batch 1092 batch loss 1.40702343 epoch total loss 1.38184118\n",
      "Trained batch 1093 batch loss 1.31202769 epoch total loss 1.38177729\n",
      "Trained batch 1094 batch loss 1.32109904 epoch total loss 1.38172174\n",
      "Trained batch 1095 batch loss 1.41011965 epoch total loss 1.38174772\n",
      "Trained batch 1096 batch loss 1.27884865 epoch total loss 1.38165379\n",
      "Trained batch 1097 batch loss 1.26018631 epoch total loss 1.38154304\n",
      "Trained batch 1098 batch loss 1.30887461 epoch total loss 1.38147688\n",
      "Trained batch 1099 batch loss 1.40289152 epoch total loss 1.38149631\n",
      "Trained batch 1100 batch loss 1.33750582 epoch total loss 1.38145626\n",
      "Trained batch 1101 batch loss 1.39016342 epoch total loss 1.38146412\n",
      "Trained batch 1102 batch loss 1.35929286 epoch total loss 1.38144398\n",
      "Trained batch 1103 batch loss 1.32831633 epoch total loss 1.38139594\n",
      "Trained batch 1104 batch loss 1.24659455 epoch total loss 1.38127375\n",
      "Trained batch 1105 batch loss 1.29135895 epoch total loss 1.38119245\n",
      "Trained batch 1106 batch loss 1.42222548 epoch total loss 1.38122952\n",
      "Trained batch 1107 batch loss 1.33189797 epoch total loss 1.38118505\n",
      "Trained batch 1108 batch loss 1.30130959 epoch total loss 1.38111293\n",
      "Trained batch 1109 batch loss 1.23670053 epoch total loss 1.38098264\n",
      "Trained batch 1110 batch loss 1.31857061 epoch total loss 1.38092649\n",
      "Trained batch 1111 batch loss 1.38334441 epoch total loss 1.38092864\n",
      "Trained batch 1112 batch loss 1.46851444 epoch total loss 1.38100731\n",
      "Trained batch 1113 batch loss 1.32891619 epoch total loss 1.38096046\n",
      "Trained batch 1114 batch loss 1.40604186 epoch total loss 1.380983\n",
      "Trained batch 1115 batch loss 1.35004854 epoch total loss 1.38095534\n",
      "Trained batch 1116 batch loss 1.44952083 epoch total loss 1.38101661\n",
      "Trained batch 1117 batch loss 1.34999216 epoch total loss 1.38098884\n",
      "Trained batch 1118 batch loss 1.33989179 epoch total loss 1.38095212\n",
      "Trained batch 1119 batch loss 1.33816075 epoch total loss 1.38091385\n",
      "Trained batch 1120 batch loss 1.3000921 epoch total loss 1.38084161\n",
      "Trained batch 1121 batch loss 1.22339344 epoch total loss 1.38070118\n",
      "Trained batch 1122 batch loss 1.29791152 epoch total loss 1.38062727\n",
      "Trained batch 1123 batch loss 1.38130629 epoch total loss 1.38062799\n",
      "Trained batch 1124 batch loss 1.32707143 epoch total loss 1.38058031\n",
      "Trained batch 1125 batch loss 1.27830529 epoch total loss 1.38048935\n",
      "Trained batch 1126 batch loss 1.21943796 epoch total loss 1.38034642\n",
      "Trained batch 1127 batch loss 1.32711399 epoch total loss 1.38029921\n",
      "Trained batch 1128 batch loss 1.33008504 epoch total loss 1.38025463\n",
      "Trained batch 1129 batch loss 1.45493948 epoch total loss 1.38032079\n",
      "Trained batch 1130 batch loss 1.42137337 epoch total loss 1.38035715\n",
      "Trained batch 1131 batch loss 1.24956119 epoch total loss 1.38024151\n",
      "Trained batch 1132 batch loss 1.25924611 epoch total loss 1.38013458\n",
      "Trained batch 1133 batch loss 1.25482547 epoch total loss 1.38002408\n",
      "Trained batch 1134 batch loss 1.24245417 epoch total loss 1.37990272\n",
      "Trained batch 1135 batch loss 1.31758 epoch total loss 1.37984788\n",
      "Trained batch 1136 batch loss 1.40473461 epoch total loss 1.37986982\n",
      "Trained batch 1137 batch loss 1.36082876 epoch total loss 1.37985313\n",
      "Trained batch 1138 batch loss 1.34734702 epoch total loss 1.3798244\n",
      "Trained batch 1139 batch loss 1.34880078 epoch total loss 1.37979722\n",
      "Trained batch 1140 batch loss 1.29398656 epoch total loss 1.37972188\n",
      "Trained batch 1141 batch loss 1.27031708 epoch total loss 1.37962592\n",
      "Trained batch 1142 batch loss 1.37800503 epoch total loss 1.37962461\n",
      "Trained batch 1143 batch loss 1.24245358 epoch total loss 1.37950456\n",
      "Trained batch 1144 batch loss 1.334849 epoch total loss 1.37946546\n",
      "Trained batch 1145 batch loss 1.3310672 epoch total loss 1.37942326\n",
      "Trained batch 1146 batch loss 1.33741164 epoch total loss 1.37938654\n",
      "Trained batch 1147 batch loss 1.26982307 epoch total loss 1.37929094\n",
      "Trained batch 1148 batch loss 1.29256439 epoch total loss 1.37921548\n",
      "Trained batch 1149 batch loss 1.21818912 epoch total loss 1.37907529\n",
      "Trained batch 1150 batch loss 1.29321897 epoch total loss 1.37900066\n",
      "Trained batch 1151 batch loss 1.29384828 epoch total loss 1.37892663\n",
      "Trained batch 1152 batch loss 1.34505749 epoch total loss 1.37889719\n",
      "Trained batch 1153 batch loss 1.21344 epoch total loss 1.37875378\n",
      "Trained batch 1154 batch loss 1.19595504 epoch total loss 1.37859535\n",
      "Trained batch 1155 batch loss 1.32677472 epoch total loss 1.37855053\n",
      "Trained batch 1156 batch loss 1.26878619 epoch total loss 1.37845552\n",
      "Trained batch 1157 batch loss 1.37065721 epoch total loss 1.37844872\n",
      "Trained batch 1158 batch loss 1.28552151 epoch total loss 1.3783685\n",
      "Trained batch 1159 batch loss 1.32932067 epoch total loss 1.37832618\n",
      "Trained batch 1160 batch loss 1.29910803 epoch total loss 1.37825787\n",
      "Trained batch 1161 batch loss 1.37060726 epoch total loss 1.37825131\n",
      "Trained batch 1162 batch loss 1.42749023 epoch total loss 1.37829363\n",
      "Trained batch 1163 batch loss 1.43739343 epoch total loss 1.37834454\n",
      "Trained batch 1164 batch loss 1.25990617 epoch total loss 1.37824273\n",
      "Trained batch 1165 batch loss 1.36175418 epoch total loss 1.37822855\n",
      "Trained batch 1166 batch loss 1.24954796 epoch total loss 1.37811816\n",
      "Trained batch 1167 batch loss 1.16493082 epoch total loss 1.37793541\n",
      "Trained batch 1168 batch loss 1.26679325 epoch total loss 1.37784028\n",
      "Trained batch 1169 batch loss 1.33293891 epoch total loss 1.3778019\n",
      "Trained batch 1170 batch loss 1.33088243 epoch total loss 1.37776184\n",
      "Trained batch 1171 batch loss 1.3923409 epoch total loss 1.37777424\n",
      "Trained batch 1172 batch loss 1.40247178 epoch total loss 1.37779534\n",
      "Trained batch 1173 batch loss 1.32133508 epoch total loss 1.37774718\n",
      "Trained batch 1174 batch loss 1.326231 epoch total loss 1.37770319\n",
      "Trained batch 1175 batch loss 1.43420768 epoch total loss 1.37775135\n",
      "Trained batch 1176 batch loss 1.42392635 epoch total loss 1.37779057\n",
      "Trained batch 1177 batch loss 1.37407315 epoch total loss 1.37778735\n",
      "Trained batch 1178 batch loss 1.4527204 epoch total loss 1.37785101\n",
      "Trained batch 1179 batch loss 1.35931051 epoch total loss 1.37783527\n",
      "Trained batch 1180 batch loss 1.31926835 epoch total loss 1.37778556\n",
      "Trained batch 1181 batch loss 1.26507282 epoch total loss 1.37769008\n",
      "Trained batch 1182 batch loss 1.39052272 epoch total loss 1.37770092\n",
      "Trained batch 1183 batch loss 1.26709163 epoch total loss 1.37760746\n",
      "Trained batch 1184 batch loss 1.42392135 epoch total loss 1.37764657\n",
      "Trained batch 1185 batch loss 1.31632066 epoch total loss 1.37759483\n",
      "Trained batch 1186 batch loss 1.25589216 epoch total loss 1.37749219\n",
      "Trained batch 1187 batch loss 1.19600642 epoch total loss 1.37733924\n",
      "Trained batch 1188 batch loss 1.14699209 epoch total loss 1.37714541\n",
      "Trained batch 1189 batch loss 1.26684964 epoch total loss 1.37705266\n",
      "Trained batch 1190 batch loss 1.27986336 epoch total loss 1.37697101\n",
      "Trained batch 1191 batch loss 1.23377705 epoch total loss 1.37685072\n",
      "Trained batch 1192 batch loss 1.1897974 epoch total loss 1.37669384\n",
      "Trained batch 1193 batch loss 1.30727696 epoch total loss 1.37663567\n",
      "Trained batch 1194 batch loss 1.3614068 epoch total loss 1.37662292\n",
      "Trained batch 1195 batch loss 1.29295659 epoch total loss 1.37655294\n",
      "Trained batch 1196 batch loss 1.32800388 epoch total loss 1.37651229\n",
      "Trained batch 1197 batch loss 1.3203274 epoch total loss 1.37646532\n",
      "Trained batch 1198 batch loss 1.31674433 epoch total loss 1.37641549\n",
      "Trained batch 1199 batch loss 1.40302706 epoch total loss 1.37643778\n",
      "Trained batch 1200 batch loss 1.33889866 epoch total loss 1.37640643\n",
      "Trained batch 1201 batch loss 1.27203977 epoch total loss 1.37631965\n",
      "Trained batch 1202 batch loss 1.24292815 epoch total loss 1.37620866\n",
      "Trained batch 1203 batch loss 1.23670983 epoch total loss 1.37609267\n",
      "Trained batch 1204 batch loss 1.26222587 epoch total loss 1.37599802\n",
      "Trained batch 1205 batch loss 1.32817888 epoch total loss 1.37595832\n",
      "Trained batch 1206 batch loss 1.43429184 epoch total loss 1.37600672\n",
      "Trained batch 1207 batch loss 1.57207501 epoch total loss 1.37616909\n",
      "Trained batch 1208 batch loss 1.38779819 epoch total loss 1.37617874\n",
      "Trained batch 1209 batch loss 1.58570266 epoch total loss 1.37635207\n",
      "Trained batch 1210 batch loss 1.36351192 epoch total loss 1.37634146\n",
      "Trained batch 1211 batch loss 1.27105892 epoch total loss 1.37625456\n",
      "Trained batch 1212 batch loss 1.07773542 epoch total loss 1.37600827\n",
      "Trained batch 1213 batch loss 1.09897494 epoch total loss 1.37577987\n",
      "Trained batch 1214 batch loss 1.08172417 epoch total loss 1.37553763\n",
      "Trained batch 1215 batch loss 1.20458674 epoch total loss 1.37539697\n",
      "Trained batch 1216 batch loss 1.15467012 epoch total loss 1.37521541\n",
      "Trained batch 1217 batch loss 1.24543309 epoch total loss 1.37510884\n",
      "Trained batch 1218 batch loss 1.29869628 epoch total loss 1.37504613\n",
      "Trained batch 1219 batch loss 1.39314938 epoch total loss 1.37506104\n",
      "Trained batch 1220 batch loss 1.38840199 epoch total loss 1.37507188\n",
      "Trained batch 1221 batch loss 1.40791202 epoch total loss 1.37509882\n",
      "Trained batch 1222 batch loss 1.33536875 epoch total loss 1.37506628\n",
      "Trained batch 1223 batch loss 1.40856934 epoch total loss 1.3750937\n",
      "Trained batch 1224 batch loss 1.50074244 epoch total loss 1.37519634\n",
      "Trained batch 1225 batch loss 1.38181353 epoch total loss 1.37520182\n",
      "Trained batch 1226 batch loss 1.31972528 epoch total loss 1.37515652\n",
      "Trained batch 1227 batch loss 1.35053253 epoch total loss 1.37513649\n",
      "Trained batch 1228 batch loss 1.33021569 epoch total loss 1.3750999\n",
      "Trained batch 1229 batch loss 1.39125514 epoch total loss 1.37511301\n",
      "Trained batch 1230 batch loss 1.26400912 epoch total loss 1.37502277\n",
      "Trained batch 1231 batch loss 1.25026381 epoch total loss 1.37492132\n",
      "Trained batch 1232 batch loss 1.25326538 epoch total loss 1.37482262\n",
      "Trained batch 1233 batch loss 1.24303281 epoch total loss 1.37471581\n",
      "Trained batch 1234 batch loss 1.25205672 epoch total loss 1.37461638\n",
      "Trained batch 1235 batch loss 1.36350155 epoch total loss 1.37460744\n",
      "Trained batch 1236 batch loss 1.32552743 epoch total loss 1.37456775\n",
      "Trained batch 1237 batch loss 1.24242437 epoch total loss 1.37446094\n",
      "Trained batch 1238 batch loss 1.26065052 epoch total loss 1.37436891\n",
      "Trained batch 1239 batch loss 1.31484425 epoch total loss 1.37432086\n",
      "Trained batch 1240 batch loss 1.31319976 epoch total loss 1.37427163\n",
      "Trained batch 1241 batch loss 1.29183817 epoch total loss 1.37420523\n",
      "Trained batch 1242 batch loss 1.22164917 epoch total loss 1.37408245\n",
      "Trained batch 1243 batch loss 1.31131828 epoch total loss 1.3740319\n",
      "Trained batch 1244 batch loss 1.20607185 epoch total loss 1.37389684\n",
      "Trained batch 1245 batch loss 1.23545837 epoch total loss 1.37378561\n",
      "Trained batch 1246 batch loss 1.27934575 epoch total loss 1.3737098\n",
      "Trained batch 1247 batch loss 1.1594286 epoch total loss 1.37353802\n",
      "Trained batch 1248 batch loss 1.20155811 epoch total loss 1.37340021\n",
      "Trained batch 1249 batch loss 1.19923043 epoch total loss 1.37326074\n",
      "Trained batch 1250 batch loss 1.27929366 epoch total loss 1.37318552\n",
      "Trained batch 1251 batch loss 1.32283676 epoch total loss 1.37314534\n",
      "Trained batch 1252 batch loss 1.25647581 epoch total loss 1.37305212\n",
      "Trained batch 1253 batch loss 1.32277036 epoch total loss 1.37301195\n",
      "Trained batch 1254 batch loss 1.43071222 epoch total loss 1.37305796\n",
      "Trained batch 1255 batch loss 1.36506319 epoch total loss 1.37305164\n",
      "Trained batch 1256 batch loss 1.37166309 epoch total loss 1.37305057\n",
      "Trained batch 1257 batch loss 1.37345123 epoch total loss 1.37305081\n",
      "Trained batch 1258 batch loss 1.22689652 epoch total loss 1.3729347\n",
      "Trained batch 1259 batch loss 1.21823323 epoch total loss 1.37281179\n",
      "Trained batch 1260 batch loss 1.19097865 epoch total loss 1.37266743\n",
      "Trained batch 1261 batch loss 1.22416604 epoch total loss 1.37254965\n",
      "Trained batch 1262 batch loss 1.3465302 epoch total loss 1.37252915\n",
      "Trained batch 1263 batch loss 1.34148216 epoch total loss 1.37250447\n",
      "Trained batch 1264 batch loss 1.37713 epoch total loss 1.37250805\n",
      "Trained batch 1265 batch loss 1.4360466 epoch total loss 1.37255836\n",
      "Trained batch 1266 batch loss 1.43255019 epoch total loss 1.37260568\n",
      "Trained batch 1267 batch loss 1.30179429 epoch total loss 1.37254977\n",
      "Trained batch 1268 batch loss 1.39915562 epoch total loss 1.37257075\n",
      "Trained batch 1269 batch loss 1.36483693 epoch total loss 1.37256467\n",
      "Trained batch 1270 batch loss 1.30990195 epoch total loss 1.37251532\n",
      "Trained batch 1271 batch loss 1.35343361 epoch total loss 1.3725003\n",
      "Trained batch 1272 batch loss 1.43523109 epoch total loss 1.37254953\n",
      "Trained batch 1273 batch loss 1.41957235 epoch total loss 1.37258649\n",
      "Trained batch 1274 batch loss 1.32690227 epoch total loss 1.37255061\n",
      "Trained batch 1275 batch loss 1.39006114 epoch total loss 1.37256432\n",
      "Trained batch 1276 batch loss 1.42345572 epoch total loss 1.37260425\n",
      "Trained batch 1277 batch loss 1.51116276 epoch total loss 1.37271273\n",
      "Trained batch 1278 batch loss 1.38913023 epoch total loss 1.37272561\n",
      "Trained batch 1279 batch loss 1.3019011 epoch total loss 1.37267017\n",
      "Trained batch 1280 batch loss 1.35883343 epoch total loss 1.37265944\n",
      "Trained batch 1281 batch loss 1.3879118 epoch total loss 1.37267137\n",
      "Trained batch 1282 batch loss 1.26757276 epoch total loss 1.37258935\n",
      "Trained batch 1283 batch loss 1.41959238 epoch total loss 1.37262595\n",
      "Trained batch 1284 batch loss 1.30538321 epoch total loss 1.37257361\n",
      "Trained batch 1285 batch loss 1.26583624 epoch total loss 1.37249053\n",
      "Trained batch 1286 batch loss 1.27455187 epoch total loss 1.37241435\n",
      "Trained batch 1287 batch loss 1.30360746 epoch total loss 1.37236094\n",
      "Trained batch 1288 batch loss 1.29738581 epoch total loss 1.37230265\n",
      "Trained batch 1289 batch loss 1.20410705 epoch total loss 1.37217224\n",
      "Trained batch 1290 batch loss 1.21310461 epoch total loss 1.37204897\n",
      "Trained batch 1291 batch loss 1.2363348 epoch total loss 1.37194383\n",
      "Trained batch 1292 batch loss 1.2150166 epoch total loss 1.37182236\n",
      "Trained batch 1293 batch loss 1.19553566 epoch total loss 1.37168598\n",
      "Trained batch 1294 batch loss 1.26443946 epoch total loss 1.37160301\n",
      "Trained batch 1295 batch loss 1.30053186 epoch total loss 1.37154818\n",
      "Trained batch 1296 batch loss 1.22654057 epoch total loss 1.37143636\n",
      "Trained batch 1297 batch loss 1.42358017 epoch total loss 1.37147653\n",
      "Trained batch 1298 batch loss 1.3271656 epoch total loss 1.37144232\n",
      "Trained batch 1299 batch loss 1.48611689 epoch total loss 1.37153065\n",
      "Trained batch 1300 batch loss 1.42089891 epoch total loss 1.37156856\n",
      "Trained batch 1301 batch loss 1.34404576 epoch total loss 1.37154746\n",
      "Trained batch 1302 batch loss 1.2328459 epoch total loss 1.37144089\n",
      "Trained batch 1303 batch loss 1.42134166 epoch total loss 1.37147915\n",
      "Trained batch 1304 batch loss 1.46454144 epoch total loss 1.37155056\n",
      "Trained batch 1305 batch loss 1.29577088 epoch total loss 1.37149251\n",
      "Trained batch 1306 batch loss 1.40495169 epoch total loss 1.37151814\n",
      "Trained batch 1307 batch loss 1.33507991 epoch total loss 1.37149024\n",
      "Trained batch 1308 batch loss 1.34655595 epoch total loss 1.37147117\n",
      "Trained batch 1309 batch loss 1.39127946 epoch total loss 1.37148631\n",
      "Trained batch 1310 batch loss 1.35535049 epoch total loss 1.37147391\n",
      "Trained batch 1311 batch loss 1.42909908 epoch total loss 1.3715179\n",
      "Trained batch 1312 batch loss 1.35702085 epoch total loss 1.37150681\n",
      "Trained batch 1313 batch loss 1.24942541 epoch total loss 1.37141383\n",
      "Trained batch 1314 batch loss 1.32892513 epoch total loss 1.37138152\n",
      "Trained batch 1315 batch loss 1.44979215 epoch total loss 1.37144125\n",
      "Trained batch 1316 batch loss 1.32581723 epoch total loss 1.37140656\n",
      "Trained batch 1317 batch loss 1.31023383 epoch total loss 1.37136006\n",
      "Trained batch 1318 batch loss 1.36768377 epoch total loss 1.3713572\n",
      "Trained batch 1319 batch loss 1.35617518 epoch total loss 1.37134576\n",
      "Trained batch 1320 batch loss 1.360129 epoch total loss 1.37133729\n",
      "Trained batch 1321 batch loss 1.34873748 epoch total loss 1.37132013\n",
      "Trained batch 1322 batch loss 1.40465486 epoch total loss 1.3713454\n",
      "Trained batch 1323 batch loss 1.23018074 epoch total loss 1.37123871\n",
      "Trained batch 1324 batch loss 1.2519908 epoch total loss 1.37114859\n",
      "Trained batch 1325 batch loss 1.38481879 epoch total loss 1.37115884\n",
      "Trained batch 1326 batch loss 1.35814381 epoch total loss 1.37114906\n",
      "Trained batch 1327 batch loss 1.23582423 epoch total loss 1.37104714\n",
      "Trained batch 1328 batch loss 1.29878 epoch total loss 1.37099278\n",
      "Trained batch 1329 batch loss 1.29794931 epoch total loss 1.37093782\n",
      "Trained batch 1330 batch loss 1.30447364 epoch total loss 1.37088776\n",
      "Trained batch 1331 batch loss 1.30868578 epoch total loss 1.37084103\n",
      "Trained batch 1332 batch loss 1.31902313 epoch total loss 1.37080216\n",
      "Trained batch 1333 batch loss 1.33951592 epoch total loss 1.37077868\n",
      "Trained batch 1334 batch loss 1.41731524 epoch total loss 1.37081361\n",
      "Trained batch 1335 batch loss 1.28853381 epoch total loss 1.37075198\n",
      "Trained batch 1336 batch loss 1.27470767 epoch total loss 1.37068\n",
      "Trained batch 1337 batch loss 1.20042288 epoch total loss 1.37055266\n",
      "Trained batch 1338 batch loss 1.31434464 epoch total loss 1.3705107\n",
      "Trained batch 1339 batch loss 1.45974612 epoch total loss 1.37057734\n",
      "Trained batch 1340 batch loss 1.37117088 epoch total loss 1.37057781\n",
      "Trained batch 1341 batch loss 1.27486539 epoch total loss 1.37050641\n",
      "Trained batch 1342 batch loss 1.42004132 epoch total loss 1.37054336\n",
      "Trained batch 1343 batch loss 1.25700092 epoch total loss 1.37045872\n",
      "Trained batch 1344 batch loss 1.26720417 epoch total loss 1.37038195\n",
      "Trained batch 1345 batch loss 1.23615599 epoch total loss 1.37028217\n",
      "Trained batch 1346 batch loss 1.29972649 epoch total loss 1.37022972\n",
      "Trained batch 1347 batch loss 1.21644509 epoch total loss 1.37011552\n",
      "Trained batch 1348 batch loss 1.22579181 epoch total loss 1.37000847\n",
      "Trained batch 1349 batch loss 1.37201726 epoch total loss 1.37001\n",
      "Trained batch 1350 batch loss 1.35237312 epoch total loss 1.36999702\n",
      "Trained batch 1351 batch loss 1.39645076 epoch total loss 1.37001657\n",
      "Trained batch 1352 batch loss 1.31831598 epoch total loss 1.36997843\n",
      "Trained batch 1353 batch loss 1.19767427 epoch total loss 1.36985099\n",
      "Trained batch 1354 batch loss 1.24601507 epoch total loss 1.36975956\n",
      "Trained batch 1355 batch loss 1.23406482 epoch total loss 1.3696593\n",
      "Trained batch 1356 batch loss 1.24727666 epoch total loss 1.36956918\n",
      "Trained batch 1357 batch loss 1.37043285 epoch total loss 1.36956978\n",
      "Trained batch 1358 batch loss 1.4077599 epoch total loss 1.36959791\n",
      "Trained batch 1359 batch loss 1.38172424 epoch total loss 1.36960685\n",
      "Trained batch 1360 batch loss 1.26983714 epoch total loss 1.36953354\n",
      "Trained batch 1361 batch loss 1.3047601 epoch total loss 1.36948597\n",
      "Trained batch 1362 batch loss 1.27241158 epoch total loss 1.36941469\n",
      "Trained batch 1363 batch loss 1.28833473 epoch total loss 1.3693552\n",
      "Trained batch 1364 batch loss 1.30856872 epoch total loss 1.36931062\n",
      "Trained batch 1365 batch loss 1.37791693 epoch total loss 1.36931694\n",
      "Trained batch 1366 batch loss 1.34609783 epoch total loss 1.3693\n",
      "Trained batch 1367 batch loss 1.34335709 epoch total loss 1.36928105\n",
      "Trained batch 1368 batch loss 1.41354954 epoch total loss 1.36931336\n",
      "Trained batch 1369 batch loss 1.58986557 epoch total loss 1.36947441\n",
      "Trained batch 1370 batch loss 1.38182628 epoch total loss 1.36948347\n",
      "Trained batch 1371 batch loss 1.5273385 epoch total loss 1.36959863\n",
      "Trained batch 1372 batch loss 1.33625865 epoch total loss 1.36957431\n",
      "Trained batch 1373 batch loss 1.38895667 epoch total loss 1.36958849\n",
      "Trained batch 1374 batch loss 1.26959467 epoch total loss 1.36951578\n",
      "Trained batch 1375 batch loss 1.17901278 epoch total loss 1.36937714\n",
      "Trained batch 1376 batch loss 1.44015861 epoch total loss 1.36942863\n",
      "Trained batch 1377 batch loss 1.34922624 epoch total loss 1.36941397\n",
      "Trained batch 1378 batch loss 1.21082461 epoch total loss 1.36929882\n",
      "Trained batch 1379 batch loss 1.15715599 epoch total loss 1.36914492\n",
      "Trained batch 1380 batch loss 1.22719228 epoch total loss 1.36904204\n",
      "Trained batch 1381 batch loss 1.17900252 epoch total loss 1.36890447\n",
      "Trained batch 1382 batch loss 1.50533426 epoch total loss 1.36900318\n",
      "Trained batch 1383 batch loss 1.56612575 epoch total loss 1.36914575\n",
      "Trained batch 1384 batch loss 1.52604246 epoch total loss 1.36925912\n",
      "Trained batch 1385 batch loss 1.46490741 epoch total loss 1.36932814\n",
      "Trained batch 1386 batch loss 1.43467677 epoch total loss 1.36937535\n",
      "Trained batch 1387 batch loss 1.53580379 epoch total loss 1.36949527\n",
      "Trained batch 1388 batch loss 1.48620439 epoch total loss 1.36957943\n",
      "Epoch 2 train loss 1.3695794343948364\n",
      "Validated batch 1 batch loss 1.32403183\n",
      "Validated batch 2 batch loss 1.28855586\n",
      "Validated batch 3 batch loss 1.33802891\n",
      "Validated batch 4 batch loss 1.32694495\n",
      "Validated batch 5 batch loss 1.34376884\n",
      "Validated batch 6 batch loss 1.37897933\n",
      "Validated batch 7 batch loss 1.36927438\n",
      "Validated batch 8 batch loss 1.33442783\n",
      "Validated batch 9 batch loss 1.40054893\n",
      "Validated batch 10 batch loss 1.38234282\n",
      "Validated batch 11 batch loss 1.37189674\n",
      "Validated batch 12 batch loss 1.39095795\n",
      "Validated batch 13 batch loss 1.34849012\n",
      "Validated batch 14 batch loss 1.31487358\n",
      "Validated batch 15 batch loss 1.36928141\n",
      "Validated batch 16 batch loss 1.35134053\n",
      "Validated batch 17 batch loss 1.46151519\n",
      "Validated batch 18 batch loss 1.40548682\n",
      "Validated batch 19 batch loss 1.27202797\n",
      "Validated batch 20 batch loss 1.46223712\n",
      "Validated batch 21 batch loss 1.31869054\n",
      "Validated batch 22 batch loss 1.29937172\n",
      "Validated batch 23 batch loss 1.36127865\n",
      "Validated batch 24 batch loss 1.44768107\n",
      "Validated batch 25 batch loss 1.37405491\n",
      "Validated batch 26 batch loss 1.28922343\n",
      "Validated batch 27 batch loss 1.28471494\n",
      "Validated batch 28 batch loss 1.3229537\n",
      "Validated batch 29 batch loss 1.39055705\n",
      "Validated batch 30 batch loss 1.34521937\n",
      "Validated batch 31 batch loss 1.31439114\n",
      "Validated batch 32 batch loss 1.37447691\n",
      "Validated batch 33 batch loss 1.393893\n",
      "Validated batch 34 batch loss 1.28611767\n",
      "Validated batch 35 batch loss 1.28701472\n",
      "Validated batch 36 batch loss 1.38088989\n",
      "Validated batch 37 batch loss 1.35326171\n",
      "Validated batch 38 batch loss 1.49954927\n",
      "Validated batch 39 batch loss 1.46822631\n",
      "Validated batch 40 batch loss 1.33423078\n",
      "Validated batch 41 batch loss 1.5009892\n",
      "Validated batch 42 batch loss 1.33592534\n",
      "Validated batch 43 batch loss 1.38729811\n",
      "Validated batch 44 batch loss 1.41343141\n",
      "Validated batch 45 batch loss 1.15120554\n",
      "Validated batch 46 batch loss 1.42605174\n",
      "Validated batch 47 batch loss 1.43293488\n",
      "Validated batch 48 batch loss 1.33513927\n",
      "Validated batch 49 batch loss 1.30712676\n",
      "Validated batch 50 batch loss 1.31534696\n",
      "Validated batch 51 batch loss 1.36227632\n",
      "Validated batch 52 batch loss 1.47307408\n",
      "Validated batch 53 batch loss 1.27185798\n",
      "Validated batch 54 batch loss 1.4048574\n",
      "Validated batch 55 batch loss 1.36224937\n",
      "Validated batch 56 batch loss 1.37201798\n",
      "Validated batch 57 batch loss 1.36735213\n",
      "Validated batch 58 batch loss 1.23739421\n",
      "Validated batch 59 batch loss 1.47957182\n",
      "Validated batch 60 batch loss 1.31525981\n",
      "Validated batch 61 batch loss 1.42296457\n",
      "Validated batch 62 batch loss 1.31441772\n",
      "Validated batch 63 batch loss 1.43108451\n",
      "Validated batch 64 batch loss 1.26894391\n",
      "Validated batch 65 batch loss 1.34698486\n",
      "Validated batch 66 batch loss 1.29117131\n",
      "Validated batch 67 batch loss 1.30938888\n",
      "Validated batch 68 batch loss 1.38482153\n",
      "Validated batch 69 batch loss 1.3714819\n",
      "Validated batch 70 batch loss 1.35907185\n",
      "Validated batch 71 batch loss 1.35924244\n",
      "Validated batch 72 batch loss 1.34294724\n",
      "Validated batch 73 batch loss 1.23165417\n",
      "Validated batch 74 batch loss 1.27594578\n",
      "Validated batch 75 batch loss 1.42033887\n",
      "Validated batch 76 batch loss 1.28261948\n",
      "Validated batch 77 batch loss 1.25660217\n",
      "Validated batch 78 batch loss 1.31345725\n",
      "Validated batch 79 batch loss 1.3575629\n",
      "Validated batch 80 batch loss 1.24184024\n",
      "Validated batch 81 batch loss 1.36746645\n",
      "Validated batch 82 batch loss 1.4048928\n",
      "Validated batch 83 batch loss 1.30487657\n",
      "Validated batch 84 batch loss 1.41741669\n",
      "Validated batch 85 batch loss 1.46456456\n",
      "Validated batch 86 batch loss 1.3318758\n",
      "Validated batch 87 batch loss 1.4420253\n",
      "Validated batch 88 batch loss 1.24844444\n",
      "Validated batch 89 batch loss 1.30617571\n",
      "Validated batch 90 batch loss 1.30418169\n",
      "Validated batch 91 batch loss 1.36050403\n",
      "Validated batch 92 batch loss 1.50358677\n",
      "Validated batch 93 batch loss 1.28975797\n",
      "Validated batch 94 batch loss 1.36904013\n",
      "Validated batch 95 batch loss 1.44981432\n",
      "Validated batch 96 batch loss 1.30454266\n",
      "Validated batch 97 batch loss 1.41412687\n",
      "Validated batch 98 batch loss 1.49117661\n",
      "Validated batch 99 batch loss 1.20420897\n",
      "Validated batch 100 batch loss 1.32536829\n",
      "Validated batch 101 batch loss 1.28738165\n",
      "Validated batch 102 batch loss 1.40245402\n",
      "Validated batch 103 batch loss 1.40507054\n",
      "Validated batch 104 batch loss 1.27803087\n",
      "Validated batch 105 batch loss 1.16941166\n",
      "Validated batch 106 batch loss 1.35112512\n",
      "Validated batch 107 batch loss 1.29679\n",
      "Validated batch 108 batch loss 1.34518409\n",
      "Validated batch 109 batch loss 1.35225594\n",
      "Validated batch 110 batch loss 1.27665472\n",
      "Validated batch 111 batch loss 1.34869552\n",
      "Validated batch 112 batch loss 1.42735982\n",
      "Validated batch 113 batch loss 1.38164258\n",
      "Validated batch 114 batch loss 1.353912\n",
      "Validated batch 115 batch loss 1.22632277\n",
      "Validated batch 116 batch loss 1.32023\n",
      "Validated batch 117 batch loss 1.26982677\n",
      "Validated batch 118 batch loss 1.32439089\n",
      "Validated batch 119 batch loss 1.27037024\n",
      "Validated batch 120 batch loss 1.27798271\n",
      "Validated batch 121 batch loss 1.4410913\n",
      "Validated batch 122 batch loss 1.33617496\n",
      "Validated batch 123 batch loss 1.4204421\n",
      "Validated batch 124 batch loss 1.4343158\n",
      "Validated batch 125 batch loss 1.4064256\n",
      "Validated batch 126 batch loss 1.37690616\n",
      "Validated batch 127 batch loss 1.47065389\n",
      "Validated batch 128 batch loss 1.37649894\n",
      "Validated batch 129 batch loss 1.4335537\n",
      "Validated batch 130 batch loss 1.4223814\n",
      "Validated batch 131 batch loss 1.4838587\n",
      "Validated batch 132 batch loss 1.41441202\n",
      "Validated batch 133 batch loss 1.30017459\n",
      "Validated batch 134 batch loss 1.37108696\n",
      "Validated batch 135 batch loss 1.41055155\n",
      "Validated batch 136 batch loss 1.43226457\n",
      "Validated batch 137 batch loss 1.35456729\n",
      "Validated batch 138 batch loss 1.38182247\n",
      "Validated batch 139 batch loss 1.33807766\n",
      "Validated batch 140 batch loss 1.44530642\n",
      "Validated batch 141 batch loss 1.3363564\n",
      "Validated batch 142 batch loss 1.27696633\n",
      "Validated batch 143 batch loss 1.34562397\n",
      "Validated batch 144 batch loss 1.3926475\n",
      "Validated batch 145 batch loss 1.43111694\n",
      "Validated batch 146 batch loss 1.45314431\n",
      "Validated batch 147 batch loss 1.38725662\n",
      "Validated batch 148 batch loss 1.36300862\n",
      "Validated batch 149 batch loss 1.40044594\n",
      "Validated batch 150 batch loss 1.39777637\n",
      "Validated batch 151 batch loss 1.33587551\n",
      "Validated batch 152 batch loss 1.42199171\n",
      "Validated batch 153 batch loss 1.45107019\n",
      "Validated batch 154 batch loss 1.37633586\n",
      "Validated batch 155 batch loss 1.46251094\n",
      "Validated batch 156 batch loss 1.31633818\n",
      "Validated batch 157 batch loss 1.30810487\n",
      "Validated batch 158 batch loss 1.32831573\n",
      "Validated batch 159 batch loss 1.27539158\n",
      "Validated batch 160 batch loss 1.44878125\n",
      "Validated batch 161 batch loss 1.34337401\n",
      "Validated batch 162 batch loss 1.42995906\n",
      "Validated batch 163 batch loss 1.35625684\n",
      "Validated batch 164 batch loss 1.35867906\n",
      "Validated batch 165 batch loss 1.34341097\n",
      "Validated batch 166 batch loss 1.28773701\n",
      "Validated batch 167 batch loss 1.37785316\n",
      "Validated batch 168 batch loss 1.38773525\n",
      "Validated batch 169 batch loss 1.29117489\n",
      "Validated batch 170 batch loss 1.27857339\n",
      "Validated batch 171 batch loss 1.37314546\n",
      "Validated batch 172 batch loss 1.36887026\n",
      "Validated batch 173 batch loss 1.42567909\n",
      "Validated batch 174 batch loss 1.36634123\n",
      "Validated batch 175 batch loss 1.28250015\n",
      "Validated batch 176 batch loss 1.34340453\n",
      "Validated batch 177 batch loss 1.3590982\n",
      "Validated batch 178 batch loss 1.34570396\n",
      "Validated batch 179 batch loss 1.36311531\n",
      "Validated batch 180 batch loss 1.3962822\n",
      "Validated batch 181 batch loss 1.51229262\n",
      "Validated batch 182 batch loss 1.46267939\n",
      "Validated batch 183 batch loss 1.36428356\n",
      "Validated batch 184 batch loss 1.28134084\n",
      "Validated batch 185 batch loss 1.2061491\n",
      "Epoch 2 val loss 1.358535647392273\n",
      "Model /aiffel/aiffel/mpii/models/model-epoch-2-loss-1.3585.h5 saved.\n",
      "Start epoch 3 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.20530617 epoch total loss 1.20530617\n",
      "Trained batch 2 batch loss 1.21228361 epoch total loss 1.20879483\n",
      "Trained batch 3 batch loss 1.26037836 epoch total loss 1.22598934\n",
      "Trained batch 4 batch loss 1.36019588 epoch total loss 1.25954103\n",
      "Trained batch 5 batch loss 1.29548669 epoch total loss 1.26673007\n",
      "Trained batch 6 batch loss 1.34810686 epoch total loss 1.28029287\n",
      "Trained batch 7 batch loss 1.46280289 epoch total loss 1.30636573\n",
      "Trained batch 8 batch loss 1.43376255 epoch total loss 1.3222903\n",
      "Trained batch 9 batch loss 1.43390191 epoch total loss 1.33469152\n",
      "Trained batch 10 batch loss 1.41629875 epoch total loss 1.34285235\n",
      "Trained batch 11 batch loss 1.28840113 epoch total loss 1.33790219\n",
      "Trained batch 12 batch loss 1.23138106 epoch total loss 1.32902551\n",
      "Trained batch 13 batch loss 1.36548 epoch total loss 1.33182967\n",
      "Trained batch 14 batch loss 1.42247963 epoch total loss 1.33830464\n",
      "Trained batch 15 batch loss 1.21212733 epoch total loss 1.32989287\n",
      "Trained batch 16 batch loss 1.23336744 epoch total loss 1.32386\n",
      "Trained batch 17 batch loss 1.25931835 epoch total loss 1.32006347\n",
      "Trained batch 18 batch loss 1.28022611 epoch total loss 1.31785023\n",
      "Trained batch 19 batch loss 1.16072583 epoch total loss 1.30958056\n",
      "Trained batch 20 batch loss 1.31373835 epoch total loss 1.30978847\n",
      "Trained batch 21 batch loss 1.3029809 epoch total loss 1.30946422\n",
      "Trained batch 22 batch loss 1.35244668 epoch total loss 1.31141806\n",
      "Trained batch 23 batch loss 1.33595908 epoch total loss 1.31248498\n",
      "Trained batch 24 batch loss 1.32444072 epoch total loss 1.31298316\n",
      "Trained batch 25 batch loss 1.39393413 epoch total loss 1.31622112\n",
      "Trained batch 26 batch loss 1.35563207 epoch total loss 1.31773698\n",
      "Trained batch 27 batch loss 1.37934339 epoch total loss 1.32001877\n",
      "Trained batch 28 batch loss 1.36130905 epoch total loss 1.32149339\n",
      "Trained batch 29 batch loss 1.32464933 epoch total loss 1.32160223\n",
      "Trained batch 30 batch loss 1.41947412 epoch total loss 1.32486475\n",
      "Trained batch 31 batch loss 1.30444658 epoch total loss 1.32420611\n",
      "Trained batch 32 batch loss 1.21546245 epoch total loss 1.32080781\n",
      "Trained batch 33 batch loss 1.23958194 epoch total loss 1.31834638\n",
      "Trained batch 34 batch loss 1.30251217 epoch total loss 1.31788075\n",
      "Trained batch 35 batch loss 1.29997766 epoch total loss 1.31736922\n",
      "Trained batch 36 batch loss 1.22293806 epoch total loss 1.31474614\n",
      "Trained batch 37 batch loss 1.21824074 epoch total loss 1.31213796\n",
      "Trained batch 38 batch loss 1.29499114 epoch total loss 1.31168664\n",
      "Trained batch 39 batch loss 1.28692925 epoch total loss 1.31105185\n",
      "Trained batch 40 batch loss 1.20527804 epoch total loss 1.30840755\n",
      "Trained batch 41 batch loss 1.3278501 epoch total loss 1.30888176\n",
      "Trained batch 42 batch loss 1.35239315 epoch total loss 1.30991769\n",
      "Trained batch 43 batch loss 1.42383516 epoch total loss 1.312567\n",
      "Trained batch 44 batch loss 1.40698934 epoch total loss 1.314713\n",
      "Trained batch 45 batch loss 1.35536802 epoch total loss 1.31561649\n",
      "Trained batch 46 batch loss 1.35352969 epoch total loss 1.3164407\n",
      "Trained batch 47 batch loss 1.27929032 epoch total loss 1.31565022\n",
      "Trained batch 48 batch loss 1.37151396 epoch total loss 1.31681406\n",
      "Trained batch 49 batch loss 1.36429453 epoch total loss 1.317783\n",
      "Trained batch 50 batch loss 1.2992959 epoch total loss 1.31741321\n",
      "Trained batch 51 batch loss 1.35991585 epoch total loss 1.3182466\n",
      "Trained batch 52 batch loss 1.34242272 epoch total loss 1.31871152\n",
      "Trained batch 53 batch loss 1.31014049 epoch total loss 1.31854987\n",
      "Trained batch 54 batch loss 1.31793523 epoch total loss 1.31853843\n",
      "Trained batch 55 batch loss 1.3320781 epoch total loss 1.31878459\n",
      "Trained batch 56 batch loss 1.4821955 epoch total loss 1.3217026\n",
      "Trained batch 57 batch loss 1.26886606 epoch total loss 1.32077563\n",
      "Trained batch 58 batch loss 1.23412168 epoch total loss 1.31928158\n",
      "Trained batch 59 batch loss 1.18221545 epoch total loss 1.31695843\n",
      "Trained batch 60 batch loss 1.10733223 epoch total loss 1.31346464\n",
      "Trained batch 61 batch loss 1.20290899 epoch total loss 1.3116523\n",
      "Trained batch 62 batch loss 1.24074626 epoch total loss 1.31050861\n",
      "Trained batch 63 batch loss 1.21503806 epoch total loss 1.3089931\n",
      "Trained batch 64 batch loss 1.13247776 epoch total loss 1.30623507\n",
      "Trained batch 65 batch loss 1.20024323 epoch total loss 1.30460441\n",
      "Trained batch 66 batch loss 1.23840761 epoch total loss 1.3036015\n",
      "Trained batch 67 batch loss 1.35980666 epoch total loss 1.30444038\n",
      "Trained batch 68 batch loss 1.48626804 epoch total loss 1.30711436\n",
      "Trained batch 69 batch loss 1.38161612 epoch total loss 1.30819404\n",
      "Trained batch 70 batch loss 1.39896739 epoch total loss 1.3094908\n",
      "Trained batch 71 batch loss 1.36631918 epoch total loss 1.31029117\n",
      "Trained batch 72 batch loss 1.30766451 epoch total loss 1.31025457\n",
      "Trained batch 73 batch loss 1.14541125 epoch total loss 1.30799651\n",
      "Trained batch 74 batch loss 1.26319814 epoch total loss 1.30739105\n",
      "Trained batch 75 batch loss 1.28073943 epoch total loss 1.30703568\n",
      "Trained batch 76 batch loss 1.25863528 epoch total loss 1.30639887\n",
      "Trained batch 77 batch loss 1.23167801 epoch total loss 1.30542839\n",
      "Trained batch 78 batch loss 1.13248253 epoch total loss 1.30321121\n",
      "Trained batch 79 batch loss 1.1741519 epoch total loss 1.30157745\n",
      "Trained batch 80 batch loss 1.15400743 epoch total loss 1.29973292\n",
      "Trained batch 81 batch loss 1.44890559 epoch total loss 1.30157447\n",
      "Trained batch 82 batch loss 1.53189778 epoch total loss 1.3043834\n",
      "Trained batch 83 batch loss 1.53844309 epoch total loss 1.30720341\n",
      "Trained batch 84 batch loss 1.34623349 epoch total loss 1.30766809\n",
      "Trained batch 85 batch loss 1.40128088 epoch total loss 1.30876946\n",
      "Trained batch 86 batch loss 1.42343593 epoch total loss 1.31010282\n",
      "Trained batch 87 batch loss 1.36614561 epoch total loss 1.31074691\n",
      "Trained batch 88 batch loss 1.33239758 epoch total loss 1.31099296\n",
      "Trained batch 89 batch loss 1.21546078 epoch total loss 1.3099196\n",
      "Trained batch 90 batch loss 1.14003074 epoch total loss 1.30803192\n",
      "Trained batch 91 batch loss 1.17568243 epoch total loss 1.30657744\n",
      "Trained batch 92 batch loss 1.22098875 epoch total loss 1.30564713\n",
      "Trained batch 93 batch loss 1.29349315 epoch total loss 1.30551648\n",
      "Trained batch 94 batch loss 1.33622468 epoch total loss 1.30584323\n",
      "Trained batch 95 batch loss 1.32836163 epoch total loss 1.30608022\n",
      "Trained batch 96 batch loss 1.29723239 epoch total loss 1.30598807\n",
      "Trained batch 97 batch loss 1.35550022 epoch total loss 1.30649853\n",
      "Trained batch 98 batch loss 1.23840165 epoch total loss 1.30580366\n",
      "Trained batch 99 batch loss 1.21136987 epoch total loss 1.30484974\n",
      "Trained batch 100 batch loss 1.37275827 epoch total loss 1.30552888\n",
      "Trained batch 101 batch loss 1.3352828 epoch total loss 1.30582345\n",
      "Trained batch 102 batch loss 1.07261431 epoch total loss 1.30353713\n",
      "Trained batch 103 batch loss 1.04210651 epoch total loss 1.30099893\n",
      "Trained batch 104 batch loss 1.05649745 epoch total loss 1.298648\n",
      "Trained batch 105 batch loss 1.36547947 epoch total loss 1.29928446\n",
      "Trained batch 106 batch loss 1.53900611 epoch total loss 1.30154586\n",
      "Trained batch 107 batch loss 1.47592783 epoch total loss 1.30317557\n",
      "Trained batch 108 batch loss 1.44008899 epoch total loss 1.30444336\n",
      "Trained batch 109 batch loss 1.48873162 epoch total loss 1.3061341\n",
      "Trained batch 110 batch loss 1.46608412 epoch total loss 1.30758822\n",
      "Trained batch 111 batch loss 1.38202429 epoch total loss 1.30825877\n",
      "Trained batch 112 batch loss 1.31377161 epoch total loss 1.30830789\n",
      "Trained batch 113 batch loss 1.22457337 epoch total loss 1.307567\n",
      "Trained batch 114 batch loss 1.28581095 epoch total loss 1.30737615\n",
      "Trained batch 115 batch loss 1.38921261 epoch total loss 1.30808771\n",
      "Trained batch 116 batch loss 1.36552787 epoch total loss 1.30858278\n",
      "Trained batch 117 batch loss 1.38989925 epoch total loss 1.30927777\n",
      "Trained batch 118 batch loss 1.36040211 epoch total loss 1.30971098\n",
      "Trained batch 119 batch loss 1.45034468 epoch total loss 1.31089282\n",
      "Trained batch 120 batch loss 1.40395951 epoch total loss 1.3116684\n",
      "Trained batch 121 batch loss 1.36433625 epoch total loss 1.31210363\n",
      "Trained batch 122 batch loss 1.33205378 epoch total loss 1.31226707\n",
      "Trained batch 123 batch loss 1.28054619 epoch total loss 1.31200922\n",
      "Trained batch 124 batch loss 1.31065118 epoch total loss 1.31199825\n",
      "Trained batch 125 batch loss 1.32035244 epoch total loss 1.31206524\n",
      "Trained batch 126 batch loss 1.2197721 epoch total loss 1.3113327\n",
      "Trained batch 127 batch loss 1.2485044 epoch total loss 1.31083798\n",
      "Trained batch 128 batch loss 1.22682965 epoch total loss 1.31018174\n",
      "Trained batch 129 batch loss 1.26271021 epoch total loss 1.30981374\n",
      "Trained batch 130 batch loss 1.21842265 epoch total loss 1.30911076\n",
      "Trained batch 131 batch loss 1.28024125 epoch total loss 1.30889046\n",
      "Trained batch 132 batch loss 1.1685909 epoch total loss 1.30782759\n",
      "Trained batch 133 batch loss 1.35982907 epoch total loss 1.3082186\n",
      "Trained batch 134 batch loss 1.20045912 epoch total loss 1.30741441\n",
      "Trained batch 135 batch loss 1.17450166 epoch total loss 1.30642986\n",
      "Trained batch 136 batch loss 1.3118751 epoch total loss 1.30646992\n",
      "Trained batch 137 batch loss 1.2042 epoch total loss 1.30572331\n",
      "Trained batch 138 batch loss 1.2218318 epoch total loss 1.30511546\n",
      "Trained batch 139 batch loss 1.23396337 epoch total loss 1.30460358\n",
      "Trained batch 140 batch loss 1.18309879 epoch total loss 1.30373573\n",
      "Trained batch 141 batch loss 1.1617161 epoch total loss 1.30272841\n",
      "Trained batch 142 batch loss 1.18379617 epoch total loss 1.30189085\n",
      "Trained batch 143 batch loss 1.14603925 epoch total loss 1.30080104\n",
      "Trained batch 144 batch loss 1.24925029 epoch total loss 1.30044305\n",
      "Trained batch 145 batch loss 1.29202139 epoch total loss 1.300385\n",
      "Trained batch 146 batch loss 1.22018254 epoch total loss 1.29983568\n",
      "Trained batch 147 batch loss 1.32228827 epoch total loss 1.29998827\n",
      "Trained batch 148 batch loss 1.28748536 epoch total loss 1.29990387\n",
      "Trained batch 149 batch loss 1.35369229 epoch total loss 1.30026495\n",
      "Trained batch 150 batch loss 1.29650402 epoch total loss 1.30023992\n",
      "Trained batch 151 batch loss 1.36189437 epoch total loss 1.30064821\n",
      "Trained batch 152 batch loss 1.21375394 epoch total loss 1.30007648\n",
      "Trained batch 153 batch loss 1.27071249 epoch total loss 1.29988456\n",
      "Trained batch 154 batch loss 1.37307894 epoch total loss 1.30035985\n",
      "Trained batch 155 batch loss 1.33890808 epoch total loss 1.30060863\n",
      "Trained batch 156 batch loss 1.24554706 epoch total loss 1.30025566\n",
      "Trained batch 157 batch loss 1.30636513 epoch total loss 1.30029452\n",
      "Trained batch 158 batch loss 1.30984831 epoch total loss 1.30035496\n",
      "Trained batch 159 batch loss 1.34201276 epoch total loss 1.30061698\n",
      "Trained batch 160 batch loss 1.3951956 epoch total loss 1.30120814\n",
      "Trained batch 161 batch loss 1.32395399 epoch total loss 1.3013494\n",
      "Trained batch 162 batch loss 1.30749941 epoch total loss 1.30138731\n",
      "Trained batch 163 batch loss 1.21257126 epoch total loss 1.30084252\n",
      "Trained batch 164 batch loss 1.40051663 epoch total loss 1.30145025\n",
      "Trained batch 165 batch loss 1.28711247 epoch total loss 1.30136335\n",
      "Trained batch 166 batch loss 1.33997941 epoch total loss 1.30159593\n",
      "Trained batch 167 batch loss 1.33611047 epoch total loss 1.30180264\n",
      "Trained batch 168 batch loss 1.26500368 epoch total loss 1.30158353\n",
      "Trained batch 169 batch loss 1.28242278 epoch total loss 1.30147016\n",
      "Trained batch 170 batch loss 1.26949847 epoch total loss 1.30128205\n",
      "Trained batch 171 batch loss 1.43218541 epoch total loss 1.30204761\n",
      "Trained batch 172 batch loss 1.35647166 epoch total loss 1.30236411\n",
      "Trained batch 173 batch loss 1.19471681 epoch total loss 1.30174184\n",
      "Trained batch 174 batch loss 1.16494107 epoch total loss 1.30095565\n",
      "Trained batch 175 batch loss 1.34440172 epoch total loss 1.30120397\n",
      "Trained batch 176 batch loss 1.52369618 epoch total loss 1.30246818\n",
      "Trained batch 177 batch loss 1.23369765 epoch total loss 1.30207968\n",
      "Trained batch 178 batch loss 1.21548891 epoch total loss 1.30159318\n",
      "Trained batch 179 batch loss 1.13373256 epoch total loss 1.30065536\n",
      "Trained batch 180 batch loss 1.22571874 epoch total loss 1.30023909\n",
      "Trained batch 181 batch loss 1.27103746 epoch total loss 1.3000778\n",
      "Trained batch 182 batch loss 1.44255674 epoch total loss 1.30086052\n",
      "Trained batch 183 batch loss 1.3405112 epoch total loss 1.30107725\n",
      "Trained batch 184 batch loss 1.29555011 epoch total loss 1.30104721\n",
      "Trained batch 185 batch loss 1.30373061 epoch total loss 1.30106163\n",
      "Trained batch 186 batch loss 1.23988616 epoch total loss 1.30073273\n",
      "Trained batch 187 batch loss 1.29805088 epoch total loss 1.30071843\n",
      "Trained batch 188 batch loss 1.25717163 epoch total loss 1.3004868\n",
      "Trained batch 189 batch loss 1.48565376 epoch total loss 1.30146646\n",
      "Trained batch 190 batch loss 1.43162072 epoch total loss 1.30215156\n",
      "Trained batch 191 batch loss 1.40324152 epoch total loss 1.30268085\n",
      "Trained batch 192 batch loss 1.2593646 epoch total loss 1.30245531\n",
      "Trained batch 193 batch loss 1.22274649 epoch total loss 1.30204225\n",
      "Trained batch 194 batch loss 1.23788428 epoch total loss 1.30171156\n",
      "Trained batch 195 batch loss 1.36854947 epoch total loss 1.30205429\n",
      "Trained batch 196 batch loss 1.27953148 epoch total loss 1.30193937\n",
      "Trained batch 197 batch loss 1.31290698 epoch total loss 1.30199504\n",
      "Trained batch 198 batch loss 1.48478627 epoch total loss 1.30291808\n",
      "Trained batch 199 batch loss 1.38518739 epoch total loss 1.30333149\n",
      "Trained batch 200 batch loss 1.43362987 epoch total loss 1.30398297\n",
      "Trained batch 201 batch loss 1.43935454 epoch total loss 1.30465651\n",
      "Trained batch 202 batch loss 1.49082434 epoch total loss 1.30557811\n",
      "Trained batch 203 batch loss 1.27336717 epoch total loss 1.30541945\n",
      "Trained batch 204 batch loss 1.28179407 epoch total loss 1.30530369\n",
      "Trained batch 205 batch loss 1.22785378 epoch total loss 1.3049258\n",
      "Trained batch 206 batch loss 1.29766786 epoch total loss 1.30489063\n",
      "Trained batch 207 batch loss 1.25507975 epoch total loss 1.30465\n",
      "Trained batch 208 batch loss 1.21429574 epoch total loss 1.30421555\n",
      "Trained batch 209 batch loss 1.20326972 epoch total loss 1.30373251\n",
      "Trained batch 210 batch loss 1.24546182 epoch total loss 1.303455\n",
      "Trained batch 211 batch loss 1.19468427 epoch total loss 1.30293941\n",
      "Trained batch 212 batch loss 1.13322151 epoch total loss 1.30213881\n",
      "Trained batch 213 batch loss 1.16408825 epoch total loss 1.30149078\n",
      "Trained batch 214 batch loss 1.26053882 epoch total loss 1.30129933\n",
      "Trained batch 215 batch loss 1.2018894 epoch total loss 1.30083704\n",
      "Trained batch 216 batch loss 1.18074036 epoch total loss 1.30028105\n",
      "Trained batch 217 batch loss 1.28606832 epoch total loss 1.3002156\n",
      "Trained batch 218 batch loss 1.37416768 epoch total loss 1.30055487\n",
      "Trained batch 219 batch loss 1.3317616 epoch total loss 1.30069733\n",
      "Trained batch 220 batch loss 1.33572316 epoch total loss 1.30085659\n",
      "Trained batch 221 batch loss 1.29486597 epoch total loss 1.30082941\n",
      "Trained batch 222 batch loss 1.37309444 epoch total loss 1.30115497\n",
      "Trained batch 223 batch loss 1.36076283 epoch total loss 1.30142224\n",
      "Trained batch 224 batch loss 1.30828714 epoch total loss 1.30145288\n",
      "Trained batch 225 batch loss 1.23777986 epoch total loss 1.30117\n",
      "Trained batch 226 batch loss 1.24383116 epoch total loss 1.30091631\n",
      "Trained batch 227 batch loss 1.36893535 epoch total loss 1.30121589\n",
      "Trained batch 228 batch loss 1.2910533 epoch total loss 1.3011713\n",
      "Trained batch 229 batch loss 1.41873741 epoch total loss 1.30168462\n",
      "Trained batch 230 batch loss 1.30638254 epoch total loss 1.30170512\n",
      "Trained batch 231 batch loss 1.27905726 epoch total loss 1.30160713\n",
      "Trained batch 232 batch loss 1.36906374 epoch total loss 1.30189776\n",
      "Trained batch 233 batch loss 1.30895829 epoch total loss 1.30192804\n",
      "Trained batch 234 batch loss 1.19314528 epoch total loss 1.30146325\n",
      "Trained batch 235 batch loss 1.28669953 epoch total loss 1.30140042\n",
      "Trained batch 236 batch loss 1.32192755 epoch total loss 1.30148745\n",
      "Trained batch 237 batch loss 1.32785141 epoch total loss 1.30159867\n",
      "Trained batch 238 batch loss 1.30330539 epoch total loss 1.30160582\n",
      "Trained batch 239 batch loss 1.15616393 epoch total loss 1.30099726\n",
      "Trained batch 240 batch loss 1.04392827 epoch total loss 1.29992616\n",
      "Trained batch 241 batch loss 1.10618484 epoch total loss 1.29912221\n",
      "Trained batch 242 batch loss 1.27129352 epoch total loss 1.29900718\n",
      "Trained batch 243 batch loss 1.45695806 epoch total loss 1.29965723\n",
      "Trained batch 244 batch loss 1.64885974 epoch total loss 1.30108845\n",
      "Trained batch 245 batch loss 1.35907388 epoch total loss 1.30132508\n",
      "Trained batch 246 batch loss 1.31337619 epoch total loss 1.30137408\n",
      "Trained batch 247 batch loss 1.26325738 epoch total loss 1.3012197\n",
      "Trained batch 248 batch loss 1.40326142 epoch total loss 1.30163121\n",
      "Trained batch 249 batch loss 1.30479228 epoch total loss 1.30164385\n",
      "Trained batch 250 batch loss 1.34660137 epoch total loss 1.30182362\n",
      "Trained batch 251 batch loss 1.36337698 epoch total loss 1.30206883\n",
      "Trained batch 252 batch loss 1.33399951 epoch total loss 1.30219543\n",
      "Trained batch 253 batch loss 1.36952043 epoch total loss 1.3024615\n",
      "Trained batch 254 batch loss 1.28196192 epoch total loss 1.3023808\n",
      "Trained batch 255 batch loss 1.23776555 epoch total loss 1.30212736\n",
      "Trained batch 256 batch loss 1.20633507 epoch total loss 1.30175316\n",
      "Trained batch 257 batch loss 1.29258633 epoch total loss 1.3017174\n",
      "Trained batch 258 batch loss 1.38433135 epoch total loss 1.30203772\n",
      "Trained batch 259 batch loss 1.18282056 epoch total loss 1.30157745\n",
      "Trained batch 260 batch loss 1.15343225 epoch total loss 1.30100763\n",
      "Trained batch 261 batch loss 1.12760353 epoch total loss 1.30034328\n",
      "Trained batch 262 batch loss 1.45281041 epoch total loss 1.30092525\n",
      "Trained batch 263 batch loss 1.42485309 epoch total loss 1.30139649\n",
      "Trained batch 264 batch loss 1.41934967 epoch total loss 1.30184329\n",
      "Trained batch 265 batch loss 1.42545891 epoch total loss 1.30230963\n",
      "Trained batch 266 batch loss 1.33455205 epoch total loss 1.30243087\n",
      "Trained batch 267 batch loss 1.35221446 epoch total loss 1.30261731\n",
      "Trained batch 268 batch loss 1.27726531 epoch total loss 1.30252266\n",
      "Trained batch 269 batch loss 1.35381198 epoch total loss 1.30271339\n",
      "Trained batch 270 batch loss 1.30615675 epoch total loss 1.30272615\n",
      "Trained batch 271 batch loss 1.32639432 epoch total loss 1.30281341\n",
      "Trained batch 272 batch loss 1.29749799 epoch total loss 1.30279386\n",
      "Trained batch 273 batch loss 1.31629395 epoch total loss 1.30284321\n",
      "Trained batch 274 batch loss 1.1739291 epoch total loss 1.30237269\n",
      "Trained batch 275 batch loss 1.17841625 epoch total loss 1.30192196\n",
      "Trained batch 276 batch loss 1.21813536 epoch total loss 1.30161834\n",
      "Trained batch 277 batch loss 1.16694188 epoch total loss 1.3011322\n",
      "Trained batch 278 batch loss 1.29173172 epoch total loss 1.30109823\n",
      "Trained batch 279 batch loss 1.32687736 epoch total loss 1.30119061\n",
      "Trained batch 280 batch loss 1.41712236 epoch total loss 1.30160463\n",
      "Trained batch 281 batch loss 1.38961947 epoch total loss 1.30191791\n",
      "Trained batch 282 batch loss 1.39595532 epoch total loss 1.30225134\n",
      "Trained batch 283 batch loss 1.33773887 epoch total loss 1.30237675\n",
      "Trained batch 284 batch loss 1.2831552 epoch total loss 1.30230904\n",
      "Trained batch 285 batch loss 1.41288292 epoch total loss 1.30269706\n",
      "Trained batch 286 batch loss 1.21843672 epoch total loss 1.30240238\n",
      "Trained batch 287 batch loss 1.29870594 epoch total loss 1.3023895\n",
      "Trained batch 288 batch loss 1.23602819 epoch total loss 1.30215907\n",
      "Trained batch 289 batch loss 1.23128843 epoch total loss 1.30191386\n",
      "Trained batch 290 batch loss 1.289747 epoch total loss 1.3018719\n",
      "Trained batch 291 batch loss 1.39474058 epoch total loss 1.30219102\n",
      "Trained batch 292 batch loss 1.36471 epoch total loss 1.30240512\n",
      "Trained batch 293 batch loss 1.33842921 epoch total loss 1.30252814\n",
      "Trained batch 294 batch loss 1.44331121 epoch total loss 1.30300701\n",
      "Trained batch 295 batch loss 1.40221143 epoch total loss 1.3033433\n",
      "Trained batch 296 batch loss 1.29504716 epoch total loss 1.30331528\n",
      "Trained batch 297 batch loss 1.37081349 epoch total loss 1.30354249\n",
      "Trained batch 298 batch loss 1.30813742 epoch total loss 1.30355799\n",
      "Trained batch 299 batch loss 1.31858468 epoch total loss 1.30360818\n",
      "Trained batch 300 batch loss 1.24922895 epoch total loss 1.30342686\n",
      "Trained batch 301 batch loss 1.31811 epoch total loss 1.30347574\n",
      "Trained batch 302 batch loss 1.20070338 epoch total loss 1.3031354\n",
      "Trained batch 303 batch loss 1.1000371 epoch total loss 1.3024652\n",
      "Trained batch 304 batch loss 1.37249959 epoch total loss 1.30269551\n",
      "Trained batch 305 batch loss 1.2753278 epoch total loss 1.30260575\n",
      "Trained batch 306 batch loss 1.3658551 epoch total loss 1.30281246\n",
      "Trained batch 307 batch loss 1.38914096 epoch total loss 1.30309367\n",
      "Trained batch 308 batch loss 1.32142234 epoch total loss 1.30315316\n",
      "Trained batch 309 batch loss 1.39770663 epoch total loss 1.30345905\n",
      "Trained batch 310 batch loss 1.35842264 epoch total loss 1.30363643\n",
      "Trained batch 311 batch loss 1.25310814 epoch total loss 1.30347395\n",
      "Trained batch 312 batch loss 1.23913682 epoch total loss 1.30326772\n",
      "Trained batch 313 batch loss 1.35971093 epoch total loss 1.30344808\n",
      "Trained batch 314 batch loss 1.50819635 epoch total loss 1.30410016\n",
      "Trained batch 315 batch loss 1.38331711 epoch total loss 1.30435169\n",
      "Trained batch 316 batch loss 1.28114247 epoch total loss 1.30427825\n",
      "Trained batch 317 batch loss 1.28834891 epoch total loss 1.30422795\n",
      "Trained batch 318 batch loss 1.38634574 epoch total loss 1.30448627\n",
      "Trained batch 319 batch loss 1.26102 epoch total loss 1.30435\n",
      "Trained batch 320 batch loss 1.35227323 epoch total loss 1.30449975\n",
      "Trained batch 321 batch loss 1.40762007 epoch total loss 1.30482101\n",
      "Trained batch 322 batch loss 1.45425618 epoch total loss 1.3052851\n",
      "Trained batch 323 batch loss 1.25992036 epoch total loss 1.30514455\n",
      "Trained batch 324 batch loss 1.21503818 epoch total loss 1.30486643\n",
      "Trained batch 325 batch loss 1.20528841 epoch total loss 1.30456007\n",
      "Trained batch 326 batch loss 1.25330281 epoch total loss 1.30440283\n",
      "Trained batch 327 batch loss 1.29034185 epoch total loss 1.30435979\n",
      "Trained batch 328 batch loss 1.25714803 epoch total loss 1.30421591\n",
      "Trained batch 329 batch loss 1.34996212 epoch total loss 1.30435491\n",
      "Trained batch 330 batch loss 1.42309904 epoch total loss 1.3047148\n",
      "Trained batch 331 batch loss 1.48566067 epoch total loss 1.30526149\n",
      "Trained batch 332 batch loss 1.33746743 epoch total loss 1.30535841\n",
      "Trained batch 333 batch loss 1.35228944 epoch total loss 1.30549943\n",
      "Trained batch 334 batch loss 1.2946229 epoch total loss 1.30546677\n",
      "Trained batch 335 batch loss 1.26624322 epoch total loss 1.30534971\n",
      "Trained batch 336 batch loss 1.31777 epoch total loss 1.30538666\n",
      "Trained batch 337 batch loss 1.32313919 epoch total loss 1.30543935\n",
      "Trained batch 338 batch loss 1.40991163 epoch total loss 1.30574846\n",
      "Trained batch 339 batch loss 1.40637064 epoch total loss 1.30604529\n",
      "Trained batch 340 batch loss 1.4203912 epoch total loss 1.30638158\n",
      "Trained batch 341 batch loss 1.44120955 epoch total loss 1.306777\n",
      "Trained batch 342 batch loss 1.49758029 epoch total loss 1.3073349\n",
      "Trained batch 343 batch loss 1.43204117 epoch total loss 1.30769849\n",
      "Trained batch 344 batch loss 1.42691779 epoch total loss 1.30804503\n",
      "Trained batch 345 batch loss 1.36065459 epoch total loss 1.3081975\n",
      "Trained batch 346 batch loss 1.43544483 epoch total loss 1.30856538\n",
      "Trained batch 347 batch loss 1.40899158 epoch total loss 1.30885482\n",
      "Trained batch 348 batch loss 1.38765144 epoch total loss 1.3090812\n",
      "Trained batch 349 batch loss 1.38748026 epoch total loss 1.30930591\n",
      "Trained batch 350 batch loss 1.25173151 epoch total loss 1.3091414\n",
      "Trained batch 351 batch loss 1.17504478 epoch total loss 1.30875933\n",
      "Trained batch 352 batch loss 1.22478557 epoch total loss 1.30852079\n",
      "Trained batch 353 batch loss 1.27569139 epoch total loss 1.30842781\n",
      "Trained batch 354 batch loss 1.19636774 epoch total loss 1.30811131\n",
      "Trained batch 355 batch loss 1.30490625 epoch total loss 1.30810225\n",
      "Trained batch 356 batch loss 1.22541642 epoch total loss 1.30787\n",
      "Trained batch 357 batch loss 1.40387332 epoch total loss 1.30813885\n",
      "Trained batch 358 batch loss 1.30894351 epoch total loss 1.30814111\n",
      "Trained batch 359 batch loss 1.37399817 epoch total loss 1.30832458\n",
      "Trained batch 360 batch loss 1.44715607 epoch total loss 1.3087101\n",
      "Trained batch 361 batch loss 1.50444603 epoch total loss 1.30925238\n",
      "Trained batch 362 batch loss 1.56371689 epoch total loss 1.30995536\n",
      "Trained batch 363 batch loss 1.40000093 epoch total loss 1.31020331\n",
      "Trained batch 364 batch loss 1.2293545 epoch total loss 1.30998123\n",
      "Trained batch 365 batch loss 1.32835782 epoch total loss 1.31003153\n",
      "Trained batch 366 batch loss 1.37069035 epoch total loss 1.31019735\n",
      "Trained batch 367 batch loss 1.56974185 epoch total loss 1.3109045\n",
      "Trained batch 368 batch loss 1.41801882 epoch total loss 1.31119561\n",
      "Trained batch 369 batch loss 1.37713718 epoch total loss 1.31137431\n",
      "Trained batch 370 batch loss 1.29963577 epoch total loss 1.3113426\n",
      "Trained batch 371 batch loss 1.31901813 epoch total loss 1.31136334\n",
      "Trained batch 372 batch loss 1.31307578 epoch total loss 1.31136787\n",
      "Trained batch 373 batch loss 1.34944355 epoch total loss 1.31147\n",
      "Trained batch 374 batch loss 1.20850265 epoch total loss 1.31119466\n",
      "Trained batch 375 batch loss 1.09550261 epoch total loss 1.31061947\n",
      "Trained batch 376 batch loss 1.2112658 epoch total loss 1.31035531\n",
      "Trained batch 377 batch loss 1.38688672 epoch total loss 1.31055832\n",
      "Trained batch 378 batch loss 1.41769969 epoch total loss 1.31084168\n",
      "Trained batch 379 batch loss 1.38204253 epoch total loss 1.31102955\n",
      "Trained batch 380 batch loss 1.44281554 epoch total loss 1.31137633\n",
      "Trained batch 381 batch loss 1.33322453 epoch total loss 1.31143367\n",
      "Trained batch 382 batch loss 1.33334374 epoch total loss 1.31149113\n",
      "Trained batch 383 batch loss 1.37851954 epoch total loss 1.31166601\n",
      "Trained batch 384 batch loss 1.3012979 epoch total loss 1.31163907\n",
      "Trained batch 385 batch loss 1.43678272 epoch total loss 1.31196404\n",
      "Trained batch 386 batch loss 1.25241435 epoch total loss 1.31180978\n",
      "Trained batch 387 batch loss 1.42056012 epoch total loss 1.31209075\n",
      "Trained batch 388 batch loss 1.40562582 epoch total loss 1.31233191\n",
      "Trained batch 389 batch loss 1.35380852 epoch total loss 1.31243861\n",
      "Trained batch 390 batch loss 1.3048867 epoch total loss 1.3124193\n",
      "Trained batch 391 batch loss 1.27780008 epoch total loss 1.3123306\n",
      "Trained batch 392 batch loss 1.21612358 epoch total loss 1.31208515\n",
      "Trained batch 393 batch loss 1.26297712 epoch total loss 1.31196034\n",
      "Trained batch 394 batch loss 1.19634223 epoch total loss 1.31166685\n",
      "Trained batch 395 batch loss 1.21004319 epoch total loss 1.31140959\n",
      "Trained batch 396 batch loss 1.10592544 epoch total loss 1.31089056\n",
      "Trained batch 397 batch loss 1.14860415 epoch total loss 1.31048179\n",
      "Trained batch 398 batch loss 1.20648587 epoch total loss 1.31022048\n",
      "Trained batch 399 batch loss 1.21306288 epoch total loss 1.30997705\n",
      "Trained batch 400 batch loss 1.21492195 epoch total loss 1.30973935\n",
      "Trained batch 401 batch loss 1.21408892 epoch total loss 1.30950093\n",
      "Trained batch 402 batch loss 1.15613329 epoch total loss 1.30911934\n",
      "Trained batch 403 batch loss 1.21240342 epoch total loss 1.30887938\n",
      "Trained batch 404 batch loss 1.37425232 epoch total loss 1.30904126\n",
      "Trained batch 405 batch loss 1.55964422 epoch total loss 1.30966\n",
      "Trained batch 406 batch loss 1.28600812 epoch total loss 1.30960178\n",
      "Trained batch 407 batch loss 1.33413327 epoch total loss 1.30966198\n",
      "Trained batch 408 batch loss 1.46672785 epoch total loss 1.31004691\n",
      "Trained batch 409 batch loss 1.47541118 epoch total loss 1.31045127\n",
      "Trained batch 410 batch loss 1.33687127 epoch total loss 1.31051564\n",
      "Trained batch 411 batch loss 1.25310564 epoch total loss 1.31037593\n",
      "Trained batch 412 batch loss 1.28376436 epoch total loss 1.31031132\n",
      "Trained batch 413 batch loss 1.39078879 epoch total loss 1.31050622\n",
      "Trained batch 414 batch loss 1.38525414 epoch total loss 1.31068683\n",
      "Trained batch 415 batch loss 1.42847502 epoch total loss 1.31097054\n",
      "Trained batch 416 batch loss 1.42122483 epoch total loss 1.31123555\n",
      "Trained batch 417 batch loss 1.4500705 epoch total loss 1.3115685\n",
      "Trained batch 418 batch loss 1.41441953 epoch total loss 1.31181455\n",
      "Trained batch 419 batch loss 1.33471644 epoch total loss 1.31186926\n",
      "Trained batch 420 batch loss 1.33692622 epoch total loss 1.31192887\n",
      "Trained batch 421 batch loss 1.18002403 epoch total loss 1.31161559\n",
      "Trained batch 422 batch loss 1.12486434 epoch total loss 1.31117308\n",
      "Trained batch 423 batch loss 1.12795711 epoch total loss 1.31074\n",
      "Trained batch 424 batch loss 1.22068071 epoch total loss 1.31052756\n",
      "Trained batch 425 batch loss 1.15520167 epoch total loss 1.31016219\n",
      "Trained batch 426 batch loss 0.990781546 epoch total loss 1.30941248\n",
      "Trained batch 427 batch loss 1.08242512 epoch total loss 1.30888081\n",
      "Trained batch 428 batch loss 0.980709553 epoch total loss 1.30811405\n",
      "Trained batch 429 batch loss 1.21264124 epoch total loss 1.30789149\n",
      "Trained batch 430 batch loss 1.37381184 epoch total loss 1.30804491\n",
      "Trained batch 431 batch loss 1.31633306 epoch total loss 1.3080641\n",
      "Trained batch 432 batch loss 1.20203805 epoch total loss 1.30781865\n",
      "Trained batch 433 batch loss 1.28558552 epoch total loss 1.30776727\n",
      "Trained batch 434 batch loss 1.11688733 epoch total loss 1.30732751\n",
      "Trained batch 435 batch loss 1.13526356 epoch total loss 1.30693185\n",
      "Trained batch 436 batch loss 1.02672541 epoch total loss 1.3062892\n",
      "Trained batch 437 batch loss 1.31235945 epoch total loss 1.30630314\n",
      "Trained batch 438 batch loss 1.21651924 epoch total loss 1.3060981\n",
      "Trained batch 439 batch loss 1.28903353 epoch total loss 1.30605936\n",
      "Trained batch 440 batch loss 1.14992607 epoch total loss 1.30570447\n",
      "Trained batch 441 batch loss 1.23726249 epoch total loss 1.30554914\n",
      "Trained batch 442 batch loss 1.2987957 epoch total loss 1.30553389\n",
      "Trained batch 443 batch loss 1.18106532 epoch total loss 1.30525291\n",
      "Trained batch 444 batch loss 1.30282533 epoch total loss 1.30524743\n",
      "Trained batch 445 batch loss 1.2950207 epoch total loss 1.30522442\n",
      "Trained batch 446 batch loss 1.11055684 epoch total loss 1.30478799\n",
      "Trained batch 447 batch loss 1.20738482 epoch total loss 1.30457008\n",
      "Trained batch 448 batch loss 1.18424165 epoch total loss 1.3043015\n",
      "Trained batch 449 batch loss 1.30485022 epoch total loss 1.30430281\n",
      "Trained batch 450 batch loss 1.39093971 epoch total loss 1.30449533\n",
      "Trained batch 451 batch loss 1.59681129 epoch total loss 1.30514348\n",
      "Trained batch 452 batch loss 1.50181365 epoch total loss 1.30557859\n",
      "Trained batch 453 batch loss 1.24125087 epoch total loss 1.30543661\n",
      "Trained batch 454 batch loss 1.21647942 epoch total loss 1.30524075\n",
      "Trained batch 455 batch loss 1.34117651 epoch total loss 1.30531967\n",
      "Trained batch 456 batch loss 1.37013984 epoch total loss 1.30546176\n",
      "Trained batch 457 batch loss 1.33978987 epoch total loss 1.30553687\n",
      "Trained batch 458 batch loss 1.26846409 epoch total loss 1.30545604\n",
      "Trained batch 459 batch loss 1.25266743 epoch total loss 1.30534112\n",
      "Trained batch 460 batch loss 1.37618208 epoch total loss 1.30549502\n",
      "Trained batch 461 batch loss 1.32018733 epoch total loss 1.30552685\n",
      "Trained batch 462 batch loss 1.30674541 epoch total loss 1.30552959\n",
      "Trained batch 463 batch loss 1.35652876 epoch total loss 1.30563962\n",
      "Trained batch 464 batch loss 1.38625443 epoch total loss 1.30581331\n",
      "Trained batch 465 batch loss 1.42437387 epoch total loss 1.3060683\n",
      "Trained batch 466 batch loss 1.41734457 epoch total loss 1.3063072\n",
      "Trained batch 467 batch loss 1.30537558 epoch total loss 1.30630517\n",
      "Trained batch 468 batch loss 1.4143157 epoch total loss 1.30653596\n",
      "Trained batch 469 batch loss 1.30098152 epoch total loss 1.30652404\n",
      "Trained batch 470 batch loss 1.30478382 epoch total loss 1.30652034\n",
      "Trained batch 471 batch loss 1.28055811 epoch total loss 1.30646527\n",
      "Trained batch 472 batch loss 1.3141005 epoch total loss 1.30648148\n",
      "Trained batch 473 batch loss 1.26620269 epoch total loss 1.30639625\n",
      "Trained batch 474 batch loss 1.30659497 epoch total loss 1.3063966\n",
      "Trained batch 475 batch loss 1.31377363 epoch total loss 1.30641222\n",
      "Trained batch 476 batch loss 1.2938453 epoch total loss 1.30638576\n",
      "Trained batch 477 batch loss 1.31793702 epoch total loss 1.30641\n",
      "Trained batch 478 batch loss 1.30522823 epoch total loss 1.30640745\n",
      "Trained batch 479 batch loss 1.36534214 epoch total loss 1.30653048\n",
      "Trained batch 480 batch loss 1.44564509 epoch total loss 1.30682027\n",
      "Trained batch 481 batch loss 1.3215332 epoch total loss 1.30685091\n",
      "Trained batch 482 batch loss 1.52757537 epoch total loss 1.30730879\n",
      "Trained batch 483 batch loss 1.45034707 epoch total loss 1.30760491\n",
      "Trained batch 484 batch loss 1.36404669 epoch total loss 1.30772161\n",
      "Trained batch 485 batch loss 1.26513553 epoch total loss 1.30763376\n",
      "Trained batch 486 batch loss 1.17212033 epoch total loss 1.30735493\n",
      "Trained batch 487 batch loss 1.05282724 epoch total loss 1.30683243\n",
      "Trained batch 488 batch loss 1.20055807 epoch total loss 1.30661464\n",
      "Trained batch 489 batch loss 1.25718403 epoch total loss 1.30651355\n",
      "Trained batch 490 batch loss 1.07667077 epoch total loss 1.30604446\n",
      "Trained batch 491 batch loss 1.04996169 epoch total loss 1.30552292\n",
      "Trained batch 492 batch loss 1.1111114 epoch total loss 1.30512774\n",
      "Trained batch 493 batch loss 1.07985961 epoch total loss 1.30467081\n",
      "Trained batch 494 batch loss 1.18400598 epoch total loss 1.30442655\n",
      "Trained batch 495 batch loss 1.27065802 epoch total loss 1.30435824\n",
      "Trained batch 496 batch loss 1.38283062 epoch total loss 1.30451643\n",
      "Trained batch 497 batch loss 1.32739556 epoch total loss 1.30456245\n",
      "Trained batch 498 batch loss 1.47034359 epoch total loss 1.3048954\n",
      "Trained batch 499 batch loss 1.37648165 epoch total loss 1.30503881\n",
      "Trained batch 500 batch loss 1.35754514 epoch total loss 1.30514383\n",
      "Trained batch 501 batch loss 1.31854212 epoch total loss 1.30517054\n",
      "Trained batch 502 batch loss 1.37764359 epoch total loss 1.3053149\n",
      "Trained batch 503 batch loss 1.28695393 epoch total loss 1.3052783\n",
      "Trained batch 504 batch loss 1.15144789 epoch total loss 1.30497301\n",
      "Trained batch 505 batch loss 1.18765831 epoch total loss 1.30474079\n",
      "Trained batch 506 batch loss 1.31901491 epoch total loss 1.30476904\n",
      "Trained batch 507 batch loss 1.31432843 epoch total loss 1.30478787\n",
      "Trained batch 508 batch loss 1.30461693 epoch total loss 1.30478764\n",
      "Trained batch 509 batch loss 1.24298882 epoch total loss 1.30466616\n",
      "Trained batch 510 batch loss 1.2673285 epoch total loss 1.30459297\n",
      "Trained batch 511 batch loss 1.32753742 epoch total loss 1.30463779\n",
      "Trained batch 512 batch loss 1.18263769 epoch total loss 1.30439949\n",
      "Trained batch 513 batch loss 1.23831344 epoch total loss 1.30427074\n",
      "Trained batch 514 batch loss 1.34258711 epoch total loss 1.30434525\n",
      "Trained batch 515 batch loss 1.21573567 epoch total loss 1.30417323\n",
      "Trained batch 516 batch loss 1.34937894 epoch total loss 1.30426085\n",
      "Trained batch 517 batch loss 1.3066678 epoch total loss 1.3042655\n",
      "Trained batch 518 batch loss 1.15103209 epoch total loss 1.30396974\n",
      "Trained batch 519 batch loss 1.31374562 epoch total loss 1.30398846\n",
      "Trained batch 520 batch loss 1.350986 epoch total loss 1.30407894\n",
      "Trained batch 521 batch loss 1.24441981 epoch total loss 1.3039645\n",
      "Trained batch 522 batch loss 1.24952877 epoch total loss 1.30386019\n",
      "Trained batch 523 batch loss 1.23032856 epoch total loss 1.30371952\n",
      "Trained batch 524 batch loss 1.2904098 epoch total loss 1.30369413\n",
      "Trained batch 525 batch loss 1.34265208 epoch total loss 1.3037684\n",
      "Trained batch 526 batch loss 1.4115634 epoch total loss 1.30397332\n",
      "Trained batch 527 batch loss 1.38224316 epoch total loss 1.30412185\n",
      "Trained batch 528 batch loss 1.31698537 epoch total loss 1.30414617\n",
      "Trained batch 529 batch loss 1.24628115 epoch total loss 1.30403674\n",
      "Trained batch 530 batch loss 1.25785446 epoch total loss 1.30394971\n",
      "Trained batch 531 batch loss 1.31192088 epoch total loss 1.30396473\n",
      "Trained batch 532 batch loss 1.31990552 epoch total loss 1.30399466\n",
      "Trained batch 533 batch loss 1.46594286 epoch total loss 1.30429852\n",
      "Trained batch 534 batch loss 1.60601616 epoch total loss 1.30486357\n",
      "Trained batch 535 batch loss 1.31584454 epoch total loss 1.30488408\n",
      "Trained batch 536 batch loss 1.483266 epoch total loss 1.30521691\n",
      "Trained batch 537 batch loss 1.46824622 epoch total loss 1.30552053\n",
      "Trained batch 538 batch loss 1.29870427 epoch total loss 1.3055079\n",
      "Trained batch 539 batch loss 1.42025173 epoch total loss 1.30572069\n",
      "Trained batch 540 batch loss 1.48908901 epoch total loss 1.30606019\n",
      "Trained batch 541 batch loss 1.27365255 epoch total loss 1.30600035\n",
      "Trained batch 542 batch loss 1.16295886 epoch total loss 1.30573642\n",
      "Trained batch 543 batch loss 1.26859438 epoch total loss 1.30566812\n",
      "Trained batch 544 batch loss 1.41271448 epoch total loss 1.30586493\n",
      "Trained batch 545 batch loss 1.31701243 epoch total loss 1.30588531\n",
      "Trained batch 546 batch loss 1.26532495 epoch total loss 1.30581105\n",
      "Trained batch 547 batch loss 1.22777975 epoch total loss 1.30566847\n",
      "Trained batch 548 batch loss 1.3380729 epoch total loss 1.3057276\n",
      "Trained batch 549 batch loss 1.29938555 epoch total loss 1.30571604\n",
      "Trained batch 550 batch loss 1.29637289 epoch total loss 1.30569899\n",
      "Trained batch 551 batch loss 1.32890368 epoch total loss 1.30574119\n",
      "Trained batch 552 batch loss 1.29501164 epoch total loss 1.30572164\n",
      "Trained batch 553 batch loss 1.43834984 epoch total loss 1.30596149\n",
      "Trained batch 554 batch loss 1.42408776 epoch total loss 1.30617476\n",
      "Trained batch 555 batch loss 1.33252835 epoch total loss 1.3062222\n",
      "Trained batch 556 batch loss 1.3853029 epoch total loss 1.30636442\n",
      "Trained batch 557 batch loss 1.35605621 epoch total loss 1.3064537\n",
      "Trained batch 558 batch loss 1.29591072 epoch total loss 1.30643475\n",
      "Trained batch 559 batch loss 1.27759433 epoch total loss 1.30638313\n",
      "Trained batch 560 batch loss 1.26116 epoch total loss 1.30630243\n",
      "Trained batch 561 batch loss 1.1552254 epoch total loss 1.30603313\n",
      "Trained batch 562 batch loss 1.13237143 epoch total loss 1.30572414\n",
      "Trained batch 563 batch loss 1.34935546 epoch total loss 1.30580163\n",
      "Trained batch 564 batch loss 1.26215172 epoch total loss 1.30572426\n",
      "Trained batch 565 batch loss 1.30941057 epoch total loss 1.3057307\n",
      "Trained batch 566 batch loss 1.2591964 epoch total loss 1.30564857\n",
      "Trained batch 567 batch loss 1.1884023 epoch total loss 1.30544174\n",
      "Trained batch 568 batch loss 1.10634756 epoch total loss 1.30509126\n",
      "Trained batch 569 batch loss 1.20718479 epoch total loss 1.30491924\n",
      "Trained batch 570 batch loss 1.25448728 epoch total loss 1.30483079\n",
      "Trained batch 571 batch loss 1.32594573 epoch total loss 1.30486774\n",
      "Trained batch 572 batch loss 1.26639652 epoch total loss 1.30480051\n",
      "Trained batch 573 batch loss 1.28192484 epoch total loss 1.30476058\n",
      "Trained batch 574 batch loss 1.18316507 epoch total loss 1.30454874\n",
      "Trained batch 575 batch loss 1.22940695 epoch total loss 1.30441809\n",
      "Trained batch 576 batch loss 1.36373937 epoch total loss 1.30452108\n",
      "Trained batch 577 batch loss 1.33905339 epoch total loss 1.30458093\n",
      "Trained batch 578 batch loss 1.28191888 epoch total loss 1.30454183\n",
      "Trained batch 579 batch loss 1.28375816 epoch total loss 1.30450583\n",
      "Trained batch 580 batch loss 1.27472734 epoch total loss 1.30445457\n",
      "Trained batch 581 batch loss 1.18279481 epoch total loss 1.30424511\n",
      "Trained batch 582 batch loss 1.18565154 epoch total loss 1.30404139\n",
      "Trained batch 583 batch loss 1.23987567 epoch total loss 1.30393136\n",
      "Trained batch 584 batch loss 1.25435007 epoch total loss 1.30384636\n",
      "Trained batch 585 batch loss 1.22960544 epoch total loss 1.30371952\n",
      "Trained batch 586 batch loss 1.22672176 epoch total loss 1.30358815\n",
      "Trained batch 587 batch loss 1.28048539 epoch total loss 1.30354869\n",
      "Trained batch 588 batch loss 1.27937543 epoch total loss 1.30350757\n",
      "Trained batch 589 batch loss 1.24687982 epoch total loss 1.30341148\n",
      "Trained batch 590 batch loss 1.43924153 epoch total loss 1.30364168\n",
      "Trained batch 591 batch loss 1.30615139 epoch total loss 1.30364597\n",
      "Trained batch 592 batch loss 1.26281011 epoch total loss 1.30357707\n",
      "Trained batch 593 batch loss 1.3703984 epoch total loss 1.30368972\n",
      "Trained batch 594 batch loss 1.1563971 epoch total loss 1.30344176\n",
      "Trained batch 595 batch loss 1.2965095 epoch total loss 1.30343008\n",
      "Trained batch 596 batch loss 1.14824152 epoch total loss 1.30316973\n",
      "Trained batch 597 batch loss 1.19832051 epoch total loss 1.30299401\n",
      "Trained batch 598 batch loss 1.30652034 epoch total loss 1.303\n",
      "Trained batch 599 batch loss 1.4417429 epoch total loss 1.3032316\n",
      "Trained batch 600 batch loss 1.51312947 epoch total loss 1.30358148\n",
      "Trained batch 601 batch loss 1.40115333 epoch total loss 1.30374372\n",
      "Trained batch 602 batch loss 1.45044482 epoch total loss 1.30398738\n",
      "Trained batch 603 batch loss 1.27528274 epoch total loss 1.30393982\n",
      "Trained batch 604 batch loss 1.20669866 epoch total loss 1.30377889\n",
      "Trained batch 605 batch loss 1.0973835 epoch total loss 1.30343771\n",
      "Trained batch 606 batch loss 1.04922795 epoch total loss 1.30301833\n",
      "Trained batch 607 batch loss 1.03710938 epoch total loss 1.30258024\n",
      "Trained batch 608 batch loss 1.26542556 epoch total loss 1.3025192\n",
      "Trained batch 609 batch loss 1.20980358 epoch total loss 1.30236685\n",
      "Trained batch 610 batch loss 1.35334551 epoch total loss 1.30245042\n",
      "Trained batch 611 batch loss 1.27094471 epoch total loss 1.3023988\n",
      "Trained batch 612 batch loss 1.17446434 epoch total loss 1.30218971\n",
      "Trained batch 613 batch loss 1.1181612 epoch total loss 1.30188954\n",
      "Trained batch 614 batch loss 1.36210656 epoch total loss 1.30198765\n",
      "Trained batch 615 batch loss 1.49081683 epoch total loss 1.30229473\n",
      "Trained batch 616 batch loss 1.30422783 epoch total loss 1.30229783\n",
      "Trained batch 617 batch loss 1.14462 epoch total loss 1.30204225\n",
      "Trained batch 618 batch loss 1.11690974 epoch total loss 1.30174255\n",
      "Trained batch 619 batch loss 1.21812093 epoch total loss 1.30160749\n",
      "Trained batch 620 batch loss 1.06947398 epoch total loss 1.30123305\n",
      "Trained batch 621 batch loss 1.12888503 epoch total loss 1.30095565\n",
      "Trained batch 622 batch loss 1.14042437 epoch total loss 1.30069757\n",
      "Trained batch 623 batch loss 1.15264022 epoch total loss 1.30045986\n",
      "Trained batch 624 batch loss 1.18291247 epoch total loss 1.30027151\n",
      "Trained batch 625 batch loss 1.21823287 epoch total loss 1.30014038\n",
      "Trained batch 626 batch loss 1.30790269 epoch total loss 1.30015278\n",
      "Trained batch 627 batch loss 1.29882681 epoch total loss 1.30015063\n",
      "Trained batch 628 batch loss 1.31579876 epoch total loss 1.30017555\n",
      "Trained batch 629 batch loss 1.34860432 epoch total loss 1.30025256\n",
      "Trained batch 630 batch loss 1.28927958 epoch total loss 1.30023527\n",
      "Trained batch 631 batch loss 1.39574683 epoch total loss 1.30038655\n",
      "Trained batch 632 batch loss 1.24334788 epoch total loss 1.30029631\n",
      "Trained batch 633 batch loss 1.28461444 epoch total loss 1.30027151\n",
      "Trained batch 634 batch loss 1.19099009 epoch total loss 1.30009913\n",
      "Trained batch 635 batch loss 1.29242969 epoch total loss 1.30008709\n",
      "Trained batch 636 batch loss 1.45151973 epoch total loss 1.30032516\n",
      "Trained batch 637 batch loss 1.31831 epoch total loss 1.30035341\n",
      "Trained batch 638 batch loss 1.22424781 epoch total loss 1.30023408\n",
      "Trained batch 639 batch loss 1.25650394 epoch total loss 1.30016577\n",
      "Trained batch 640 batch loss 1.29644752 epoch total loss 1.30015993\n",
      "Trained batch 641 batch loss 1.29726124 epoch total loss 1.3001554\n",
      "Trained batch 642 batch loss 1.22887433 epoch total loss 1.30004442\n",
      "Trained batch 643 batch loss 1.30840755 epoch total loss 1.30005741\n",
      "Trained batch 644 batch loss 1.33827865 epoch total loss 1.30011666\n",
      "Trained batch 645 batch loss 1.33196652 epoch total loss 1.30016613\n",
      "Trained batch 646 batch loss 1.29960966 epoch total loss 1.30016518\n",
      "Trained batch 647 batch loss 1.31367028 epoch total loss 1.30018604\n",
      "Trained batch 648 batch loss 1.32750237 epoch total loss 1.30022824\n",
      "Trained batch 649 batch loss 1.25805736 epoch total loss 1.30016327\n",
      "Trained batch 650 batch loss 1.32686234 epoch total loss 1.30020428\n",
      "Trained batch 651 batch loss 1.32544446 epoch total loss 1.30024314\n",
      "Trained batch 652 batch loss 1.30145967 epoch total loss 1.30024493\n",
      "Trained batch 653 batch loss 1.2912668 epoch total loss 1.30023122\n",
      "Trained batch 654 batch loss 1.21553481 epoch total loss 1.30010164\n",
      "Trained batch 655 batch loss 1.15130448 epoch total loss 1.29987442\n",
      "Trained batch 656 batch loss 1.08730173 epoch total loss 1.29955041\n",
      "Trained batch 657 batch loss 1.19122 epoch total loss 1.29938555\n",
      "Trained batch 658 batch loss 1.31202674 epoch total loss 1.29940474\n",
      "Trained batch 659 batch loss 1.42066467 epoch total loss 1.29958868\n",
      "Trained batch 660 batch loss 1.53456724 epoch total loss 1.29994464\n",
      "Trained batch 661 batch loss 1.41639781 epoch total loss 1.30012083\n",
      "Trained batch 662 batch loss 1.40220153 epoch total loss 1.30027509\n",
      "Trained batch 663 batch loss 1.35528016 epoch total loss 1.30035806\n",
      "Trained batch 664 batch loss 1.32724667 epoch total loss 1.30039859\n",
      "Trained batch 665 batch loss 1.15418196 epoch total loss 1.30017865\n",
      "Trained batch 666 batch loss 1.18951356 epoch total loss 1.30001259\n",
      "Trained batch 667 batch loss 1.29834473 epoch total loss 1.30001009\n",
      "Trained batch 668 batch loss 1.27550578 epoch total loss 1.29997337\n",
      "Trained batch 669 batch loss 1.32543564 epoch total loss 1.3000114\n",
      "Trained batch 670 batch loss 1.34036756 epoch total loss 1.30007172\n",
      "Trained batch 671 batch loss 1.2887044 epoch total loss 1.30005479\n",
      "Trained batch 672 batch loss 1.2875843 epoch total loss 1.30003619\n",
      "Trained batch 673 batch loss 1.33377874 epoch total loss 1.30008638\n",
      "Trained batch 674 batch loss 1.22734284 epoch total loss 1.29997849\n",
      "Trained batch 675 batch loss 1.2357831 epoch total loss 1.29988337\n",
      "Trained batch 676 batch loss 1.370597 epoch total loss 1.29998803\n",
      "Trained batch 677 batch loss 1.15184808 epoch total loss 1.29976916\n",
      "Trained batch 678 batch loss 1.14993501 epoch total loss 1.29954827\n",
      "Trained batch 679 batch loss 1.12612522 epoch total loss 1.2992928\n",
      "Trained batch 680 batch loss 1.14888144 epoch total loss 1.29907155\n",
      "Trained batch 681 batch loss 1.14905024 epoch total loss 1.29885125\n",
      "Trained batch 682 batch loss 1.19858027 epoch total loss 1.29870427\n",
      "Trained batch 683 batch loss 1.18762851 epoch total loss 1.29854167\n",
      "Trained batch 684 batch loss 1.18223035 epoch total loss 1.29837155\n",
      "Trained batch 685 batch loss 1.20876622 epoch total loss 1.29824078\n",
      "Trained batch 686 batch loss 1.39115465 epoch total loss 1.2983762\n",
      "Trained batch 687 batch loss 1.25388384 epoch total loss 1.29831147\n",
      "Trained batch 688 batch loss 1.16946054 epoch total loss 1.29812419\n",
      "Trained batch 689 batch loss 1.3273977 epoch total loss 1.29816663\n",
      "Trained batch 690 batch loss 1.32683372 epoch total loss 1.29820824\n",
      "Trained batch 691 batch loss 1.25106 epoch total loss 1.29813993\n",
      "Trained batch 692 batch loss 1.26095545 epoch total loss 1.29808617\n",
      "Trained batch 693 batch loss 1.26830232 epoch total loss 1.29804325\n",
      "Trained batch 694 batch loss 1.36216974 epoch total loss 1.29813564\n",
      "Trained batch 695 batch loss 1.32194066 epoch total loss 1.29817\n",
      "Trained batch 696 batch loss 1.28579211 epoch total loss 1.29815209\n",
      "Trained batch 697 batch loss 1.35785818 epoch total loss 1.29823768\n",
      "Trained batch 698 batch loss 1.23793542 epoch total loss 1.29815137\n",
      "Trained batch 699 batch loss 1.24679065 epoch total loss 1.29807782\n",
      "Trained batch 700 batch loss 1.37139881 epoch total loss 1.29818261\n",
      "Trained batch 701 batch loss 1.23226798 epoch total loss 1.29808843\n",
      "Trained batch 702 batch loss 1.45189071 epoch total loss 1.29830754\n",
      "Trained batch 703 batch loss 1.26573014 epoch total loss 1.29826128\n",
      "Trained batch 704 batch loss 1.05888867 epoch total loss 1.2979213\n",
      "Trained batch 705 batch loss 1.0868634 epoch total loss 1.29762185\n",
      "Trained batch 706 batch loss 1.25873411 epoch total loss 1.29756677\n",
      "Trained batch 707 batch loss 1.14147794 epoch total loss 1.297346\n",
      "Trained batch 708 batch loss 1.20585382 epoch total loss 1.29721677\n",
      "Trained batch 709 batch loss 1.34853351 epoch total loss 1.29728913\n",
      "Trained batch 710 batch loss 1.28330624 epoch total loss 1.29726946\n",
      "Trained batch 711 batch loss 1.2852447 epoch total loss 1.29725254\n",
      "Trained batch 712 batch loss 1.26880395 epoch total loss 1.2972126\n",
      "Trained batch 713 batch loss 1.3603909 epoch total loss 1.29730117\n",
      "Trained batch 714 batch loss 1.52102685 epoch total loss 1.29761457\n",
      "Trained batch 715 batch loss 1.54349327 epoch total loss 1.29795849\n",
      "Trained batch 716 batch loss 1.32377183 epoch total loss 1.29799461\n",
      "Trained batch 717 batch loss 1.30909753 epoch total loss 1.29801011\n",
      "Trained batch 718 batch loss 1.42206323 epoch total loss 1.29818285\n",
      "Trained batch 719 batch loss 1.38758242 epoch total loss 1.29830718\n",
      "Trained batch 720 batch loss 1.26835322 epoch total loss 1.29826558\n",
      "Trained batch 721 batch loss 1.33410096 epoch total loss 1.29831529\n",
      "Trained batch 722 batch loss 1.29774415 epoch total loss 1.29831445\n",
      "Trained batch 723 batch loss 1.32839 epoch total loss 1.29835606\n",
      "Trained batch 724 batch loss 1.32125735 epoch total loss 1.29838765\n",
      "Trained batch 725 batch loss 1.36194766 epoch total loss 1.29847527\n",
      "Trained batch 726 batch loss 1.23826551 epoch total loss 1.29839242\n",
      "Trained batch 727 batch loss 1.39449525 epoch total loss 1.2985245\n",
      "Trained batch 728 batch loss 1.34706092 epoch total loss 1.29859114\n",
      "Trained batch 729 batch loss 1.34555793 epoch total loss 1.29865563\n",
      "Trained batch 730 batch loss 1.31479049 epoch total loss 1.2986778\n",
      "Trained batch 731 batch loss 1.15125155 epoch total loss 1.2984761\n",
      "Trained batch 732 batch loss 1.21813965 epoch total loss 1.29836631\n",
      "Trained batch 733 batch loss 1.21328139 epoch total loss 1.2982502\n",
      "Trained batch 734 batch loss 1.18236685 epoch total loss 1.29809237\n",
      "Trained batch 735 batch loss 1.25558829 epoch total loss 1.29803455\n",
      "Trained batch 736 batch loss 1.33117688 epoch total loss 1.29807961\n",
      "Trained batch 737 batch loss 1.40784729 epoch total loss 1.2982285\n",
      "Trained batch 738 batch loss 1.32697976 epoch total loss 1.29826748\n",
      "Trained batch 739 batch loss 1.26154745 epoch total loss 1.29821777\n",
      "Trained batch 740 batch loss 1.19018507 epoch total loss 1.29807174\n",
      "Trained batch 741 batch loss 1.26487517 epoch total loss 1.29802704\n",
      "Trained batch 742 batch loss 1.28759181 epoch total loss 1.29801297\n",
      "Trained batch 743 batch loss 1.37986171 epoch total loss 1.29812312\n",
      "Trained batch 744 batch loss 1.54925227 epoch total loss 1.29846072\n",
      "Trained batch 745 batch loss 1.52042758 epoch total loss 1.29875863\n",
      "Trained batch 746 batch loss 1.5128715 epoch total loss 1.29904568\n",
      "Trained batch 747 batch loss 1.27562237 epoch total loss 1.29901433\n",
      "Trained batch 748 batch loss 1.42553449 epoch total loss 1.29918349\n",
      "Trained batch 749 batch loss 1.32086158 epoch total loss 1.29921246\n",
      "Trained batch 750 batch loss 1.38597369 epoch total loss 1.29932809\n",
      "Trained batch 751 batch loss 1.38698804 epoch total loss 1.29944479\n",
      "Trained batch 752 batch loss 1.28298426 epoch total loss 1.29942286\n",
      "Trained batch 753 batch loss 1.27261758 epoch total loss 1.29938734\n",
      "Trained batch 754 batch loss 1.29813135 epoch total loss 1.29938567\n",
      "Trained batch 755 batch loss 1.3175782 epoch total loss 1.29940975\n",
      "Trained batch 756 batch loss 1.28442216 epoch total loss 1.29939\n",
      "Trained batch 757 batch loss 1.2373153 epoch total loss 1.29930794\n",
      "Trained batch 758 batch loss 1.21087885 epoch total loss 1.29919124\n",
      "Trained batch 759 batch loss 1.22031403 epoch total loss 1.29908741\n",
      "Trained batch 760 batch loss 1.21499765 epoch total loss 1.29897678\n",
      "Trained batch 761 batch loss 1.23379469 epoch total loss 1.29889107\n",
      "Trained batch 762 batch loss 1.25209737 epoch total loss 1.29882967\n",
      "Trained batch 763 batch loss 1.24018204 epoch total loss 1.29875278\n",
      "Trained batch 764 batch loss 1.29526567 epoch total loss 1.29874825\n",
      "Trained batch 765 batch loss 1.20593548 epoch total loss 1.2986269\n",
      "Trained batch 766 batch loss 1.17530835 epoch total loss 1.29846585\n",
      "Trained batch 767 batch loss 1.2611748 epoch total loss 1.29841733\n",
      "Trained batch 768 batch loss 1.29778814 epoch total loss 1.2984165\n",
      "Trained batch 769 batch loss 1.28531837 epoch total loss 1.29839945\n",
      "Trained batch 770 batch loss 1.25107062 epoch total loss 1.29833806\n",
      "Trained batch 771 batch loss 1.18262601 epoch total loss 1.29818797\n",
      "Trained batch 772 batch loss 1.22940898 epoch total loss 1.29809892\n",
      "Trained batch 773 batch loss 1.35342932 epoch total loss 1.29817045\n",
      "Trained batch 774 batch loss 1.27324486 epoch total loss 1.29813826\n",
      "Trained batch 775 batch loss 1.28649807 epoch total loss 1.29812324\n",
      "Trained batch 776 batch loss 1.36157966 epoch total loss 1.29820502\n",
      "Trained batch 777 batch loss 1.18114543 epoch total loss 1.29805434\n",
      "Trained batch 778 batch loss 1.16621923 epoch total loss 1.29788494\n",
      "Trained batch 779 batch loss 1.22019911 epoch total loss 1.29778516\n",
      "Trained batch 780 batch loss 1.26137555 epoch total loss 1.29773843\n",
      "Trained batch 781 batch loss 1.27238262 epoch total loss 1.29770601\n",
      "Trained batch 782 batch loss 1.17728913 epoch total loss 1.29755211\n",
      "Trained batch 783 batch loss 1.23267603 epoch total loss 1.29746926\n",
      "Trained batch 784 batch loss 1.39799428 epoch total loss 1.29759741\n",
      "Trained batch 785 batch loss 1.25766253 epoch total loss 1.29754663\n",
      "Trained batch 786 batch loss 1.19202483 epoch total loss 1.2974124\n",
      "Trained batch 787 batch loss 1.1725899 epoch total loss 1.29725373\n",
      "Trained batch 788 batch loss 1.19619119 epoch total loss 1.29712546\n",
      "Trained batch 789 batch loss 1.18298721 epoch total loss 1.29698086\n",
      "Trained batch 790 batch loss 1.09078 epoch total loss 1.29671991\n",
      "Trained batch 791 batch loss 1.17772079 epoch total loss 1.29656947\n",
      "Trained batch 792 batch loss 1.1370846 epoch total loss 1.29636812\n",
      "Trained batch 793 batch loss 1.22405076 epoch total loss 1.29627681\n",
      "Trained batch 794 batch loss 1.23883414 epoch total loss 1.29620457\n",
      "Trained batch 795 batch loss 1.18735063 epoch total loss 1.2960676\n",
      "Trained batch 796 batch loss 1.21007156 epoch total loss 1.29595959\n",
      "Trained batch 797 batch loss 1.15829957 epoch total loss 1.29578698\n",
      "Trained batch 798 batch loss 1.35808206 epoch total loss 1.29586494\n",
      "Trained batch 799 batch loss 1.33464098 epoch total loss 1.29591346\n",
      "Trained batch 800 batch loss 1.34303236 epoch total loss 1.29597235\n",
      "Trained batch 801 batch loss 1.34502864 epoch total loss 1.2960335\n",
      "Trained batch 802 batch loss 1.33768797 epoch total loss 1.29608536\n",
      "Trained batch 803 batch loss 1.42097747 epoch total loss 1.29624093\n",
      "Trained batch 804 batch loss 1.39717734 epoch total loss 1.29636657\n",
      "Trained batch 805 batch loss 1.28837514 epoch total loss 1.29635656\n",
      "Trained batch 806 batch loss 1.09598315 epoch total loss 1.29610789\n",
      "Trained batch 807 batch loss 1.26847291 epoch total loss 1.29607356\n",
      "Trained batch 808 batch loss 1.28626299 epoch total loss 1.2960614\n",
      "Trained batch 809 batch loss 1.30439615 epoch total loss 1.29607177\n",
      "Trained batch 810 batch loss 1.38979554 epoch total loss 1.29618752\n",
      "Trained batch 811 batch loss 1.3145473 epoch total loss 1.29621017\n",
      "Trained batch 812 batch loss 1.34693 epoch total loss 1.29627264\n",
      "Trained batch 813 batch loss 1.26392841 epoch total loss 1.29623282\n",
      "Trained batch 814 batch loss 1.32147098 epoch total loss 1.29626369\n",
      "Trained batch 815 batch loss 1.24486446 epoch total loss 1.29620063\n",
      "Trained batch 816 batch loss 1.22905064 epoch total loss 1.29611838\n",
      "Trained batch 817 batch loss 1.21986771 epoch total loss 1.29602504\n",
      "Trained batch 818 batch loss 1.25067413 epoch total loss 1.29596961\n",
      "Trained batch 819 batch loss 1.32470727 epoch total loss 1.29600465\n",
      "Trained batch 820 batch loss 1.27188659 epoch total loss 1.29597521\n",
      "Trained batch 821 batch loss 1.33258426 epoch total loss 1.29601991\n",
      "Trained batch 822 batch loss 1.31548786 epoch total loss 1.29604352\n",
      "Trained batch 823 batch loss 1.34114504 epoch total loss 1.29609835\n",
      "Trained batch 824 batch loss 1.33671761 epoch total loss 1.29614758\n",
      "Trained batch 825 batch loss 1.27398312 epoch total loss 1.29612064\n",
      "Trained batch 826 batch loss 1.37230551 epoch total loss 1.29621291\n",
      "Trained batch 827 batch loss 1.3460741 epoch total loss 1.29627323\n",
      "Trained batch 828 batch loss 1.28600752 epoch total loss 1.29626083\n",
      "Trained batch 829 batch loss 1.39361465 epoch total loss 1.29637814\n",
      "Trained batch 830 batch loss 1.38959968 epoch total loss 1.29649055\n",
      "Trained batch 831 batch loss 1.20730639 epoch total loss 1.29638314\n",
      "Trained batch 832 batch loss 1.37223291 epoch total loss 1.29647434\n",
      "Trained batch 833 batch loss 1.443887 epoch total loss 1.29665124\n",
      "Trained batch 834 batch loss 1.37196279 epoch total loss 1.29674149\n",
      "Trained batch 835 batch loss 1.32800245 epoch total loss 1.29677892\n",
      "Trained batch 836 batch loss 1.25576758 epoch total loss 1.2967298\n",
      "Trained batch 837 batch loss 1.22213101 epoch total loss 1.29664075\n",
      "Trained batch 838 batch loss 1.23724079 epoch total loss 1.29656982\n",
      "Trained batch 839 batch loss 1.34927571 epoch total loss 1.29663265\n",
      "Trained batch 840 batch loss 1.28692126 epoch total loss 1.29662097\n",
      "Trained batch 841 batch loss 1.1981858 epoch total loss 1.29650402\n",
      "Trained batch 842 batch loss 1.28474844 epoch total loss 1.29649007\n",
      "Trained batch 843 batch loss 1.21751249 epoch total loss 1.29639637\n",
      "Trained batch 844 batch loss 1.20715237 epoch total loss 1.29629064\n",
      "Trained batch 845 batch loss 1.23205495 epoch total loss 1.2962147\n",
      "Trained batch 846 batch loss 1.19933271 epoch total loss 1.29610014\n",
      "Trained batch 847 batch loss 1.24785078 epoch total loss 1.29604316\n",
      "Trained batch 848 batch loss 1.25470924 epoch total loss 1.2959944\n",
      "Trained batch 849 batch loss 1.20902395 epoch total loss 1.295892\n",
      "Trained batch 850 batch loss 1.28189945 epoch total loss 1.29587543\n",
      "Trained batch 851 batch loss 1.15992975 epoch total loss 1.29571569\n",
      "Trained batch 852 batch loss 1.31311727 epoch total loss 1.29573607\n",
      "Trained batch 853 batch loss 1.20874572 epoch total loss 1.29563415\n",
      "Trained batch 854 batch loss 1.17573595 epoch total loss 1.29549372\n",
      "Trained batch 855 batch loss 1.09343767 epoch total loss 1.29525745\n",
      "Trained batch 856 batch loss 1.02690911 epoch total loss 1.29494381\n",
      "Trained batch 857 batch loss 1.24915183 epoch total loss 1.2948904\n",
      "Trained batch 858 batch loss 1.28574967 epoch total loss 1.29487979\n",
      "Trained batch 859 batch loss 1.32809222 epoch total loss 1.29491842\n",
      "Trained batch 860 batch loss 1.2998879 epoch total loss 1.29492426\n",
      "Trained batch 861 batch loss 1.22045016 epoch total loss 1.29483783\n",
      "Trained batch 862 batch loss 1.34509766 epoch total loss 1.29489613\n",
      "Trained batch 863 batch loss 1.12586498 epoch total loss 1.29470026\n",
      "Trained batch 864 batch loss 1.14881814 epoch total loss 1.29453135\n",
      "Trained batch 865 batch loss 1.27529013 epoch total loss 1.29450905\n",
      "Trained batch 866 batch loss 1.36587751 epoch total loss 1.29459143\n",
      "Trained batch 867 batch loss 1.31396616 epoch total loss 1.29461384\n",
      "Trained batch 868 batch loss 1.24655437 epoch total loss 1.29455853\n",
      "Trained batch 869 batch loss 1.28569126 epoch total loss 1.29454827\n",
      "Trained batch 870 batch loss 1.393507 epoch total loss 1.294662\n",
      "Trained batch 871 batch loss 1.29540515 epoch total loss 1.29466283\n",
      "Trained batch 872 batch loss 1.13451445 epoch total loss 1.29447925\n",
      "Trained batch 873 batch loss 1.27245951 epoch total loss 1.29445398\n",
      "Trained batch 874 batch loss 1.17757905 epoch total loss 1.29432034\n",
      "Trained batch 875 batch loss 1.20411122 epoch total loss 1.29421723\n",
      "Trained batch 876 batch loss 1.34577131 epoch total loss 1.29427612\n",
      "Trained batch 877 batch loss 1.27609038 epoch total loss 1.29425538\n",
      "Trained batch 878 batch loss 1.28175223 epoch total loss 1.29424119\n",
      "Trained batch 879 batch loss 1.16384935 epoch total loss 1.29409277\n",
      "Trained batch 880 batch loss 1.18627453 epoch total loss 1.29397023\n",
      "Trained batch 881 batch loss 1.21446538 epoch total loss 1.29388011\n",
      "Trained batch 882 batch loss 1.25432217 epoch total loss 1.29383516\n",
      "Trained batch 883 batch loss 1.23474956 epoch total loss 1.29376817\n",
      "Trained batch 884 batch loss 1.2212323 epoch total loss 1.29368615\n",
      "Trained batch 885 batch loss 1.27188551 epoch total loss 1.29366148\n",
      "Trained batch 886 batch loss 1.2580421 epoch total loss 1.2936213\n",
      "Trained batch 887 batch loss 1.25535238 epoch total loss 1.29357815\n",
      "Trained batch 888 batch loss 1.19901276 epoch total loss 1.29347157\n",
      "Trained batch 889 batch loss 1.3526473 epoch total loss 1.29353821\n",
      "Trained batch 890 batch loss 1.29635894 epoch total loss 1.29354143\n",
      "Trained batch 891 batch loss 1.19810879 epoch total loss 1.29343426\n",
      "Trained batch 892 batch loss 1.22542322 epoch total loss 1.29335809\n",
      "Trained batch 893 batch loss 1.26201546 epoch total loss 1.29332292\n",
      "Trained batch 894 batch loss 1.32678246 epoch total loss 1.29336035\n",
      "Trained batch 895 batch loss 1.32405818 epoch total loss 1.29339468\n",
      "Trained batch 896 batch loss 1.22535491 epoch total loss 1.29331875\n",
      "Trained batch 897 batch loss 1.24294853 epoch total loss 1.2932626\n",
      "Trained batch 898 batch loss 1.07636285 epoch total loss 1.29302108\n",
      "Trained batch 899 batch loss 1.15328813 epoch total loss 1.29286563\n",
      "Trained batch 900 batch loss 1.24002552 epoch total loss 1.29280698\n",
      "Trained batch 901 batch loss 1.2837069 epoch total loss 1.29279685\n",
      "Trained batch 902 batch loss 1.30748391 epoch total loss 1.29281306\n",
      "Trained batch 903 batch loss 1.23654079 epoch total loss 1.29275084\n",
      "Trained batch 904 batch loss 1.32013369 epoch total loss 1.29278123\n",
      "Trained batch 905 batch loss 1.25423658 epoch total loss 1.29273868\n",
      "Trained batch 906 batch loss 1.31594 epoch total loss 1.29276419\n",
      "Trained batch 907 batch loss 1.26132989 epoch total loss 1.29272962\n",
      "Trained batch 908 batch loss 1.23958278 epoch total loss 1.29267108\n",
      "Trained batch 909 batch loss 1.2438885 epoch total loss 1.29261744\n",
      "Trained batch 910 batch loss 1.26798105 epoch total loss 1.29259038\n",
      "Trained batch 911 batch loss 1.3407445 epoch total loss 1.29264319\n",
      "Trained batch 912 batch loss 1.22356057 epoch total loss 1.29256737\n",
      "Trained batch 913 batch loss 1.3034699 epoch total loss 1.29257929\n",
      "Trained batch 914 batch loss 1.27328944 epoch total loss 1.29255819\n",
      "Trained batch 915 batch loss 1.25387073 epoch total loss 1.29251599\n",
      "Trained batch 916 batch loss 1.23633695 epoch total loss 1.2924546\n",
      "Trained batch 917 batch loss 1.19772601 epoch total loss 1.29235137\n",
      "Trained batch 918 batch loss 1.23877501 epoch total loss 1.29229295\n",
      "Trained batch 919 batch loss 1.29099798 epoch total loss 1.29229164\n",
      "Trained batch 920 batch loss 1.33502245 epoch total loss 1.29233813\n",
      "Trained batch 921 batch loss 1.35327196 epoch total loss 1.29240429\n",
      "Trained batch 922 batch loss 1.24329865 epoch total loss 1.29235101\n",
      "Trained batch 923 batch loss 1.1504581 epoch total loss 1.29219735\n",
      "Trained batch 924 batch loss 1.20421815 epoch total loss 1.2921021\n",
      "Trained batch 925 batch loss 1.29427266 epoch total loss 1.29210448\n",
      "Trained batch 926 batch loss 1.42976153 epoch total loss 1.29225326\n",
      "Trained batch 927 batch loss 1.3214606 epoch total loss 1.29228461\n",
      "Trained batch 928 batch loss 1.3826108 epoch total loss 1.292382\n",
      "Trained batch 929 batch loss 1.36594045 epoch total loss 1.29246116\n",
      "Trained batch 930 batch loss 1.35377 epoch total loss 1.29252708\n",
      "Trained batch 931 batch loss 1.38961792 epoch total loss 1.29263139\n",
      "Trained batch 932 batch loss 1.19177341 epoch total loss 1.29252315\n",
      "Trained batch 933 batch loss 1.1099602 epoch total loss 1.29232752\n",
      "Trained batch 934 batch loss 1.05762589 epoch total loss 1.29207623\n",
      "Trained batch 935 batch loss 1.20127881 epoch total loss 1.29197907\n",
      "Trained batch 936 batch loss 1.32773924 epoch total loss 1.29201734\n",
      "Trained batch 937 batch loss 1.33094049 epoch total loss 1.29205894\n",
      "Trained batch 938 batch loss 1.3773638 epoch total loss 1.29214978\n",
      "Trained batch 939 batch loss 1.41271591 epoch total loss 1.29227817\n",
      "Trained batch 940 batch loss 1.27539527 epoch total loss 1.29226017\n",
      "Trained batch 941 batch loss 1.3496747 epoch total loss 1.29232132\n",
      "Trained batch 942 batch loss 1.36573422 epoch total loss 1.29239917\n",
      "Trained batch 943 batch loss 1.38538527 epoch total loss 1.29249787\n",
      "Trained batch 944 batch loss 1.27703047 epoch total loss 1.29248142\n",
      "Trained batch 945 batch loss 1.26973903 epoch total loss 1.29245734\n",
      "Trained batch 946 batch loss 1.25396585 epoch total loss 1.29241657\n",
      "Trained batch 947 batch loss 1.08326 epoch total loss 1.29219568\n",
      "Trained batch 948 batch loss 1.19182289 epoch total loss 1.29208982\n",
      "Trained batch 949 batch loss 1.20603466 epoch total loss 1.2919991\n",
      "Trained batch 950 batch loss 1.24344158 epoch total loss 1.29194796\n",
      "Trained batch 951 batch loss 1.3324821 epoch total loss 1.29199064\n",
      "Trained batch 952 batch loss 1.31629181 epoch total loss 1.29201615\n",
      "Trained batch 953 batch loss 1.33286786 epoch total loss 1.29205906\n",
      "Trained batch 954 batch loss 1.30373204 epoch total loss 1.29207122\n",
      "Trained batch 955 batch loss 1.24826241 epoch total loss 1.29202545\n",
      "Trained batch 956 batch loss 1.09297872 epoch total loss 1.29181731\n",
      "Trained batch 957 batch loss 1.17575479 epoch total loss 1.29169595\n",
      "Trained batch 958 batch loss 1.18222547 epoch total loss 1.29158175\n",
      "Trained batch 959 batch loss 1.28987169 epoch total loss 1.29158008\n",
      "Trained batch 960 batch loss 1.19680619 epoch total loss 1.29148126\n",
      "Trained batch 961 batch loss 1.35848963 epoch total loss 1.29155099\n",
      "Trained batch 962 batch loss 1.40923786 epoch total loss 1.2916733\n",
      "Trained batch 963 batch loss 1.20466304 epoch total loss 1.29158306\n",
      "Trained batch 964 batch loss 1.36649871 epoch total loss 1.29166067\n",
      "Trained batch 965 batch loss 1.30501235 epoch total loss 1.29167449\n",
      "Trained batch 966 batch loss 1.21594703 epoch total loss 1.29159617\n",
      "Trained batch 967 batch loss 1.24225843 epoch total loss 1.29154515\n",
      "Trained batch 968 batch loss 1.39616799 epoch total loss 1.29165316\n",
      "Trained batch 969 batch loss 1.37068856 epoch total loss 1.29173481\n",
      "Trained batch 970 batch loss 1.19429696 epoch total loss 1.29163444\n",
      "Trained batch 971 batch loss 1.29628277 epoch total loss 1.29163921\n",
      "Trained batch 972 batch loss 1.39445233 epoch total loss 1.29174495\n",
      "Trained batch 973 batch loss 1.42761183 epoch total loss 1.29188454\n",
      "Trained batch 974 batch loss 1.36049318 epoch total loss 1.29195499\n",
      "Trained batch 975 batch loss 1.30538297 epoch total loss 1.29196882\n",
      "Trained batch 976 batch loss 1.24928963 epoch total loss 1.29192507\n",
      "Trained batch 977 batch loss 1.1966356 epoch total loss 1.29182756\n",
      "Trained batch 978 batch loss 1.23161519 epoch total loss 1.29176593\n",
      "Trained batch 979 batch loss 1.16497326 epoch total loss 1.29163635\n",
      "Trained batch 980 batch loss 1.22588611 epoch total loss 1.29156923\n",
      "Trained batch 981 batch loss 1.09504533 epoch total loss 1.29136884\n",
      "Trained batch 982 batch loss 1.24257684 epoch total loss 1.29131913\n",
      "Trained batch 983 batch loss 1.11736465 epoch total loss 1.29114223\n",
      "Trained batch 984 batch loss 1.22662663 epoch total loss 1.29107666\n",
      "Trained batch 985 batch loss 1.17982817 epoch total loss 1.29096365\n",
      "Trained batch 986 batch loss 1.15130055 epoch total loss 1.29082203\n",
      "Trained batch 987 batch loss 1.189991 epoch total loss 1.29071975\n",
      "Trained batch 988 batch loss 1.19840622 epoch total loss 1.29062629\n",
      "Trained batch 989 batch loss 1.1495502 epoch total loss 1.29048359\n",
      "Trained batch 990 batch loss 1.26539028 epoch total loss 1.29045832\n",
      "Trained batch 991 batch loss 1.30521691 epoch total loss 1.2904731\n",
      "Trained batch 992 batch loss 1.40285265 epoch total loss 1.29058635\n",
      "Trained batch 993 batch loss 1.37185156 epoch total loss 1.29066825\n",
      "Trained batch 994 batch loss 1.30977929 epoch total loss 1.29068744\n",
      "Trained batch 995 batch loss 1.23844981 epoch total loss 1.29063499\n",
      "Trained batch 996 batch loss 1.22878468 epoch total loss 1.29057276\n",
      "Trained batch 997 batch loss 1.22068775 epoch total loss 1.29050279\n",
      "Trained batch 998 batch loss 1.19336176 epoch total loss 1.29040539\n",
      "Trained batch 999 batch loss 1.09918094 epoch total loss 1.29021394\n",
      "Trained batch 1000 batch loss 1.24190688 epoch total loss 1.29016566\n",
      "Trained batch 1001 batch loss 1.34647477 epoch total loss 1.29022181\n",
      "Trained batch 1002 batch loss 1.27590561 epoch total loss 1.29020751\n",
      "Trained batch 1003 batch loss 1.27990687 epoch total loss 1.29019725\n",
      "Trained batch 1004 batch loss 1.28072262 epoch total loss 1.29018784\n",
      "Trained batch 1005 batch loss 1.19179404 epoch total loss 1.29009\n",
      "Trained batch 1006 batch loss 1.24531388 epoch total loss 1.2900455\n",
      "Trained batch 1007 batch loss 1.26290345 epoch total loss 1.29001856\n",
      "Trained batch 1008 batch loss 1.21718502 epoch total loss 1.28994632\n",
      "Trained batch 1009 batch loss 1.15769899 epoch total loss 1.28981519\n",
      "Trained batch 1010 batch loss 1.22908652 epoch total loss 1.28975511\n",
      "Trained batch 1011 batch loss 1.33701944 epoch total loss 1.28980196\n",
      "Trained batch 1012 batch loss 1.33192694 epoch total loss 1.28984356\n",
      "Trained batch 1013 batch loss 1.30308914 epoch total loss 1.28985667\n",
      "Trained batch 1014 batch loss 1.2343297 epoch total loss 1.28980196\n",
      "Trained batch 1015 batch loss 1.08940303 epoch total loss 1.28960443\n",
      "Trained batch 1016 batch loss 1.12020087 epoch total loss 1.28943777\n",
      "Trained batch 1017 batch loss 1.05269217 epoch total loss 1.28920496\n",
      "Trained batch 1018 batch loss 1.12767613 epoch total loss 1.28904629\n",
      "Trained batch 1019 batch loss 1.10143316 epoch total loss 1.28886223\n",
      "Trained batch 1020 batch loss 1.26483512 epoch total loss 1.28883874\n",
      "Trained batch 1021 batch loss 1.15617895 epoch total loss 1.28870869\n",
      "Trained batch 1022 batch loss 1.13485527 epoch total loss 1.28855824\n",
      "Trained batch 1023 batch loss 1.20101428 epoch total loss 1.28847265\n",
      "Trained batch 1024 batch loss 1.26735401 epoch total loss 1.28845203\n",
      "Trained batch 1025 batch loss 1.37407541 epoch total loss 1.28853548\n",
      "Trained batch 1026 batch loss 1.34212255 epoch total loss 1.28858781\n",
      "Trained batch 1027 batch loss 1.29304051 epoch total loss 1.28859222\n",
      "Trained batch 1028 batch loss 1.26995516 epoch total loss 1.28857398\n",
      "Trained batch 1029 batch loss 1.34390748 epoch total loss 1.28862774\n",
      "Trained batch 1030 batch loss 1.33629227 epoch total loss 1.288674\n",
      "Trained batch 1031 batch loss 1.27002239 epoch total loss 1.28865588\n",
      "Trained batch 1032 batch loss 1.24690497 epoch total loss 1.28861547\n",
      "Trained batch 1033 batch loss 1.18507671 epoch total loss 1.28851521\n",
      "Trained batch 1034 batch loss 1.23962188 epoch total loss 1.288468\n",
      "Trained batch 1035 batch loss 1.1273309 epoch total loss 1.28831232\n",
      "Trained batch 1036 batch loss 1.25004137 epoch total loss 1.28827524\n",
      "Trained batch 1037 batch loss 1.26114368 epoch total loss 1.28824914\n",
      "Trained batch 1038 batch loss 1.22622061 epoch total loss 1.28818929\n",
      "Trained batch 1039 batch loss 1.12474847 epoch total loss 1.28803205\n",
      "Trained batch 1040 batch loss 1.23496616 epoch total loss 1.28798103\n",
      "Trained batch 1041 batch loss 1.19901729 epoch total loss 1.28789556\n",
      "Trained batch 1042 batch loss 1.35854185 epoch total loss 1.28796327\n",
      "Trained batch 1043 batch loss 1.16250837 epoch total loss 1.28784299\n",
      "Trained batch 1044 batch loss 1.10271788 epoch total loss 1.28766561\n",
      "Trained batch 1045 batch loss 1.02028251 epoch total loss 1.28740966\n",
      "Trained batch 1046 batch loss 1.10598838 epoch total loss 1.28723621\n",
      "Trained batch 1047 batch loss 1.30870509 epoch total loss 1.28725672\n",
      "Trained batch 1048 batch loss 1.31786811 epoch total loss 1.28728592\n",
      "Trained batch 1049 batch loss 1.44425869 epoch total loss 1.28743553\n",
      "Trained batch 1050 batch loss 1.42561591 epoch total loss 1.28756714\n",
      "Trained batch 1051 batch loss 1.31759155 epoch total loss 1.28759575\n",
      "Trained batch 1052 batch loss 1.08373642 epoch total loss 1.28740203\n",
      "Trained batch 1053 batch loss 1.33149159 epoch total loss 1.287444\n",
      "Trained batch 1054 batch loss 1.28352153 epoch total loss 1.2874403\n",
      "Trained batch 1055 batch loss 1.35458362 epoch total loss 1.28750396\n",
      "Trained batch 1056 batch loss 1.32047009 epoch total loss 1.28753507\n",
      "Trained batch 1057 batch loss 1.51839352 epoch total loss 1.28775358\n",
      "Trained batch 1058 batch loss 1.37278938 epoch total loss 1.28783393\n",
      "Trained batch 1059 batch loss 1.30127 epoch total loss 1.28784668\n",
      "Trained batch 1060 batch loss 1.30850887 epoch total loss 1.28786612\n",
      "Trained batch 1061 batch loss 1.19919777 epoch total loss 1.28778255\n",
      "Trained batch 1062 batch loss 1.18823767 epoch total loss 1.28768885\n",
      "Trained batch 1063 batch loss 1.39053905 epoch total loss 1.28778553\n",
      "Trained batch 1064 batch loss 1.4659431 epoch total loss 1.28795302\n",
      "Trained batch 1065 batch loss 1.24941194 epoch total loss 1.28791678\n",
      "Trained batch 1066 batch loss 1.27931702 epoch total loss 1.28790867\n",
      "Trained batch 1067 batch loss 1.22623706 epoch total loss 1.28785086\n",
      "Trained batch 1068 batch loss 1.36218369 epoch total loss 1.28792048\n",
      "Trained batch 1069 batch loss 1.16921949 epoch total loss 1.28780937\n",
      "Trained batch 1070 batch loss 1.2052716 epoch total loss 1.28773224\n",
      "Trained batch 1071 batch loss 1.05570734 epoch total loss 1.28751564\n",
      "Trained batch 1072 batch loss 1.0651021 epoch total loss 1.2873081\n",
      "Trained batch 1073 batch loss 1.18067861 epoch total loss 1.28720868\n",
      "Trained batch 1074 batch loss 1.1931777 epoch total loss 1.28712118\n",
      "Trained batch 1075 batch loss 1.10555887 epoch total loss 1.28695238\n",
      "Trained batch 1076 batch loss 1.18404222 epoch total loss 1.28685677\n",
      "Trained batch 1077 batch loss 1.17669189 epoch total loss 1.28675437\n",
      "Trained batch 1078 batch loss 1.30325329 epoch total loss 1.28676963\n",
      "Trained batch 1079 batch loss 1.16307509 epoch total loss 1.28665507\n",
      "Trained batch 1080 batch loss 1.20446181 epoch total loss 1.28657889\n",
      "Trained batch 1081 batch loss 1.17748535 epoch total loss 1.28647804\n",
      "Trained batch 1082 batch loss 1.35549831 epoch total loss 1.28654182\n",
      "Trained batch 1083 batch loss 1.37159228 epoch total loss 1.28662026\n",
      "Trained batch 1084 batch loss 1.26213276 epoch total loss 1.28659773\n",
      "Trained batch 1085 batch loss 1.23344612 epoch total loss 1.28654861\n",
      "Trained batch 1086 batch loss 1.33731675 epoch total loss 1.28659534\n",
      "Trained batch 1087 batch loss 1.18993568 epoch total loss 1.28650641\n",
      "Trained batch 1088 batch loss 1.4209255 epoch total loss 1.28662992\n",
      "Trained batch 1089 batch loss 1.35065901 epoch total loss 1.2866888\n",
      "Trained batch 1090 batch loss 1.350528 epoch total loss 1.28674746\n",
      "Trained batch 1091 batch loss 1.27389395 epoch total loss 1.28673565\n",
      "Trained batch 1092 batch loss 1.32341814 epoch total loss 1.28676915\n",
      "Trained batch 1093 batch loss 1.28951764 epoch total loss 1.28677177\n",
      "Trained batch 1094 batch loss 1.32149088 epoch total loss 1.28680348\n",
      "Trained batch 1095 batch loss 1.38411963 epoch total loss 1.28689241\n",
      "Trained batch 1096 batch loss 1.27682173 epoch total loss 1.28688323\n",
      "Trained batch 1097 batch loss 1.35030973 epoch total loss 1.28694117\n",
      "Trained batch 1098 batch loss 1.33014441 epoch total loss 1.28698051\n",
      "Trained batch 1099 batch loss 1.3400414 epoch total loss 1.28702879\n",
      "Trained batch 1100 batch loss 1.2818259 epoch total loss 1.28702414\n",
      "Trained batch 1101 batch loss 1.33717155 epoch total loss 1.28706968\n",
      "Trained batch 1102 batch loss 1.23755026 epoch total loss 1.28702474\n",
      "Trained batch 1103 batch loss 1.28881419 epoch total loss 1.28702641\n",
      "Trained batch 1104 batch loss 1.19860256 epoch total loss 1.2869463\n",
      "Trained batch 1105 batch loss 1.21065104 epoch total loss 1.28687727\n",
      "Trained batch 1106 batch loss 1.16711283 epoch total loss 1.28676903\n",
      "Trained batch 1107 batch loss 1.20426333 epoch total loss 1.28669441\n",
      "Trained batch 1108 batch loss 1.21318603 epoch total loss 1.28662801\n",
      "Trained batch 1109 batch loss 1.25293958 epoch total loss 1.28659761\n",
      "Trained batch 1110 batch loss 1.20906365 epoch total loss 1.28652787\n",
      "Trained batch 1111 batch loss 1.25866699 epoch total loss 1.28650272\n",
      "Trained batch 1112 batch loss 1.12004018 epoch total loss 1.28635299\n",
      "Trained batch 1113 batch loss 1.26766431 epoch total loss 1.2863363\n",
      "Trained batch 1114 batch loss 1.19663465 epoch total loss 1.28625572\n",
      "Trained batch 1115 batch loss 1.18432713 epoch total loss 1.28616428\n",
      "Trained batch 1116 batch loss 1.31358838 epoch total loss 1.28618896\n",
      "Trained batch 1117 batch loss 1.35263133 epoch total loss 1.28624845\n",
      "Trained batch 1118 batch loss 1.20345902 epoch total loss 1.28617442\n",
      "Trained batch 1119 batch loss 1.23132062 epoch total loss 1.28612542\n",
      "Trained batch 1120 batch loss 1.22685075 epoch total loss 1.28607237\n",
      "Trained batch 1121 batch loss 1.22412443 epoch total loss 1.28601718\n",
      "Trained batch 1122 batch loss 1.34437335 epoch total loss 1.28606915\n",
      "Trained batch 1123 batch loss 1.31728363 epoch total loss 1.28609693\n",
      "Trained batch 1124 batch loss 1.21294618 epoch total loss 1.28603184\n",
      "Trained batch 1125 batch loss 1.35573721 epoch total loss 1.28609371\n",
      "Trained batch 1126 batch loss 1.16937685 epoch total loss 1.28599012\n",
      "Trained batch 1127 batch loss 1.19796956 epoch total loss 1.28591204\n",
      "Trained batch 1128 batch loss 1.21305966 epoch total loss 1.28584743\n",
      "Trained batch 1129 batch loss 1.1946876 epoch total loss 1.28576672\n",
      "Trained batch 1130 batch loss 1.1367178 epoch total loss 1.28563476\n",
      "Trained batch 1131 batch loss 1.0867393 epoch total loss 1.28545904\n",
      "Trained batch 1132 batch loss 1.28369 epoch total loss 1.28545749\n",
      "Trained batch 1133 batch loss 1.28538632 epoch total loss 1.28545737\n",
      "Trained batch 1134 batch loss 1.28368604 epoch total loss 1.28545582\n",
      "Trained batch 1135 batch loss 1.25429249 epoch total loss 1.2854284\n",
      "Trained batch 1136 batch loss 1.34642327 epoch total loss 1.28548205\n",
      "Trained batch 1137 batch loss 1.27415967 epoch total loss 1.28547215\n",
      "Trained batch 1138 batch loss 1.35099041 epoch total loss 1.28552961\n",
      "Trained batch 1139 batch loss 1.28949499 epoch total loss 1.28553319\n",
      "Trained batch 1140 batch loss 1.18137991 epoch total loss 1.28544188\n",
      "Trained batch 1141 batch loss 1.17699409 epoch total loss 1.28534675\n",
      "Trained batch 1142 batch loss 1.28552651 epoch total loss 1.28534698\n",
      "Trained batch 1143 batch loss 1.2759521 epoch total loss 1.28533876\n",
      "Trained batch 1144 batch loss 1.232288 epoch total loss 1.28529239\n",
      "Trained batch 1145 batch loss 1.29131484 epoch total loss 1.28529763\n",
      "Trained batch 1146 batch loss 1.39924049 epoch total loss 1.28539705\n",
      "Trained batch 1147 batch loss 1.20783949 epoch total loss 1.28532946\n",
      "Trained batch 1148 batch loss 1.29422593 epoch total loss 1.28533721\n",
      "Trained batch 1149 batch loss 1.34799671 epoch total loss 1.28539181\n",
      "Trained batch 1150 batch loss 1.36054325 epoch total loss 1.28545713\n",
      "Trained batch 1151 batch loss 1.40348542 epoch total loss 1.28555965\n",
      "Trained batch 1152 batch loss 1.45282817 epoch total loss 1.28570497\n",
      "Trained batch 1153 batch loss 1.27135587 epoch total loss 1.28569245\n",
      "Trained batch 1154 batch loss 1.29155374 epoch total loss 1.28569758\n",
      "Trained batch 1155 batch loss 1.22576892 epoch total loss 1.2856456\n",
      "Trained batch 1156 batch loss 1.18980956 epoch total loss 1.28556263\n",
      "Trained batch 1157 batch loss 1.26460505 epoch total loss 1.28554463\n",
      "Trained batch 1158 batch loss 1.19448 epoch total loss 1.28546596\n",
      "Trained batch 1159 batch loss 1.35313892 epoch total loss 1.28552437\n",
      "Trained batch 1160 batch loss 1.20405364 epoch total loss 1.28545415\n",
      "Trained batch 1161 batch loss 1.22139215 epoch total loss 1.28539908\n",
      "Trained batch 1162 batch loss 1.32738328 epoch total loss 1.2854352\n",
      "Trained batch 1163 batch loss 1.28846371 epoch total loss 1.2854377\n",
      "Trained batch 1164 batch loss 1.09908223 epoch total loss 1.28527772\n",
      "Trained batch 1165 batch loss 1.19311428 epoch total loss 1.28519857\n",
      "Trained batch 1166 batch loss 1.29302645 epoch total loss 1.28520525\n",
      "Trained batch 1167 batch loss 1.1535964 epoch total loss 1.28509247\n",
      "Trained batch 1168 batch loss 1.17427635 epoch total loss 1.28499758\n",
      "Trained batch 1169 batch loss 1.1252923 epoch total loss 1.28486097\n",
      "Trained batch 1170 batch loss 1.28402185 epoch total loss 1.28486025\n",
      "Trained batch 1171 batch loss 1.2559967 epoch total loss 1.28483558\n",
      "Trained batch 1172 batch loss 1.18794096 epoch total loss 1.28475296\n",
      "Trained batch 1173 batch loss 1.36305225 epoch total loss 1.28481972\n",
      "Trained batch 1174 batch loss 1.44818211 epoch total loss 1.28495896\n",
      "Trained batch 1175 batch loss 1.43751335 epoch total loss 1.28508878\n",
      "Trained batch 1176 batch loss 1.54038036 epoch total loss 1.28530586\n",
      "Trained batch 1177 batch loss 1.35852933 epoch total loss 1.28536808\n",
      "Trained batch 1178 batch loss 1.32382083 epoch total loss 1.28540075\n",
      "Trained batch 1179 batch loss 1.2807765 epoch total loss 1.28539681\n",
      "Trained batch 1180 batch loss 1.24099457 epoch total loss 1.28535914\n",
      "Trained batch 1181 batch loss 1.18775988 epoch total loss 1.28527641\n",
      "Trained batch 1182 batch loss 1.19075727 epoch total loss 1.28519654\n",
      "Trained batch 1183 batch loss 1.41306221 epoch total loss 1.28530467\n",
      "Trained batch 1184 batch loss 1.24941123 epoch total loss 1.28527427\n",
      "Trained batch 1185 batch loss 1.32235503 epoch total loss 1.28530562\n",
      "Trained batch 1186 batch loss 1.18170071 epoch total loss 1.28521824\n",
      "Trained batch 1187 batch loss 1.1821754 epoch total loss 1.28513134\n",
      "Trained batch 1188 batch loss 1.11382782 epoch total loss 1.28498709\n",
      "Trained batch 1189 batch loss 1.24968207 epoch total loss 1.28495741\n",
      "Trained batch 1190 batch loss 1.19394422 epoch total loss 1.28488088\n",
      "Trained batch 1191 batch loss 1.15511954 epoch total loss 1.28477204\n",
      "Trained batch 1192 batch loss 1.22913086 epoch total loss 1.28472531\n",
      "Trained batch 1193 batch loss 1.37879372 epoch total loss 1.28480411\n",
      "Trained batch 1194 batch loss 1.20089054 epoch total loss 1.28473389\n",
      "Trained batch 1195 batch loss 1.21892416 epoch total loss 1.28467882\n",
      "Trained batch 1196 batch loss 1.09959805 epoch total loss 1.28452408\n",
      "Trained batch 1197 batch loss 1.1016252 epoch total loss 1.28437126\n",
      "Trained batch 1198 batch loss 1.18788671 epoch total loss 1.28429079\n",
      "Trained batch 1199 batch loss 1.18040013 epoch total loss 1.28420413\n",
      "Trained batch 1200 batch loss 1.29538965 epoch total loss 1.28421342\n",
      "Trained batch 1201 batch loss 1.2100358 epoch total loss 1.28415179\n",
      "Trained batch 1202 batch loss 1.26771069 epoch total loss 1.28413808\n",
      "Trained batch 1203 batch loss 1.31243885 epoch total loss 1.28416157\n",
      "Trained batch 1204 batch loss 1.36179543 epoch total loss 1.28422606\n",
      "Trained batch 1205 batch loss 1.33163178 epoch total loss 1.2842654\n",
      "Trained batch 1206 batch loss 1.28876209 epoch total loss 1.28426921\n",
      "Trained batch 1207 batch loss 1.13054013 epoch total loss 1.28414178\n",
      "Trained batch 1208 batch loss 1.21807277 epoch total loss 1.28408706\n",
      "Trained batch 1209 batch loss 1.30689442 epoch total loss 1.2841059\n",
      "Trained batch 1210 batch loss 1.31699705 epoch total loss 1.28413308\n",
      "Trained batch 1211 batch loss 1.4044888 epoch total loss 1.2842325\n",
      "Trained batch 1212 batch loss 1.35850906 epoch total loss 1.28429377\n",
      "Trained batch 1213 batch loss 1.28012764 epoch total loss 1.28429043\n",
      "Trained batch 1214 batch loss 1.25816345 epoch total loss 1.28426886\n",
      "Trained batch 1215 batch loss 1.19010532 epoch total loss 1.28419137\n",
      "Trained batch 1216 batch loss 1.37438369 epoch total loss 1.28426552\n",
      "Trained batch 1217 batch loss 1.24921489 epoch total loss 1.28423679\n",
      "Trained batch 1218 batch loss 1.0949322 epoch total loss 1.28408134\n",
      "Trained batch 1219 batch loss 1.04549956 epoch total loss 1.28388572\n",
      "Trained batch 1220 batch loss 1.10438156 epoch total loss 1.28373849\n",
      "Trained batch 1221 batch loss 1.5418278 epoch total loss 1.28395\n",
      "Trained batch 1222 batch loss 1.39666665 epoch total loss 1.28404212\n",
      "Trained batch 1223 batch loss 1.33103859 epoch total loss 1.28408062\n",
      "Trained batch 1224 batch loss 1.19931483 epoch total loss 1.28401136\n",
      "Trained batch 1225 batch loss 1.22716761 epoch total loss 1.28396499\n",
      "Trained batch 1226 batch loss 1.2400645 epoch total loss 1.28392923\n",
      "Trained batch 1227 batch loss 1.15797734 epoch total loss 1.28382647\n",
      "Trained batch 1228 batch loss 1.24221396 epoch total loss 1.28379261\n",
      "Trained batch 1229 batch loss 1.2645328 epoch total loss 1.28377688\n",
      "Trained batch 1230 batch loss 1.29207432 epoch total loss 1.28378367\n",
      "Trained batch 1231 batch loss 1.24691844 epoch total loss 1.28375375\n",
      "Trained batch 1232 batch loss 1.33208835 epoch total loss 1.28379297\n",
      "Trained batch 1233 batch loss 1.24788702 epoch total loss 1.28376389\n",
      "Trained batch 1234 batch loss 1.15004778 epoch total loss 1.28365552\n",
      "Trained batch 1235 batch loss 1.25293958 epoch total loss 1.28363061\n",
      "Trained batch 1236 batch loss 1.23251832 epoch total loss 1.28358924\n",
      "Trained batch 1237 batch loss 1.26384473 epoch total loss 1.28357327\n",
      "Trained batch 1238 batch loss 1.27333701 epoch total loss 1.28356504\n",
      "Trained batch 1239 batch loss 1.26234961 epoch total loss 1.28354788\n",
      "Trained batch 1240 batch loss 1.31751156 epoch total loss 1.2835753\n",
      "Trained batch 1241 batch loss 1.32831764 epoch total loss 1.2836113\n",
      "Trained batch 1242 batch loss 1.25259495 epoch total loss 1.28358638\n",
      "Trained batch 1243 batch loss 1.1904453 epoch total loss 1.2835114\n",
      "Trained batch 1244 batch loss 1.16613126 epoch total loss 1.28341699\n",
      "Trained batch 1245 batch loss 1.13689506 epoch total loss 1.28329933\n",
      "Trained batch 1246 batch loss 1.26434624 epoch total loss 1.28328419\n",
      "Trained batch 1247 batch loss 1.29825354 epoch total loss 1.28329611\n",
      "Trained batch 1248 batch loss 1.17653894 epoch total loss 1.28321052\n",
      "Trained batch 1249 batch loss 1.18420899 epoch total loss 1.28313124\n",
      "Trained batch 1250 batch loss 1.14610064 epoch total loss 1.28302169\n",
      "Trained batch 1251 batch loss 1.35310102 epoch total loss 1.28307772\n",
      "Trained batch 1252 batch loss 1.49107 epoch total loss 1.28324389\n",
      "Trained batch 1253 batch loss 1.18809688 epoch total loss 1.28316796\n",
      "Trained batch 1254 batch loss 1.25707698 epoch total loss 1.2831471\n",
      "Trained batch 1255 batch loss 1.09266579 epoch total loss 1.28299534\n",
      "Trained batch 1256 batch loss 0.938721657 epoch total loss 1.28272128\n",
      "Trained batch 1257 batch loss 1.01374245 epoch total loss 1.2825073\n",
      "Trained batch 1258 batch loss 1.05243337 epoch total loss 1.28232443\n",
      "Trained batch 1259 batch loss 0.965540826 epoch total loss 1.2820729\n",
      "Trained batch 1260 batch loss 1.30211008 epoch total loss 1.28208876\n",
      "Trained batch 1261 batch loss 1.31618667 epoch total loss 1.28211582\n",
      "Trained batch 1262 batch loss 1.25131 epoch total loss 1.28209138\n",
      "Trained batch 1263 batch loss 1.41154575 epoch total loss 1.2821939\n",
      "Trained batch 1264 batch loss 1.37126064 epoch total loss 1.28226435\n",
      "Trained batch 1265 batch loss 1.33790183 epoch total loss 1.28230834\n",
      "Trained batch 1266 batch loss 1.1638484 epoch total loss 1.28221476\n",
      "Trained batch 1267 batch loss 1.13795495 epoch total loss 1.2821008\n",
      "Trained batch 1268 batch loss 1.18661952 epoch total loss 1.28202558\n",
      "Trained batch 1269 batch loss 1.28493595 epoch total loss 1.28202784\n",
      "Trained batch 1270 batch loss 1.26299453 epoch total loss 1.28201282\n",
      "Trained batch 1271 batch loss 1.3224535 epoch total loss 1.28204465\n",
      "Trained batch 1272 batch loss 1.37557864 epoch total loss 1.2821182\n",
      "Trained batch 1273 batch loss 1.40946054 epoch total loss 1.28221822\n",
      "Trained batch 1274 batch loss 1.30593348 epoch total loss 1.28223681\n",
      "Trained batch 1275 batch loss 1.38174319 epoch total loss 1.28231478\n",
      "Trained batch 1276 batch loss 1.33177006 epoch total loss 1.28235364\n",
      "Trained batch 1277 batch loss 1.33923173 epoch total loss 1.2823981\n",
      "Trained batch 1278 batch loss 1.45406616 epoch total loss 1.28253245\n",
      "Trained batch 1279 batch loss 1.30675983 epoch total loss 1.28255141\n",
      "Trained batch 1280 batch loss 1.32827365 epoch total loss 1.28258717\n",
      "Trained batch 1281 batch loss 1.31981564 epoch total loss 1.28261626\n",
      "Trained batch 1282 batch loss 1.27352214 epoch total loss 1.28260911\n",
      "Trained batch 1283 batch loss 1.32490432 epoch total loss 1.28264213\n",
      "Trained batch 1284 batch loss 1.34643388 epoch total loss 1.28269184\n",
      "Trained batch 1285 batch loss 1.19668436 epoch total loss 1.28262484\n",
      "Trained batch 1286 batch loss 1.28136992 epoch total loss 1.28262389\n",
      "Trained batch 1287 batch loss 1.27818274 epoch total loss 1.28262043\n",
      "Trained batch 1288 batch loss 1.23118937 epoch total loss 1.28258049\n",
      "Trained batch 1289 batch loss 1.16139936 epoch total loss 1.28248656\n",
      "Trained batch 1290 batch loss 1.28068769 epoch total loss 1.28248513\n",
      "Trained batch 1291 batch loss 1.34603691 epoch total loss 1.28253436\n",
      "Trained batch 1292 batch loss 1.36739874 epoch total loss 1.2826\n",
      "Trained batch 1293 batch loss 1.34075797 epoch total loss 1.28264499\n",
      "Trained batch 1294 batch loss 1.23298395 epoch total loss 1.2826066\n",
      "Trained batch 1295 batch loss 1.25819874 epoch total loss 1.28258777\n",
      "Trained batch 1296 batch loss 1.35442424 epoch total loss 1.2826432\n",
      "Trained batch 1297 batch loss 1.20820189 epoch total loss 1.28258586\n",
      "Trained batch 1298 batch loss 1.20991218 epoch total loss 1.28252983\n",
      "Trained batch 1299 batch loss 1.23402238 epoch total loss 1.28249252\n",
      "Trained batch 1300 batch loss 1.20629454 epoch total loss 1.28243387\n",
      "Trained batch 1301 batch loss 1.25427508 epoch total loss 1.28241229\n",
      "Trained batch 1302 batch loss 1.37308598 epoch total loss 1.28248191\n",
      "Trained batch 1303 batch loss 1.26856613 epoch total loss 1.28247118\n",
      "Trained batch 1304 batch loss 1.22154951 epoch total loss 1.28242445\n",
      "Trained batch 1305 batch loss 1.2772665 epoch total loss 1.28242052\n",
      "Trained batch 1306 batch loss 1.36803198 epoch total loss 1.28248608\n",
      "Trained batch 1307 batch loss 1.3731451 epoch total loss 1.28255546\n",
      "Trained batch 1308 batch loss 1.18516 epoch total loss 1.28248096\n",
      "Trained batch 1309 batch loss 1.10549724 epoch total loss 1.28234577\n",
      "Trained batch 1310 batch loss 1.18461227 epoch total loss 1.28227115\n",
      "Trained batch 1311 batch loss 1.13907444 epoch total loss 1.28216183\n",
      "Trained batch 1312 batch loss 1.15887344 epoch total loss 1.28206778\n",
      "Trained batch 1313 batch loss 1.22744203 epoch total loss 1.28202617\n",
      "Trained batch 1314 batch loss 1.21598184 epoch total loss 1.28197587\n",
      "Trained batch 1315 batch loss 1.20849085 epoch total loss 1.28192008\n",
      "Trained batch 1316 batch loss 1.17629099 epoch total loss 1.28183973\n",
      "Trained batch 1317 batch loss 1.12517428 epoch total loss 1.28172076\n",
      "Trained batch 1318 batch loss 1.1719389 epoch total loss 1.28163755\n",
      "Trained batch 1319 batch loss 1.24705601 epoch total loss 1.28161132\n",
      "Trained batch 1320 batch loss 1.32220614 epoch total loss 1.28164208\n",
      "Trained batch 1321 batch loss 1.35451961 epoch total loss 1.28169727\n",
      "Trained batch 1322 batch loss 1.27382994 epoch total loss 1.28169131\n",
      "Trained batch 1323 batch loss 1.35559821 epoch total loss 1.2817471\n",
      "Trained batch 1324 batch loss 1.44883811 epoch total loss 1.28187335\n",
      "Trained batch 1325 batch loss 1.25502372 epoch total loss 1.28185308\n",
      "Trained batch 1326 batch loss 1.31636322 epoch total loss 1.28187919\n",
      "Trained batch 1327 batch loss 1.46885252 epoch total loss 1.28202009\n",
      "Trained batch 1328 batch loss 1.345227 epoch total loss 1.28206766\n",
      "Trained batch 1329 batch loss 1.29150093 epoch total loss 1.28207469\n",
      "Trained batch 1330 batch loss 1.24652553 epoch total loss 1.28204799\n",
      "Trained batch 1331 batch loss 1.31652355 epoch total loss 1.28207397\n",
      "Trained batch 1332 batch loss 1.22322619 epoch total loss 1.28202975\n",
      "Trained batch 1333 batch loss 1.22209287 epoch total loss 1.28198481\n",
      "Trained batch 1334 batch loss 1.26595902 epoch total loss 1.28197277\n",
      "Trained batch 1335 batch loss 1.32213831 epoch total loss 1.28200293\n",
      "Trained batch 1336 batch loss 1.38425791 epoch total loss 1.28207946\n",
      "Trained batch 1337 batch loss 1.35268688 epoch total loss 1.28213227\n",
      "Trained batch 1338 batch loss 1.32066429 epoch total loss 1.282161\n",
      "Trained batch 1339 batch loss 1.21516562 epoch total loss 1.28211105\n",
      "Trained batch 1340 batch loss 1.24393988 epoch total loss 1.28208256\n",
      "Trained batch 1341 batch loss 1.11997724 epoch total loss 1.28196168\n",
      "Trained batch 1342 batch loss 1.19129598 epoch total loss 1.28189409\n",
      "Trained batch 1343 batch loss 1.37190342 epoch total loss 1.2819612\n",
      "Trained batch 1344 batch loss 1.20451498 epoch total loss 1.28190351\n",
      "Trained batch 1345 batch loss 1.28392327 epoch total loss 1.28190506\n",
      "Trained batch 1346 batch loss 1.2765919 epoch total loss 1.28190112\n",
      "Trained batch 1347 batch loss 1.35476935 epoch total loss 1.28195512\n",
      "Trained batch 1348 batch loss 1.26826358 epoch total loss 1.28194499\n",
      "Trained batch 1349 batch loss 1.18632114 epoch total loss 1.28187406\n",
      "Trained batch 1350 batch loss 1.2091496 epoch total loss 1.28182018\n",
      "Trained batch 1351 batch loss 1.16994715 epoch total loss 1.28173733\n",
      "Trained batch 1352 batch loss 1.2785362 epoch total loss 1.28173506\n",
      "Trained batch 1353 batch loss 1.2711699 epoch total loss 1.28172719\n",
      "Trained batch 1354 batch loss 1.30231953 epoch total loss 1.28174245\n",
      "Trained batch 1355 batch loss 1.25259542 epoch total loss 1.28172088\n",
      "Trained batch 1356 batch loss 1.21471441 epoch total loss 1.28167152\n",
      "Trained batch 1357 batch loss 1.20220315 epoch total loss 1.28161287\n",
      "Trained batch 1358 batch loss 1.22606361 epoch total loss 1.28157198\n",
      "Trained batch 1359 batch loss 1.20523715 epoch total loss 1.28151584\n",
      "Trained batch 1360 batch loss 1.14797008 epoch total loss 1.28141761\n",
      "Trained batch 1361 batch loss 1.2338239 epoch total loss 1.28138256\n",
      "Trained batch 1362 batch loss 1.28600252 epoch total loss 1.28138602\n",
      "Trained batch 1363 batch loss 1.28353739 epoch total loss 1.28138757\n",
      "Trained batch 1364 batch loss 1.18170297 epoch total loss 1.28131449\n",
      "Trained batch 1365 batch loss 1.23530602 epoch total loss 1.28128088\n",
      "Trained batch 1366 batch loss 1.1561749 epoch total loss 1.2811892\n",
      "Trained batch 1367 batch loss 1.0943892 epoch total loss 1.28105259\n",
      "Trained batch 1368 batch loss 1.135939 epoch total loss 1.28094649\n",
      "Trained batch 1369 batch loss 1.21807599 epoch total loss 1.2809006\n",
      "Trained batch 1370 batch loss 1.14164007 epoch total loss 1.28079891\n",
      "Trained batch 1371 batch loss 1.20734656 epoch total loss 1.28074539\n",
      "Trained batch 1372 batch loss 1.26850021 epoch total loss 1.28073645\n",
      "Trained batch 1373 batch loss 1.26200533 epoch total loss 1.28072274\n",
      "Trained batch 1374 batch loss 1.33103883 epoch total loss 1.28075945\n",
      "Trained batch 1375 batch loss 1.33892238 epoch total loss 1.28080165\n",
      "Trained batch 1376 batch loss 1.34935868 epoch total loss 1.28085148\n",
      "Trained batch 1377 batch loss 1.19641113 epoch total loss 1.28079021\n",
      "Trained batch 1378 batch loss 1.1667031 epoch total loss 1.28070736\n",
      "Trained batch 1379 batch loss 1.14466441 epoch total loss 1.28060877\n",
      "Trained batch 1380 batch loss 1.23496914 epoch total loss 1.28057575\n",
      "Trained batch 1381 batch loss 1.24735916 epoch total loss 1.28055155\n",
      "Trained batch 1382 batch loss 1.3171382 epoch total loss 1.28057814\n",
      "Trained batch 1383 batch loss 1.31941462 epoch total loss 1.28060615\n",
      "Trained batch 1384 batch loss 1.24752188 epoch total loss 1.28058231\n",
      "Trained batch 1385 batch loss 1.22002387 epoch total loss 1.28053856\n",
      "Trained batch 1386 batch loss 1.29903805 epoch total loss 1.28055191\n",
      "Trained batch 1387 batch loss 1.21856225 epoch total loss 1.28050721\n",
      "Trained batch 1388 batch loss 1.05364943 epoch total loss 1.28034365\n",
      "Epoch 3 train loss 1.2803436517715454\n",
      "Validated batch 1 batch loss 1.25657451\n",
      "Validated batch 2 batch loss 1.14435506\n",
      "Validated batch 3 batch loss 1.30786681\n",
      "Validated batch 4 batch loss 1.18177485\n",
      "Validated batch 5 batch loss 1.2498858\n",
      "Validated batch 6 batch loss 1.28437972\n",
      "Validated batch 7 batch loss 1.31082582\n",
      "Validated batch 8 batch loss 1.39267468\n",
      "Validated batch 9 batch loss 1.36556458\n",
      "Validated batch 10 batch loss 1.27101064\n",
      "Validated batch 11 batch loss 1.30142725\n",
      "Validated batch 12 batch loss 1.35795832\n",
      "Validated batch 13 batch loss 1.35218024\n",
      "Validated batch 14 batch loss 1.35106289\n",
      "Validated batch 15 batch loss 1.33112431\n",
      "Validated batch 16 batch loss 1.35583055\n",
      "Validated batch 17 batch loss 1.31603885\n",
      "Validated batch 18 batch loss 1.1861161\n",
      "Validated batch 19 batch loss 1.28058529\n",
      "Validated batch 20 batch loss 1.3450346\n",
      "Validated batch 21 batch loss 1.30360651\n",
      "Validated batch 22 batch loss 1.30457187\n",
      "Validated batch 23 batch loss 1.24629\n",
      "Validated batch 24 batch loss 1.18199587\n",
      "Validated batch 25 batch loss 1.30306184\n",
      "Validated batch 26 batch loss 1.29473829\n",
      "Validated batch 27 batch loss 1.22898388\n",
      "Validated batch 28 batch loss 1.36893034\n",
      "Validated batch 29 batch loss 1.41360044\n",
      "Validated batch 30 batch loss 1.14515817\n",
      "Validated batch 31 batch loss 1.29283869\n",
      "Validated batch 32 batch loss 1.26698685\n",
      "Validated batch 33 batch loss 1.34803247\n",
      "Validated batch 34 batch loss 1.29948616\n",
      "Validated batch 35 batch loss 1.14020014\n",
      "Validated batch 36 batch loss 1.15243292\n",
      "Validated batch 37 batch loss 1.28044772\n",
      "Validated batch 38 batch loss 1.24773431\n",
      "Validated batch 39 batch loss 1.22119069\n",
      "Validated batch 40 batch loss 1.28416634\n",
      "Validated batch 41 batch loss 1.20902514\n",
      "Validated batch 42 batch loss 1.31540382\n",
      "Validated batch 43 batch loss 1.34677696\n",
      "Validated batch 44 batch loss 1.34164548\n",
      "Validated batch 45 batch loss 1.25399745\n",
      "Validated batch 46 batch loss 1.19306254\n",
      "Validated batch 47 batch loss 1.21355367\n",
      "Validated batch 48 batch loss 1.24009335\n",
      "Validated batch 49 batch loss 1.24584413\n",
      "Validated batch 50 batch loss 1.16318655\n",
      "Validated batch 51 batch loss 1.2110455\n",
      "Validated batch 52 batch loss 1.34033358\n",
      "Validated batch 53 batch loss 1.21444178\n",
      "Validated batch 54 batch loss 1.10919774\n",
      "Validated batch 55 batch loss 1.22141767\n",
      "Validated batch 56 batch loss 1.23192251\n",
      "Validated batch 57 batch loss 1.18244588\n",
      "Validated batch 58 batch loss 1.25305867\n",
      "Validated batch 59 batch loss 1.27366447\n",
      "Validated batch 60 batch loss 1.24448061\n",
      "Validated batch 61 batch loss 1.34307957\n",
      "Validated batch 62 batch loss 1.38284886\n",
      "Validated batch 63 batch loss 1.21596\n",
      "Validated batch 64 batch loss 1.39904249\n",
      "Validated batch 65 batch loss 1.08047271\n",
      "Validated batch 66 batch loss 1.24719965\n",
      "Validated batch 67 batch loss 1.18074083\n",
      "Validated batch 68 batch loss 1.28432894\n",
      "Validated batch 69 batch loss 1.41643155\n",
      "Validated batch 70 batch loss 1.26608253\n",
      "Validated batch 71 batch loss 1.30163193\n",
      "Validated batch 72 batch loss 1.19583941\n",
      "Validated batch 73 batch loss 1.29200327\n",
      "Validated batch 74 batch loss 1.24118662\n",
      "Validated batch 75 batch loss 1.30486739\n",
      "Validated batch 76 batch loss 1.33891535\n",
      "Validated batch 77 batch loss 1.36994553\n",
      "Validated batch 78 batch loss 1.30846286\n",
      "Validated batch 79 batch loss 1.22954369\n",
      "Validated batch 80 batch loss 1.36903143\n",
      "Validated batch 81 batch loss 1.27383149\n",
      "Validated batch 82 batch loss 1.30681062\n",
      "Validated batch 83 batch loss 1.38485324\n",
      "Validated batch 84 batch loss 1.32457781\n",
      "Validated batch 85 batch loss 1.30160785\n",
      "Validated batch 86 batch loss 1.45266461\n",
      "Validated batch 87 batch loss 1.12212801\n",
      "Validated batch 88 batch loss 1.31266129\n",
      "Validated batch 89 batch loss 1.13335967\n",
      "Validated batch 90 batch loss 1.23021364\n",
      "Validated batch 91 batch loss 1.42436779\n",
      "Validated batch 92 batch loss 1.21905041\n",
      "Validated batch 93 batch loss 1.30555987\n",
      "Validated batch 94 batch loss 1.34946322\n",
      "Validated batch 95 batch loss 1.17196143\n",
      "Validated batch 96 batch loss 1.20333862\n",
      "Validated batch 97 batch loss 1.29614925\n",
      "Validated batch 98 batch loss 1.24771643\n",
      "Validated batch 99 batch loss 1.2501334\n",
      "Validated batch 100 batch loss 1.27485871\n",
      "Validated batch 101 batch loss 1.1963414\n",
      "Validated batch 102 batch loss 1.37582982\n",
      "Validated batch 103 batch loss 1.19810224\n",
      "Validated batch 104 batch loss 1.1550138\n",
      "Validated batch 105 batch loss 1.25943685\n",
      "Validated batch 106 batch loss 1.41038203\n",
      "Validated batch 107 batch loss 1.36210728\n",
      "Validated batch 108 batch loss 1.44932353\n",
      "Validated batch 109 batch loss 1.19068837\n",
      "Validated batch 110 batch loss 1.41467369\n",
      "Validated batch 111 batch loss 1.24822164\n",
      "Validated batch 112 batch loss 1.35062838\n",
      "Validated batch 113 batch loss 1.33736026\n",
      "Validated batch 114 batch loss 1.0680635\n",
      "Validated batch 115 batch loss 1.26551962\n",
      "Validated batch 116 batch loss 1.31165576\n",
      "Validated batch 117 batch loss 1.30175459\n",
      "Validated batch 118 batch loss 1.26875663\n",
      "Validated batch 119 batch loss 1.26711178\n",
      "Validated batch 120 batch loss 1.28541207\n",
      "Validated batch 121 batch loss 1.45079494\n",
      "Validated batch 122 batch loss 1.22389483\n",
      "Validated batch 123 batch loss 1.32579923\n",
      "Validated batch 124 batch loss 1.24965167\n",
      "Validated batch 125 batch loss 1.32899094\n",
      "Validated batch 126 batch loss 1.26881075\n",
      "Validated batch 127 batch loss 1.19366527\n",
      "Validated batch 128 batch loss 1.34013987\n",
      "Validated batch 129 batch loss 1.35192084\n",
      "Validated batch 130 batch loss 1.3393929\n",
      "Validated batch 131 batch loss 1.27335763\n",
      "Validated batch 132 batch loss 1.31768668\n",
      "Validated batch 133 batch loss 1.21671748\n",
      "Validated batch 134 batch loss 1.21686471\n",
      "Validated batch 135 batch loss 1.27337706\n",
      "Validated batch 136 batch loss 1.205194\n",
      "Validated batch 137 batch loss 1.28758264\n",
      "Validated batch 138 batch loss 1.27077305\n",
      "Validated batch 139 batch loss 1.32324\n",
      "Validated batch 140 batch loss 1.12777901\n",
      "Validated batch 141 batch loss 1.23727679\n",
      "Validated batch 142 batch loss 1.23735774\n",
      "Validated batch 143 batch loss 1.19721365\n",
      "Validated batch 144 batch loss 1.27193904\n",
      "Validated batch 145 batch loss 1.24365616\n",
      "Validated batch 146 batch loss 1.30246949\n",
      "Validated batch 147 batch loss 1.36860359\n",
      "Validated batch 148 batch loss 1.14772248\n",
      "Validated batch 149 batch loss 1.40803909\n",
      "Validated batch 150 batch loss 1.27263\n",
      "Validated batch 151 batch loss 1.16750598\n",
      "Validated batch 152 batch loss 1.30010557\n",
      "Validated batch 153 batch loss 1.27573085\n",
      "Validated batch 154 batch loss 1.20744181\n",
      "Validated batch 155 batch loss 1.41821539\n",
      "Validated batch 156 batch loss 1.28933525\n",
      "Validated batch 157 batch loss 1.3352176\n",
      "Validated batch 158 batch loss 1.21181738\n",
      "Validated batch 159 batch loss 1.26031661\n",
      "Validated batch 160 batch loss 1.24805343\n",
      "Validated batch 161 batch loss 1.20552349\n",
      "Validated batch 162 batch loss 1.33253443\n",
      "Validated batch 163 batch loss 1.27918422\n",
      "Validated batch 164 batch loss 1.2832948\n",
      "Validated batch 165 batch loss 1.27672982\n",
      "Validated batch 166 batch loss 1.16407919\n",
      "Validated batch 167 batch loss 1.3261764\n",
      "Validated batch 168 batch loss 1.32591259\n",
      "Validated batch 169 batch loss 1.18323803\n",
      "Validated batch 170 batch loss 1.19780946\n",
      "Validated batch 171 batch loss 1.30693483\n",
      "Validated batch 172 batch loss 1.29530573\n",
      "Validated batch 173 batch loss 1.36704063\n",
      "Validated batch 174 batch loss 1.30205417\n",
      "Validated batch 175 batch loss 1.19233894\n",
      "Validated batch 176 batch loss 1.2711072\n",
      "Validated batch 177 batch loss 1.26020777\n",
      "Validated batch 178 batch loss 1.23700309\n",
      "Validated batch 179 batch loss 1.29439819\n",
      "Validated batch 180 batch loss 1.33025503\n",
      "Validated batch 181 batch loss 1.47600174\n",
      "Validated batch 182 batch loss 1.45707202\n",
      "Validated batch 183 batch loss 1.3093276\n",
      "Validated batch 184 batch loss 1.19246602\n",
      "Validated batch 185 batch loss 1.1307025\n",
      "Epoch 3 val loss 1.276343822479248\n",
      "Model /aiffel/aiffel/mpii/models/model-epoch-3-loss-1.2763.h5 saved.\n",
      "Start epoch 4 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.1567657 epoch total loss 1.1567657\n",
      "Trained batch 2 batch loss 1.23817217 epoch total loss 1.197469\n",
      "Trained batch 3 batch loss 1.16523683 epoch total loss 1.18672502\n",
      "Trained batch 4 batch loss 1.23382974 epoch total loss 1.19850111\n",
      "Trained batch 5 batch loss 1.16251683 epoch total loss 1.19130421\n",
      "Trained batch 6 batch loss 1.20696557 epoch total loss 1.19391441\n",
      "Trained batch 7 batch loss 1.21233511 epoch total loss 1.19654596\n",
      "Trained batch 8 batch loss 1.11962211 epoch total loss 1.18693054\n",
      "Trained batch 9 batch loss 1.08542943 epoch total loss 1.17565262\n",
      "Trained batch 10 batch loss 0.931576729 epoch total loss 1.151245\n",
      "Trained batch 11 batch loss 1.11374044 epoch total loss 1.14783549\n",
      "Trained batch 12 batch loss 1.31206512 epoch total loss 1.16152132\n",
      "Trained batch 13 batch loss 1.31768513 epoch total loss 1.17353392\n",
      "Trained batch 14 batch loss 1.25887728 epoch total loss 1.17962992\n",
      "Trained batch 15 batch loss 1.25735068 epoch total loss 1.18481123\n",
      "Trained batch 16 batch loss 1.23496556 epoch total loss 1.18794596\n",
      "Trained batch 17 batch loss 1.08048654 epoch total loss 1.18162477\n",
      "Trained batch 18 batch loss 1.16816735 epoch total loss 1.18087721\n",
      "Trained batch 19 batch loss 1.15332973 epoch total loss 1.17942727\n",
      "Trained batch 20 batch loss 1.27519965 epoch total loss 1.1842159\n",
      "Trained batch 21 batch loss 1.18532884 epoch total loss 1.18426895\n",
      "Trained batch 22 batch loss 1.17990899 epoch total loss 1.18407071\n",
      "Trained batch 23 batch loss 1.204404 epoch total loss 1.18495488\n",
      "Trained batch 24 batch loss 1.15160465 epoch total loss 1.18356526\n",
      "Trained batch 25 batch loss 1.20140612 epoch total loss 1.18427896\n",
      "Trained batch 26 batch loss 1.13473964 epoch total loss 1.18237352\n",
      "Trained batch 27 batch loss 1.16308033 epoch total loss 1.18165898\n",
      "Trained batch 28 batch loss 1.192379 epoch total loss 1.18204188\n",
      "Trained batch 29 batch loss 1.2326746 epoch total loss 1.18378782\n",
      "Trained batch 30 batch loss 1.1733638 epoch total loss 1.18344033\n",
      "Trained batch 31 batch loss 1.22350979 epoch total loss 1.18473291\n",
      "Trained batch 32 batch loss 1.24917471 epoch total loss 1.18674672\n",
      "Trained batch 33 batch loss 1.15512323 epoch total loss 1.18578851\n",
      "Trained batch 34 batch loss 1.251665 epoch total loss 1.18772602\n",
      "Trained batch 35 batch loss 1.25552702 epoch total loss 1.18966317\n",
      "Trained batch 36 batch loss 1.18312049 epoch total loss 1.18948138\n",
      "Trained batch 37 batch loss 1.21884501 epoch total loss 1.19027507\n",
      "Trained batch 38 batch loss 1.35734177 epoch total loss 1.19467151\n",
      "Trained batch 39 batch loss 1.26981878 epoch total loss 1.19659829\n",
      "Trained batch 40 batch loss 1.3486861 epoch total loss 1.20040059\n",
      "Trained batch 41 batch loss 1.10787427 epoch total loss 1.19814384\n",
      "Trained batch 42 batch loss 1.18040562 epoch total loss 1.19772148\n",
      "Trained batch 43 batch loss 1.25249231 epoch total loss 1.19899523\n",
      "Trained batch 44 batch loss 1.22725058 epoch total loss 1.19963729\n",
      "Trained batch 45 batch loss 1.4062072 epoch total loss 1.20422781\n",
      "Trained batch 46 batch loss 1.34624398 epoch total loss 1.20731509\n",
      "Trained batch 47 batch loss 1.40503454 epoch total loss 1.21152186\n",
      "Trained batch 48 batch loss 1.33470654 epoch total loss 1.2140882\n",
      "Trained batch 49 batch loss 1.33611083 epoch total loss 1.21657848\n",
      "Trained batch 50 batch loss 1.2998724 epoch total loss 1.21824431\n",
      "Trained batch 51 batch loss 1.26763391 epoch total loss 1.21921277\n",
      "Trained batch 52 batch loss 1.22794223 epoch total loss 1.21938062\n",
      "Trained batch 53 batch loss 1.25026321 epoch total loss 1.21996331\n",
      "Trained batch 54 batch loss 1.24937201 epoch total loss 1.22050798\n",
      "Trained batch 55 batch loss 1.22815311 epoch total loss 1.22064698\n",
      "Trained batch 56 batch loss 1.28177333 epoch total loss 1.22173858\n",
      "Trained batch 57 batch loss 1.15385127 epoch total loss 1.22054756\n",
      "Trained batch 58 batch loss 1.33174539 epoch total loss 1.2224648\n",
      "Trained batch 59 batch loss 1.02556276 epoch total loss 1.21912754\n",
      "Trained batch 60 batch loss 1.11851013 epoch total loss 1.21745062\n",
      "Trained batch 61 batch loss 1.12868834 epoch total loss 1.21599543\n",
      "Trained batch 62 batch loss 1.3150363 epoch total loss 1.21759284\n",
      "Trained batch 63 batch loss 1.37372637 epoch total loss 1.22007108\n",
      "Trained batch 64 batch loss 1.3994236 epoch total loss 1.22287345\n",
      "Trained batch 65 batch loss 1.44055879 epoch total loss 1.22622252\n",
      "Trained batch 66 batch loss 1.24418771 epoch total loss 1.22649467\n",
      "Trained batch 67 batch loss 1.21754551 epoch total loss 1.22636104\n",
      "Trained batch 68 batch loss 1.26394176 epoch total loss 1.22691369\n",
      "Trained batch 69 batch loss 1.29015756 epoch total loss 1.22783029\n",
      "Trained batch 70 batch loss 1.17218828 epoch total loss 1.2270354\n",
      "Trained batch 71 batch loss 1.12457919 epoch total loss 1.22559237\n",
      "Trained batch 72 batch loss 1.21763849 epoch total loss 1.22548187\n",
      "Trained batch 73 batch loss 1.14072585 epoch total loss 1.22432077\n",
      "Trained batch 74 batch loss 1.07812786 epoch total loss 1.22234523\n",
      "Trained batch 75 batch loss 1.03925645 epoch total loss 1.21990395\n",
      "Trained batch 76 batch loss 1.14992964 epoch total loss 1.21898329\n",
      "Trained batch 77 batch loss 1.18050301 epoch total loss 1.21848357\n",
      "Trained batch 78 batch loss 1.15967989 epoch total loss 1.21772969\n",
      "Trained batch 79 batch loss 1.16056514 epoch total loss 1.21700621\n",
      "Trained batch 80 batch loss 1.12690079 epoch total loss 1.2158798\n",
      "Trained batch 81 batch loss 1.16793799 epoch total loss 1.21528792\n",
      "Trained batch 82 batch loss 1.14590633 epoch total loss 1.21444178\n",
      "Trained batch 83 batch loss 1.28129172 epoch total loss 1.21524715\n",
      "Trained batch 84 batch loss 1.30901432 epoch total loss 1.21636343\n",
      "Trained batch 85 batch loss 1.35228 epoch total loss 1.2179625\n",
      "Trained batch 86 batch loss 1.35714817 epoch total loss 1.21958089\n",
      "Trained batch 87 batch loss 1.21235061 epoch total loss 1.2194978\n",
      "Trained batch 88 batch loss 1.20661962 epoch total loss 1.21935141\n",
      "Trained batch 89 batch loss 1.34593689 epoch total loss 1.22077382\n",
      "Trained batch 90 batch loss 1.30781412 epoch total loss 1.22174084\n",
      "Trained batch 91 batch loss 1.29936123 epoch total loss 1.2225939\n",
      "Trained batch 92 batch loss 1.26817465 epoch total loss 1.22308934\n",
      "Trained batch 93 batch loss 1.15437388 epoch total loss 1.22235036\n",
      "Trained batch 94 batch loss 1.1308049 epoch total loss 1.22137654\n",
      "Trained batch 95 batch loss 1.09319472 epoch total loss 1.22002721\n",
      "Trained batch 96 batch loss 1.17809689 epoch total loss 1.21959054\n",
      "Trained batch 97 batch loss 1.24123979 epoch total loss 1.2198137\n",
      "Trained batch 98 batch loss 1.19270921 epoch total loss 1.21953714\n",
      "Trained batch 99 batch loss 1.20202017 epoch total loss 1.21936023\n",
      "Trained batch 100 batch loss 1.13079071 epoch total loss 1.21847451\n",
      "Trained batch 101 batch loss 1.15536082 epoch total loss 1.21784961\n",
      "Trained batch 102 batch loss 1.36309576 epoch total loss 1.21927357\n",
      "Trained batch 103 batch loss 1.21976018 epoch total loss 1.21927834\n",
      "Trained batch 104 batch loss 1.28201818 epoch total loss 1.21988153\n",
      "Trained batch 105 batch loss 1.11048615 epoch total loss 1.21883976\n",
      "Trained batch 106 batch loss 0.942166448 epoch total loss 1.21622956\n",
      "Trained batch 107 batch loss 0.884781599 epoch total loss 1.2131319\n",
      "Trained batch 108 batch loss 1.07206202 epoch total loss 1.21182573\n",
      "Trained batch 109 batch loss 1.05437028 epoch total loss 1.21038115\n",
      "Trained batch 110 batch loss 1.09284186 epoch total loss 1.20931256\n",
      "Trained batch 111 batch loss 1.16299891 epoch total loss 1.20889533\n",
      "Trained batch 112 batch loss 1.19585109 epoch total loss 1.20877874\n",
      "Trained batch 113 batch loss 1.24062717 epoch total loss 1.20906067\n",
      "Trained batch 114 batch loss 1.18519461 epoch total loss 1.20885134\n",
      "Trained batch 115 batch loss 1.34691346 epoch total loss 1.21005177\n",
      "Trained batch 116 batch loss 1.33904433 epoch total loss 1.21116388\n",
      "Trained batch 117 batch loss 1.20070183 epoch total loss 1.21107447\n",
      "Trained batch 118 batch loss 1.34999 epoch total loss 1.21225166\n",
      "Trained batch 119 batch loss 1.21965933 epoch total loss 1.21231401\n",
      "Trained batch 120 batch loss 1.31013799 epoch total loss 1.21312916\n",
      "Trained batch 121 batch loss 1.21779501 epoch total loss 1.21316767\n",
      "Trained batch 122 batch loss 1.309026 epoch total loss 1.21395338\n",
      "Trained batch 123 batch loss 1.35373592 epoch total loss 1.2150898\n",
      "Trained batch 124 batch loss 1.29576385 epoch total loss 1.21574032\n",
      "Trained batch 125 batch loss 1.20389938 epoch total loss 1.21564567\n",
      "Trained batch 126 batch loss 1.29845476 epoch total loss 1.21630287\n",
      "Trained batch 127 batch loss 1.43369579 epoch total loss 1.21801472\n",
      "Trained batch 128 batch loss 1.37913179 epoch total loss 1.21927345\n",
      "Trained batch 129 batch loss 1.21058536 epoch total loss 1.21920609\n",
      "Trained batch 130 batch loss 1.23374712 epoch total loss 1.21931803\n",
      "Trained batch 131 batch loss 1.31601286 epoch total loss 1.22005606\n",
      "Trained batch 132 batch loss 1.2464633 epoch total loss 1.22025609\n",
      "Trained batch 133 batch loss 1.26962066 epoch total loss 1.22062731\n",
      "Trained batch 134 batch loss 1.35972285 epoch total loss 1.22166538\n",
      "Trained batch 135 batch loss 1.25293088 epoch total loss 1.22189689\n",
      "Trained batch 136 batch loss 1.33394837 epoch total loss 1.22272086\n",
      "Trained batch 137 batch loss 1.31206191 epoch total loss 1.22337294\n",
      "Trained batch 138 batch loss 1.22286499 epoch total loss 1.22336936\n",
      "Trained batch 139 batch loss 1.32575893 epoch total loss 1.22410595\n",
      "Trained batch 140 batch loss 1.228127 epoch total loss 1.22413468\n",
      "Trained batch 141 batch loss 1.32220709 epoch total loss 1.22483027\n",
      "Trained batch 142 batch loss 1.24769819 epoch total loss 1.22499132\n",
      "Trained batch 143 batch loss 1.27434778 epoch total loss 1.22533643\n",
      "Trained batch 144 batch loss 1.18722653 epoch total loss 1.22507179\n",
      "Trained batch 145 batch loss 1.140306 epoch total loss 1.22448719\n",
      "Trained batch 146 batch loss 1.06877077 epoch total loss 1.22342062\n",
      "Trained batch 147 batch loss 1.0567528 epoch total loss 1.22228682\n",
      "Trained batch 148 batch loss 1.18474638 epoch total loss 1.22203326\n",
      "Trained batch 149 batch loss 1.22593784 epoch total loss 1.22205937\n",
      "Trained batch 150 batch loss 1.12071896 epoch total loss 1.22138381\n",
      "Trained batch 151 batch loss 1.06014681 epoch total loss 1.22031605\n",
      "Trained batch 152 batch loss 1.11456239 epoch total loss 1.21962023\n",
      "Trained batch 153 batch loss 1.1368587 epoch total loss 1.21907926\n",
      "Trained batch 154 batch loss 1.32014203 epoch total loss 1.21973562\n",
      "Trained batch 155 batch loss 1.26064551 epoch total loss 1.21999955\n",
      "Trained batch 156 batch loss 1.24362993 epoch total loss 1.22015107\n",
      "Trained batch 157 batch loss 1.3180325 epoch total loss 1.22077453\n",
      "Trained batch 158 batch loss 1.21732211 epoch total loss 1.22075272\n",
      "Trained batch 159 batch loss 1.18077326 epoch total loss 1.22050118\n",
      "Trained batch 160 batch loss 1.35130322 epoch total loss 1.22131872\n",
      "Trained batch 161 batch loss 1.26145101 epoch total loss 1.22156799\n",
      "Trained batch 162 batch loss 1.09733033 epoch total loss 1.22080112\n",
      "Trained batch 163 batch loss 0.984917402 epoch total loss 1.21935403\n",
      "Trained batch 164 batch loss 1.17764449 epoch total loss 1.21909964\n",
      "Trained batch 165 batch loss 1.18140781 epoch total loss 1.21887124\n",
      "Trained batch 166 batch loss 1.17945051 epoch total loss 1.21863377\n",
      "Trained batch 167 batch loss 1.23299551 epoch total loss 1.21871972\n",
      "Trained batch 168 batch loss 1.28971982 epoch total loss 1.21914232\n",
      "Trained batch 169 batch loss 1.27854204 epoch total loss 1.21949387\n",
      "Trained batch 170 batch loss 1.30723584 epoch total loss 1.22001\n",
      "Trained batch 171 batch loss 1.29518366 epoch total loss 1.22044957\n",
      "Trained batch 172 batch loss 1.43894553 epoch total loss 1.22172\n",
      "Trained batch 173 batch loss 1.37486911 epoch total loss 1.22260523\n",
      "Trained batch 174 batch loss 1.35287833 epoch total loss 1.22335386\n",
      "Trained batch 175 batch loss 1.24450827 epoch total loss 1.22347474\n",
      "Trained batch 176 batch loss 1.45357394 epoch total loss 1.22478211\n",
      "Trained batch 177 batch loss 1.31226814 epoch total loss 1.22527635\n",
      "Trained batch 178 batch loss 1.23841071 epoch total loss 1.22535014\n",
      "Trained batch 179 batch loss 1.06955504 epoch total loss 1.22447968\n",
      "Trained batch 180 batch loss 1.126647 epoch total loss 1.2239362\n",
      "Trained batch 181 batch loss 1.24833822 epoch total loss 1.22407103\n",
      "Trained batch 182 batch loss 1.28232956 epoch total loss 1.2243911\n",
      "Trained batch 183 batch loss 1.35807097 epoch total loss 1.22512162\n",
      "Trained batch 184 batch loss 1.18887758 epoch total loss 1.22492468\n",
      "Trained batch 185 batch loss 1.28881204 epoch total loss 1.22527\n",
      "Trained batch 186 batch loss 1.16741884 epoch total loss 1.22495902\n",
      "Trained batch 187 batch loss 1.2304436 epoch total loss 1.22498834\n",
      "Trained batch 188 batch loss 1.30292797 epoch total loss 1.22540295\n",
      "Trained batch 189 batch loss 1.20606792 epoch total loss 1.22530067\n",
      "Trained batch 190 batch loss 1.39172089 epoch total loss 1.2261765\n",
      "Trained batch 191 batch loss 1.25846851 epoch total loss 1.22634566\n",
      "Trained batch 192 batch loss 1.15166855 epoch total loss 1.22595668\n",
      "Trained batch 193 batch loss 1.20337152 epoch total loss 1.22583961\n",
      "Trained batch 194 batch loss 1.21689117 epoch total loss 1.22579348\n",
      "Trained batch 195 batch loss 1.20744514 epoch total loss 1.22569942\n",
      "Trained batch 196 batch loss 1.48441291 epoch total loss 1.22701931\n",
      "Trained batch 197 batch loss 1.15632677 epoch total loss 1.22666049\n",
      "Trained batch 198 batch loss 1.17148387 epoch total loss 1.22638178\n",
      "Trained batch 199 batch loss 1.09441113 epoch total loss 1.22571862\n",
      "Trained batch 200 batch loss 1.02391219 epoch total loss 1.22470951\n",
      "Trained batch 201 batch loss 1.16484332 epoch total loss 1.22441173\n",
      "Trained batch 202 batch loss 1.5179224 epoch total loss 1.22586477\n",
      "Trained batch 203 batch loss 1.25704265 epoch total loss 1.22601843\n",
      "Trained batch 204 batch loss 1.24311173 epoch total loss 1.22610223\n",
      "Trained batch 205 batch loss 1.1489799 epoch total loss 1.22572601\n",
      "Trained batch 206 batch loss 1.21748114 epoch total loss 1.22568595\n",
      "Trained batch 207 batch loss 1.201334 epoch total loss 1.22556841\n",
      "Trained batch 208 batch loss 1.07986712 epoch total loss 1.22486794\n",
      "Trained batch 209 batch loss 1.13835359 epoch total loss 1.22445393\n",
      "Trained batch 210 batch loss 1.17655766 epoch total loss 1.22422588\n",
      "Trained batch 211 batch loss 1.28969038 epoch total loss 1.22453618\n",
      "Trained batch 212 batch loss 1.26638675 epoch total loss 1.22473359\n",
      "Trained batch 213 batch loss 1.23808181 epoch total loss 1.22479618\n",
      "Trained batch 214 batch loss 1.20993412 epoch total loss 1.2247268\n",
      "Trained batch 215 batch loss 1.09606647 epoch total loss 1.22412837\n",
      "Trained batch 216 batch loss 1.21415043 epoch total loss 1.22408211\n",
      "Trained batch 217 batch loss 1.25146019 epoch total loss 1.22420824\n",
      "Trained batch 218 batch loss 1.1967361 epoch total loss 1.22408235\n",
      "Trained batch 219 batch loss 1.32911813 epoch total loss 1.22456205\n",
      "Trained batch 220 batch loss 1.37549877 epoch total loss 1.22524798\n",
      "Trained batch 221 batch loss 1.30196667 epoch total loss 1.22559524\n",
      "Trained batch 222 batch loss 1.27099419 epoch total loss 1.22579968\n",
      "Trained batch 223 batch loss 1.34607697 epoch total loss 1.22633898\n",
      "Trained batch 224 batch loss 1.25337243 epoch total loss 1.22645974\n",
      "Trained batch 225 batch loss 1.16209638 epoch total loss 1.22617376\n",
      "Trained batch 226 batch loss 1.22283971 epoch total loss 1.22615898\n",
      "Trained batch 227 batch loss 1.38328743 epoch total loss 1.22685122\n",
      "Trained batch 228 batch loss 1.24563599 epoch total loss 1.22693372\n",
      "Trained batch 229 batch loss 1.26095951 epoch total loss 1.22708225\n",
      "Trained batch 230 batch loss 1.22985876 epoch total loss 1.22709429\n",
      "Trained batch 231 batch loss 1.28234196 epoch total loss 1.22733355\n",
      "Trained batch 232 batch loss 1.22485018 epoch total loss 1.22732282\n",
      "Trained batch 233 batch loss 1.24410951 epoch total loss 1.22739482\n",
      "Trained batch 234 batch loss 1.48625803 epoch total loss 1.2285012\n",
      "Trained batch 235 batch loss 1.32270908 epoch total loss 1.2289021\n",
      "Trained batch 236 batch loss 1.32773018 epoch total loss 1.22932088\n",
      "Trained batch 237 batch loss 1.21906972 epoch total loss 1.22927749\n",
      "Trained batch 238 batch loss 1.04251528 epoch total loss 1.22849286\n",
      "Trained batch 239 batch loss 0.98801887 epoch total loss 1.22748661\n",
      "Trained batch 240 batch loss 1.0399369 epoch total loss 1.22670519\n",
      "Trained batch 241 batch loss 1.27096784 epoch total loss 1.22688878\n",
      "Trained batch 242 batch loss 1.27538419 epoch total loss 1.22708929\n",
      "Trained batch 243 batch loss 1.32692981 epoch total loss 1.22750008\n",
      "Trained batch 244 batch loss 1.26826859 epoch total loss 1.22766721\n",
      "Trained batch 245 batch loss 1.29899931 epoch total loss 1.22795844\n",
      "Trained batch 246 batch loss 1.24860489 epoch total loss 1.22804236\n",
      "Trained batch 247 batch loss 1.11915588 epoch total loss 1.22760153\n",
      "Trained batch 248 batch loss 1.1554029 epoch total loss 1.22731042\n",
      "Trained batch 249 batch loss 1.24681175 epoch total loss 1.22738886\n",
      "Trained batch 250 batch loss 1.20903361 epoch total loss 1.22731543\n",
      "Trained batch 251 batch loss 1.27825069 epoch total loss 1.22751844\n",
      "Trained batch 252 batch loss 1.35339773 epoch total loss 1.22801793\n",
      "Trained batch 253 batch loss 1.29432833 epoch total loss 1.22828007\n",
      "Trained batch 254 batch loss 1.33992743 epoch total loss 1.22871959\n",
      "Trained batch 255 batch loss 1.29506648 epoch total loss 1.22897983\n",
      "Trained batch 256 batch loss 1.36730874 epoch total loss 1.2295202\n",
      "Trained batch 257 batch loss 1.29503441 epoch total loss 1.22977519\n",
      "Trained batch 258 batch loss 1.40424621 epoch total loss 1.23045135\n",
      "Trained batch 259 batch loss 1.30303979 epoch total loss 1.23073161\n",
      "Trained batch 260 batch loss 1.2937094 epoch total loss 1.23097384\n",
      "Trained batch 261 batch loss 1.18746352 epoch total loss 1.23080719\n",
      "Trained batch 262 batch loss 1.25372136 epoch total loss 1.23089457\n",
      "Trained batch 263 batch loss 1.24967742 epoch total loss 1.23096597\n",
      "Trained batch 264 batch loss 1.34233665 epoch total loss 1.23138785\n",
      "Trained batch 265 batch loss 1.41891336 epoch total loss 1.23209548\n",
      "Trained batch 266 batch loss 1.47591352 epoch total loss 1.2330122\n",
      "Trained batch 267 batch loss 1.28444684 epoch total loss 1.23320484\n",
      "Trained batch 268 batch loss 1.30847263 epoch total loss 1.2334857\n",
      "Trained batch 269 batch loss 1.22884929 epoch total loss 1.23346841\n",
      "Trained batch 270 batch loss 1.18617713 epoch total loss 1.23329329\n",
      "Trained batch 271 batch loss 1.22622705 epoch total loss 1.23326719\n",
      "Trained batch 272 batch loss 1.22853088 epoch total loss 1.2332499\n",
      "Trained batch 273 batch loss 1.35004854 epoch total loss 1.23367763\n",
      "Trained batch 274 batch loss 1.42152452 epoch total loss 1.23436332\n",
      "Trained batch 275 batch loss 1.37395215 epoch total loss 1.23487091\n",
      "Trained batch 276 batch loss 1.44041848 epoch total loss 1.23561573\n",
      "Trained batch 277 batch loss 1.44109929 epoch total loss 1.23635757\n",
      "Trained batch 278 batch loss 1.42304862 epoch total loss 1.23702908\n",
      "Trained batch 279 batch loss 1.34699869 epoch total loss 1.23742318\n",
      "Trained batch 280 batch loss 1.37914157 epoch total loss 1.23792934\n",
      "Trained batch 281 batch loss 1.33448958 epoch total loss 1.23827302\n",
      "Trained batch 282 batch loss 1.33993506 epoch total loss 1.23863351\n",
      "Trained batch 283 batch loss 1.30075455 epoch total loss 1.23885298\n",
      "Trained batch 284 batch loss 1.28378141 epoch total loss 1.23901117\n",
      "Trained batch 285 batch loss 1.17685294 epoch total loss 1.23879313\n",
      "Trained batch 286 batch loss 1.11822653 epoch total loss 1.23837149\n",
      "Trained batch 287 batch loss 1.09408677 epoch total loss 1.23786879\n",
      "Trained batch 288 batch loss 1.13968241 epoch total loss 1.23752785\n",
      "Trained batch 289 batch loss 1.20376873 epoch total loss 1.23741102\n",
      "Trained batch 290 batch loss 1.28939104 epoch total loss 1.23759031\n",
      "Trained batch 291 batch loss 1.45267677 epoch total loss 1.23832941\n",
      "Trained batch 292 batch loss 1.45884788 epoch total loss 1.2390846\n",
      "Trained batch 293 batch loss 1.3363831 epoch total loss 1.23941672\n",
      "Trained batch 294 batch loss 1.3407588 epoch total loss 1.23976147\n",
      "Trained batch 295 batch loss 1.1672045 epoch total loss 1.23951554\n",
      "Trained batch 296 batch loss 1.14142478 epoch total loss 1.23918414\n",
      "Trained batch 297 batch loss 1.19088554 epoch total loss 1.23902154\n",
      "Trained batch 298 batch loss 1.1958245 epoch total loss 1.23887658\n",
      "Trained batch 299 batch loss 1.27952898 epoch total loss 1.2390126\n",
      "Trained batch 300 batch loss 1.26553249 epoch total loss 1.23910093\n",
      "Trained batch 301 batch loss 1.26191056 epoch total loss 1.23917675\n",
      "Trained batch 302 batch loss 1.23966718 epoch total loss 1.2391783\n",
      "Trained batch 303 batch loss 1.19691193 epoch total loss 1.23903871\n",
      "Trained batch 304 batch loss 1.36276257 epoch total loss 1.23944569\n",
      "Trained batch 305 batch loss 1.27033877 epoch total loss 1.23954701\n",
      "Trained batch 306 batch loss 1.45230258 epoch total loss 1.24024224\n",
      "Trained batch 307 batch loss 1.1570828 epoch total loss 1.2399714\n",
      "Trained batch 308 batch loss 1.09099746 epoch total loss 1.23948765\n",
      "Trained batch 309 batch loss 1.18227553 epoch total loss 1.23930252\n",
      "Trained batch 310 batch loss 1.19648266 epoch total loss 1.23916435\n",
      "Trained batch 311 batch loss 1.24909472 epoch total loss 1.2391963\n",
      "Trained batch 312 batch loss 1.22971487 epoch total loss 1.2391659\n",
      "Trained batch 313 batch loss 1.22918129 epoch total loss 1.23913395\n",
      "Trained batch 314 batch loss 1.26306105 epoch total loss 1.23921025\n",
      "Trained batch 315 batch loss 1.18093228 epoch total loss 1.23902524\n",
      "Trained batch 316 batch loss 1.15447187 epoch total loss 1.23875761\n",
      "Trained batch 317 batch loss 1.16923606 epoch total loss 1.23853838\n",
      "Trained batch 318 batch loss 1.2002629 epoch total loss 1.23841798\n",
      "Trained batch 319 batch loss 1.18539059 epoch total loss 1.23825181\n",
      "Trained batch 320 batch loss 1.20105994 epoch total loss 1.23813558\n",
      "Trained batch 321 batch loss 1.2099421 epoch total loss 1.23804772\n",
      "Trained batch 322 batch loss 1.28456974 epoch total loss 1.2381922\n",
      "Trained batch 323 batch loss 1.20989299 epoch total loss 1.23810458\n",
      "Trained batch 324 batch loss 1.14271927 epoch total loss 1.23781025\n",
      "Trained batch 325 batch loss 1.05217111 epoch total loss 1.237239\n",
      "Trained batch 326 batch loss 1.1526041 epoch total loss 1.23697948\n",
      "Trained batch 327 batch loss 1.23747444 epoch total loss 1.23698103\n",
      "Trained batch 328 batch loss 1.29850078 epoch total loss 1.23716855\n",
      "Trained batch 329 batch loss 1.36620617 epoch total loss 1.23756075\n",
      "Trained batch 330 batch loss 1.26598501 epoch total loss 1.23764694\n",
      "Trained batch 331 batch loss 1.33177412 epoch total loss 1.23793137\n",
      "Trained batch 332 batch loss 1.22338331 epoch total loss 1.2378875\n",
      "Trained batch 333 batch loss 1.32980859 epoch total loss 1.23816359\n",
      "Trained batch 334 batch loss 1.33220577 epoch total loss 1.23844516\n",
      "Trained batch 335 batch loss 1.29096246 epoch total loss 1.23860192\n",
      "Trained batch 336 batch loss 1.20972681 epoch total loss 1.23851597\n",
      "Trained batch 337 batch loss 1.3210212 epoch total loss 1.23876071\n",
      "Trained batch 338 batch loss 1.30846739 epoch total loss 1.23896694\n",
      "Trained batch 339 batch loss 1.31366968 epoch total loss 1.23918736\n",
      "Trained batch 340 batch loss 1.30805922 epoch total loss 1.2393899\n",
      "Trained batch 341 batch loss 1.10510135 epoch total loss 1.23899603\n",
      "Trained batch 342 batch loss 1.09320652 epoch total loss 1.23856974\n",
      "Trained batch 343 batch loss 1.18706298 epoch total loss 1.23841965\n",
      "Trained batch 344 batch loss 1.15163267 epoch total loss 1.23816729\n",
      "Trained batch 345 batch loss 1.25116611 epoch total loss 1.23820496\n",
      "Trained batch 346 batch loss 1.35246086 epoch total loss 1.23853517\n",
      "Trained batch 347 batch loss 1.2693851 epoch total loss 1.2386241\n",
      "Trained batch 348 batch loss 1.26153564 epoch total loss 1.2386899\n",
      "Trained batch 349 batch loss 1.21650648 epoch total loss 1.23862624\n",
      "Trained batch 350 batch loss 1.19571018 epoch total loss 1.23850369\n",
      "Trained batch 351 batch loss 1.04950511 epoch total loss 1.23796523\n",
      "Trained batch 352 batch loss 1.15960336 epoch total loss 1.23774254\n",
      "Trained batch 353 batch loss 1.13533878 epoch total loss 1.23745251\n",
      "Trained batch 354 batch loss 1.14582396 epoch total loss 1.2371937\n",
      "Trained batch 355 batch loss 1.18007934 epoch total loss 1.23703277\n",
      "Trained batch 356 batch loss 1.12023783 epoch total loss 1.23670471\n",
      "Trained batch 357 batch loss 1.29687774 epoch total loss 1.23687327\n",
      "Trained batch 358 batch loss 1.28371453 epoch total loss 1.23700416\n",
      "Trained batch 359 batch loss 1.36891603 epoch total loss 1.23737156\n",
      "Trained batch 360 batch loss 1.1740917 epoch total loss 1.23719585\n",
      "Trained batch 361 batch loss 1.13710976 epoch total loss 1.23691857\n",
      "Trained batch 362 batch loss 1.1844691 epoch total loss 1.23677373\n",
      "Trained batch 363 batch loss 1.28793871 epoch total loss 1.23691463\n",
      "Trained batch 364 batch loss 1.27658081 epoch total loss 1.23702371\n",
      "Trained batch 365 batch loss 1.40192378 epoch total loss 1.2374754\n",
      "Trained batch 366 batch loss 1.22408843 epoch total loss 1.2374388\n",
      "Trained batch 367 batch loss 1.25345969 epoch total loss 1.23748243\n",
      "Trained batch 368 batch loss 1.13520908 epoch total loss 1.23720455\n",
      "Trained batch 369 batch loss 1.24490726 epoch total loss 1.23722541\n",
      "Trained batch 370 batch loss 1.31447279 epoch total loss 1.23743427\n",
      "Trained batch 371 batch loss 1.14464235 epoch total loss 1.23718417\n",
      "Trained batch 372 batch loss 1.2374562 epoch total loss 1.23718488\n",
      "Trained batch 373 batch loss 1.15508258 epoch total loss 1.23696482\n",
      "Trained batch 374 batch loss 1.2421689 epoch total loss 1.23697865\n",
      "Trained batch 375 batch loss 1.22965884 epoch total loss 1.2369591\n",
      "Trained batch 376 batch loss 1.1300894 epoch total loss 1.2366749\n",
      "Trained batch 377 batch loss 1.21414626 epoch total loss 1.23661518\n",
      "Trained batch 378 batch loss 1.39524817 epoch total loss 1.2370348\n",
      "Trained batch 379 batch loss 1.36979437 epoch total loss 1.23738503\n",
      "Trained batch 380 batch loss 1.31636059 epoch total loss 1.23759294\n",
      "Trained batch 381 batch loss 1.39491582 epoch total loss 1.23800588\n",
      "Trained batch 382 batch loss 1.29387057 epoch total loss 1.23815215\n",
      "Trained batch 383 batch loss 1.27384949 epoch total loss 1.23824537\n",
      "Trained batch 384 batch loss 1.29931128 epoch total loss 1.23840439\n",
      "Trained batch 385 batch loss 1.20318568 epoch total loss 1.23831296\n",
      "Trained batch 386 batch loss 1.2455709 epoch total loss 1.23833179\n",
      "Trained batch 387 batch loss 1.31490624 epoch total loss 1.23852968\n",
      "Trained batch 388 batch loss 1.10873032 epoch total loss 1.23819518\n",
      "Trained batch 389 batch loss 1.06324232 epoch total loss 1.2377454\n",
      "Trained batch 390 batch loss 1.16617608 epoch total loss 1.23756182\n",
      "Trained batch 391 batch loss 1.20722449 epoch total loss 1.23748422\n",
      "Trained batch 392 batch loss 1.19185877 epoch total loss 1.23736787\n",
      "Trained batch 393 batch loss 1.36238432 epoch total loss 1.23768592\n",
      "Trained batch 394 batch loss 1.42769575 epoch total loss 1.23816824\n",
      "Trained batch 395 batch loss 1.52054143 epoch total loss 1.23888314\n",
      "Trained batch 396 batch loss 1.32991445 epoch total loss 1.23911297\n",
      "Trained batch 397 batch loss 1.34830499 epoch total loss 1.23938799\n",
      "Trained batch 398 batch loss 1.24138296 epoch total loss 1.23939312\n",
      "Trained batch 399 batch loss 1.16675365 epoch total loss 1.23921096\n",
      "Trained batch 400 batch loss 1.09182036 epoch total loss 1.23884249\n",
      "Trained batch 401 batch loss 1.17634654 epoch total loss 1.23868668\n",
      "Trained batch 402 batch loss 1.29113221 epoch total loss 1.23881721\n",
      "Trained batch 403 batch loss 1.24919772 epoch total loss 1.23884296\n",
      "Trained batch 404 batch loss 1.24870455 epoch total loss 1.2388674\n",
      "Trained batch 405 batch loss 1.11651123 epoch total loss 1.23856533\n",
      "Trained batch 406 batch loss 1.14432263 epoch total loss 1.23833323\n",
      "Trained batch 407 batch loss 1.05329978 epoch total loss 1.23787856\n",
      "Trained batch 408 batch loss 1.097821 epoch total loss 1.23753524\n",
      "Trained batch 409 batch loss 1.25982523 epoch total loss 1.23758984\n",
      "Trained batch 410 batch loss 1.0917393 epoch total loss 1.237234\n",
      "Trained batch 411 batch loss 1.19155645 epoch total loss 1.23712289\n",
      "Trained batch 412 batch loss 1.14609051 epoch total loss 1.236902\n",
      "Trained batch 413 batch loss 1.34036136 epoch total loss 1.23715246\n",
      "Trained batch 414 batch loss 1.16521049 epoch total loss 1.23697877\n",
      "Trained batch 415 batch loss 1.12581921 epoch total loss 1.23671079\n",
      "Trained batch 416 batch loss 1.27067339 epoch total loss 1.23679245\n",
      "Trained batch 417 batch loss 1.24165094 epoch total loss 1.23680413\n",
      "Trained batch 418 batch loss 1.23900461 epoch total loss 1.23680937\n",
      "Trained batch 419 batch loss 1.27690172 epoch total loss 1.2369051\n",
      "Trained batch 420 batch loss 1.36021483 epoch total loss 1.23719871\n",
      "Trained batch 421 batch loss 1.21237159 epoch total loss 1.2371397\n",
      "Trained batch 422 batch loss 1.09722972 epoch total loss 1.23680818\n",
      "Trained batch 423 batch loss 1.03736782 epoch total loss 1.23633659\n",
      "Trained batch 424 batch loss 1.13380778 epoch total loss 1.23609483\n",
      "Trained batch 425 batch loss 1.06738186 epoch total loss 1.23569787\n",
      "Trained batch 426 batch loss 0.981684446 epoch total loss 1.23510158\n",
      "Trained batch 427 batch loss 1.09559178 epoch total loss 1.23477483\n",
      "Trained batch 428 batch loss 1.15287197 epoch total loss 1.2345835\n",
      "Trained batch 429 batch loss 1.14374232 epoch total loss 1.23437178\n",
      "Trained batch 430 batch loss 1.17634141 epoch total loss 1.23423672\n",
      "Trained batch 431 batch loss 1.28409123 epoch total loss 1.23435247\n",
      "Trained batch 432 batch loss 1.25139856 epoch total loss 1.23439193\n",
      "Trained batch 433 batch loss 1.24080718 epoch total loss 1.23440671\n",
      "Trained batch 434 batch loss 1.09433305 epoch total loss 1.23408401\n",
      "Trained batch 435 batch loss 0.959834754 epoch total loss 1.23345363\n",
      "Trained batch 436 batch loss 1.18460035 epoch total loss 1.23334146\n",
      "Trained batch 437 batch loss 1.3756299 epoch total loss 1.23366702\n",
      "Trained batch 438 batch loss 1.31230474 epoch total loss 1.23384655\n",
      "Trained batch 439 batch loss 1.35146904 epoch total loss 1.23411441\n",
      "Trained batch 440 batch loss 1.30134296 epoch total loss 1.23426723\n",
      "Trained batch 441 batch loss 1.42331731 epoch total loss 1.23469591\n",
      "Trained batch 442 batch loss 1.26422048 epoch total loss 1.23476279\n",
      "Trained batch 443 batch loss 1.16134143 epoch total loss 1.23459697\n",
      "Trained batch 444 batch loss 1.4048866 epoch total loss 1.23498058\n",
      "Trained batch 445 batch loss 1.35167217 epoch total loss 1.23524284\n",
      "Trained batch 446 batch loss 1.26278472 epoch total loss 1.23530447\n",
      "Trained batch 447 batch loss 1.17899191 epoch total loss 1.23517859\n",
      "Trained batch 448 batch loss 1.07323456 epoch total loss 1.23481715\n",
      "Trained batch 449 batch loss 1.03984797 epoch total loss 1.23438287\n",
      "Trained batch 450 batch loss 1.20813 epoch total loss 1.23432457\n",
      "Trained batch 451 batch loss 1.20672882 epoch total loss 1.2342633\n",
      "Trained batch 452 batch loss 1.26276875 epoch total loss 1.23432636\n",
      "Trained batch 453 batch loss 1.38079703 epoch total loss 1.23464978\n",
      "Trained batch 454 batch loss 1.31703591 epoch total loss 1.23483109\n",
      "Trained batch 455 batch loss 1.27077866 epoch total loss 1.23491013\n",
      "Trained batch 456 batch loss 1.15611601 epoch total loss 1.23473728\n",
      "Trained batch 457 batch loss 1.20981324 epoch total loss 1.2346828\n",
      "Trained batch 458 batch loss 1.12070799 epoch total loss 1.23443401\n",
      "Trained batch 459 batch loss 1.15729475 epoch total loss 1.23426592\n",
      "Trained batch 460 batch loss 1.13126707 epoch total loss 1.23404205\n",
      "Trained batch 461 batch loss 1.05920863 epoch total loss 1.23366284\n",
      "Trained batch 462 batch loss 1.06601834 epoch total loss 1.2333\n",
      "Trained batch 463 batch loss 1.09805787 epoch total loss 1.23300803\n",
      "Trained batch 464 batch loss 1.18839645 epoch total loss 1.23291183\n",
      "Trained batch 465 batch loss 1.09568894 epoch total loss 1.23261678\n",
      "Trained batch 466 batch loss 1.16912222 epoch total loss 1.23248053\n",
      "Trained batch 467 batch loss 1.22353625 epoch total loss 1.23246133\n",
      "Trained batch 468 batch loss 1.13962603 epoch total loss 1.23226297\n",
      "Trained batch 469 batch loss 1.31279528 epoch total loss 1.23243475\n",
      "Trained batch 470 batch loss 1.37019444 epoch total loss 1.23272789\n",
      "Trained batch 471 batch loss 1.25210309 epoch total loss 1.23276889\n",
      "Trained batch 472 batch loss 1.29483676 epoch total loss 1.2329005\n",
      "Trained batch 473 batch loss 1.53339994 epoch total loss 1.23353577\n",
      "Trained batch 474 batch loss 1.31618488 epoch total loss 1.23371\n",
      "Trained batch 475 batch loss 1.26016057 epoch total loss 1.23376572\n",
      "Trained batch 476 batch loss 1.159482 epoch total loss 1.23360968\n",
      "Trained batch 477 batch loss 1.21611166 epoch total loss 1.23357296\n",
      "Trained batch 478 batch loss 1.37600362 epoch total loss 1.23387086\n",
      "Trained batch 479 batch loss 1.20080733 epoch total loss 1.23380184\n",
      "Trained batch 480 batch loss 1.1958015 epoch total loss 1.23372269\n",
      "Trained batch 481 batch loss 1.29075885 epoch total loss 1.2338413\n",
      "Trained batch 482 batch loss 1.26981258 epoch total loss 1.23391593\n",
      "Trained batch 483 batch loss 1.287112 epoch total loss 1.23402607\n",
      "Trained batch 484 batch loss 1.21479797 epoch total loss 1.23398638\n",
      "Trained batch 485 batch loss 1.27690458 epoch total loss 1.23407483\n",
      "Trained batch 486 batch loss 1.41072154 epoch total loss 1.2344383\n",
      "Trained batch 487 batch loss 1.38908219 epoch total loss 1.23475587\n",
      "Trained batch 488 batch loss 1.33978832 epoch total loss 1.23497105\n",
      "Trained batch 489 batch loss 1.30871046 epoch total loss 1.23512185\n",
      "Trained batch 490 batch loss 1.33537936 epoch total loss 1.23532653\n",
      "Trained batch 491 batch loss 1.18122888 epoch total loss 1.23521626\n",
      "Trained batch 492 batch loss 1.22913706 epoch total loss 1.23520398\n",
      "Trained batch 493 batch loss 1.23350358 epoch total loss 1.23520052\n",
      "Trained batch 494 batch loss 1.21222448 epoch total loss 1.23515403\n",
      "Trained batch 495 batch loss 1.31002283 epoch total loss 1.23530519\n",
      "Trained batch 496 batch loss 1.20701337 epoch total loss 1.23524821\n",
      "Trained batch 497 batch loss 1.25791812 epoch total loss 1.23529387\n",
      "Trained batch 498 batch loss 1.24879324 epoch total loss 1.23532093\n",
      "Trained batch 499 batch loss 1.29629397 epoch total loss 1.235443\n",
      "Trained batch 500 batch loss 1.25951052 epoch total loss 1.23549116\n",
      "Trained batch 501 batch loss 1.34125233 epoch total loss 1.23570228\n",
      "Trained batch 502 batch loss 1.2653836 epoch total loss 1.2357614\n",
      "Trained batch 503 batch loss 1.28794646 epoch total loss 1.23586524\n",
      "Trained batch 504 batch loss 1.41253614 epoch total loss 1.23621571\n",
      "Trained batch 505 batch loss 1.35406291 epoch total loss 1.23644912\n",
      "Trained batch 506 batch loss 1.21645832 epoch total loss 1.23640954\n",
      "Trained batch 507 batch loss 1.39494979 epoch total loss 1.23672223\n",
      "Trained batch 508 batch loss 1.29567397 epoch total loss 1.23683822\n",
      "Trained batch 509 batch loss 1.29326725 epoch total loss 1.23694921\n",
      "Trained batch 510 batch loss 1.26265705 epoch total loss 1.23699951\n",
      "Trained batch 511 batch loss 1.1917088 epoch total loss 1.23691094\n",
      "Trained batch 512 batch loss 1.152529 epoch total loss 1.23674607\n",
      "Trained batch 513 batch loss 1.28215313 epoch total loss 1.23683465\n",
      "Trained batch 514 batch loss 1.2194829 epoch total loss 1.23680091\n",
      "Trained batch 515 batch loss 1.27920032 epoch total loss 1.23688316\n",
      "Trained batch 516 batch loss 1.27703536 epoch total loss 1.23696101\n",
      "Trained batch 517 batch loss 1.05795383 epoch total loss 1.23661482\n",
      "Trained batch 518 batch loss 1.22000742 epoch total loss 1.23658276\n",
      "Trained batch 519 batch loss 1.23163581 epoch total loss 1.23657322\n",
      "Trained batch 520 batch loss 1.23163652 epoch total loss 1.23656368\n",
      "Trained batch 521 batch loss 1.37396514 epoch total loss 1.23682737\n",
      "Trained batch 522 batch loss 1.25416458 epoch total loss 1.23686063\n",
      "Trained batch 523 batch loss 1.26154244 epoch total loss 1.23690784\n",
      "Trained batch 524 batch loss 1.23747516 epoch total loss 1.23690891\n",
      "Trained batch 525 batch loss 1.26384068 epoch total loss 1.23696017\n",
      "Trained batch 526 batch loss 1.17793107 epoch total loss 1.236848\n",
      "Trained batch 527 batch loss 1.27706122 epoch total loss 1.23692429\n",
      "Trained batch 528 batch loss 1.09522688 epoch total loss 1.23665583\n",
      "Trained batch 529 batch loss 1.05381441 epoch total loss 1.23631024\n",
      "Trained batch 530 batch loss 1.05827761 epoch total loss 1.23597431\n",
      "Trained batch 531 batch loss 1.16912079 epoch total loss 1.23584843\n",
      "Trained batch 532 batch loss 1.1207881 epoch total loss 1.23563218\n",
      "Trained batch 533 batch loss 1.01861441 epoch total loss 1.23522508\n",
      "Trained batch 534 batch loss 1.15793467 epoch total loss 1.23508036\n",
      "Trained batch 535 batch loss 1.21768618 epoch total loss 1.23504782\n",
      "Trained batch 536 batch loss 1.24899 epoch total loss 1.2350738\n",
      "Trained batch 537 batch loss 1.12523115 epoch total loss 1.23486936\n",
      "Trained batch 538 batch loss 1.13831043 epoch total loss 1.23468983\n",
      "Trained batch 539 batch loss 1.25550556 epoch total loss 1.23472846\n",
      "Trained batch 540 batch loss 1.32171583 epoch total loss 1.23488951\n",
      "Trained batch 541 batch loss 1.24726725 epoch total loss 1.2349124\n",
      "Trained batch 542 batch loss 1.19347858 epoch total loss 1.23483586\n",
      "Trained batch 543 batch loss 1.24954343 epoch total loss 1.23486304\n",
      "Trained batch 544 batch loss 1.21275449 epoch total loss 1.23482239\n",
      "Trained batch 545 batch loss 1.06191814 epoch total loss 1.23450518\n",
      "Trained batch 546 batch loss 1.22563875 epoch total loss 1.23448896\n",
      "Trained batch 547 batch loss 1.18679309 epoch total loss 1.2344017\n",
      "Trained batch 548 batch loss 1.29768777 epoch total loss 1.2345171\n",
      "Trained batch 549 batch loss 1.29924285 epoch total loss 1.234635\n",
      "Trained batch 550 batch loss 1.24433255 epoch total loss 1.23465264\n",
      "Trained batch 551 batch loss 1.19025683 epoch total loss 1.23457205\n",
      "Trained batch 552 batch loss 1.30632007 epoch total loss 1.23470211\n",
      "Trained batch 553 batch loss 1.18319845 epoch total loss 1.23460901\n",
      "Trained batch 554 batch loss 1.226946 epoch total loss 1.23459518\n",
      "Trained batch 555 batch loss 1.11406112 epoch total loss 1.23437798\n",
      "Trained batch 556 batch loss 1.17000318 epoch total loss 1.23426211\n",
      "Trained batch 557 batch loss 1.17709184 epoch total loss 1.23415947\n",
      "Trained batch 558 batch loss 1.20336819 epoch total loss 1.23410428\n",
      "Trained batch 559 batch loss 1.19587171 epoch total loss 1.23403585\n",
      "Trained batch 560 batch loss 1.12944436 epoch total loss 1.23384917\n",
      "Trained batch 561 batch loss 1.10652697 epoch total loss 1.23362207\n",
      "Trained batch 562 batch loss 1.09675658 epoch total loss 1.23337853\n",
      "Trained batch 563 batch loss 1.15941834 epoch total loss 1.23324716\n",
      "Trained batch 564 batch loss 1.17360592 epoch total loss 1.23314142\n",
      "Trained batch 565 batch loss 1.23376238 epoch total loss 1.2331425\n",
      "Trained batch 566 batch loss 1.2450037 epoch total loss 1.23316348\n",
      "Trained batch 567 batch loss 1.28839731 epoch total loss 1.23326087\n",
      "Trained batch 568 batch loss 1.28715205 epoch total loss 1.23335576\n",
      "Trained batch 569 batch loss 1.21052516 epoch total loss 1.23331559\n",
      "Trained batch 570 batch loss 1.3333497 epoch total loss 1.23349118\n",
      "Trained batch 571 batch loss 1.24616539 epoch total loss 1.23351336\n",
      "Trained batch 572 batch loss 1.23003244 epoch total loss 1.23350728\n",
      "Trained batch 573 batch loss 1.09177566 epoch total loss 1.23325992\n",
      "Trained batch 574 batch loss 1.23262155 epoch total loss 1.23325884\n",
      "Trained batch 575 batch loss 1.3346417 epoch total loss 1.23343515\n",
      "Trained batch 576 batch loss 1.33406699 epoch total loss 1.2336098\n",
      "Trained batch 577 batch loss 1.28606176 epoch total loss 1.23370075\n",
      "Trained batch 578 batch loss 1.24902976 epoch total loss 1.23372722\n",
      "Trained batch 579 batch loss 1.19466543 epoch total loss 1.23365974\n",
      "Trained batch 580 batch loss 1.33804834 epoch total loss 1.23383975\n",
      "Trained batch 581 batch loss 1.05120444 epoch total loss 1.2335254\n",
      "Trained batch 582 batch loss 1.15444648 epoch total loss 1.2333895\n",
      "Trained batch 583 batch loss 1.2894268 epoch total loss 1.2334857\n",
      "Trained batch 584 batch loss 1.36540616 epoch total loss 1.2337116\n",
      "Trained batch 585 batch loss 1.26288533 epoch total loss 1.23376143\n",
      "Trained batch 586 batch loss 1.1681813 epoch total loss 1.23364949\n",
      "Trained batch 587 batch loss 1.09422374 epoch total loss 1.23341191\n",
      "Trained batch 588 batch loss 1.14045548 epoch total loss 1.23325384\n",
      "Trained batch 589 batch loss 1.12047029 epoch total loss 1.23306239\n",
      "Trained batch 590 batch loss 1.1892184 epoch total loss 1.232988\n",
      "Trained batch 591 batch loss 1.25107229 epoch total loss 1.23301876\n",
      "Trained batch 592 batch loss 1.25840056 epoch total loss 1.23306155\n",
      "Trained batch 593 batch loss 1.19979858 epoch total loss 1.23300552\n",
      "Trained batch 594 batch loss 1.33896899 epoch total loss 1.23318386\n",
      "Trained batch 595 batch loss 1.18264508 epoch total loss 1.23309886\n",
      "Trained batch 596 batch loss 1.41499054 epoch total loss 1.23340404\n",
      "Trained batch 597 batch loss 1.24068272 epoch total loss 1.2334162\n",
      "Trained batch 598 batch loss 1.17849267 epoch total loss 1.23332429\n",
      "Trained batch 599 batch loss 1.10502219 epoch total loss 1.23311019\n",
      "Trained batch 600 batch loss 1.19851351 epoch total loss 1.23305249\n",
      "Trained batch 601 batch loss 1.14702487 epoch total loss 1.23290932\n",
      "Trained batch 602 batch loss 1.38523817 epoch total loss 1.2331624\n",
      "Trained batch 603 batch loss 1.35557604 epoch total loss 1.23336542\n",
      "Trained batch 604 batch loss 1.36451077 epoch total loss 1.2335825\n",
      "Trained batch 605 batch loss 1.12145901 epoch total loss 1.23339725\n",
      "Trained batch 606 batch loss 1.15353858 epoch total loss 1.23326552\n",
      "Trained batch 607 batch loss 1.32867694 epoch total loss 1.23342264\n",
      "Trained batch 608 batch loss 1.30030298 epoch total loss 1.23353267\n",
      "Trained batch 609 batch loss 1.32383025 epoch total loss 1.23368096\n",
      "Trained batch 610 batch loss 1.12507749 epoch total loss 1.23350286\n",
      "Trained batch 611 batch loss 1.15895128 epoch total loss 1.23338091\n",
      "Trained batch 612 batch loss 1.13474536 epoch total loss 1.23321974\n",
      "Trained batch 613 batch loss 1.16446769 epoch total loss 1.23310757\n",
      "Trained batch 614 batch loss 1.17811704 epoch total loss 1.23301804\n",
      "Trained batch 615 batch loss 1.12028909 epoch total loss 1.2328347\n",
      "Trained batch 616 batch loss 1.28115427 epoch total loss 1.23291314\n",
      "Trained batch 617 batch loss 1.24281895 epoch total loss 1.23292911\n",
      "Trained batch 618 batch loss 1.23767424 epoch total loss 1.23293686\n",
      "Trained batch 619 batch loss 1.2849071 epoch total loss 1.23302078\n",
      "Trained batch 620 batch loss 1.23371124 epoch total loss 1.23302186\n",
      "Trained batch 621 batch loss 1.439255 epoch total loss 1.23335397\n",
      "Trained batch 622 batch loss 1.50543594 epoch total loss 1.23379147\n",
      "Trained batch 623 batch loss 1.4786551 epoch total loss 1.2341845\n",
      "Trained batch 624 batch loss 1.15157163 epoch total loss 1.23405206\n",
      "Trained batch 625 batch loss 1.13934338 epoch total loss 1.23390055\n",
      "Trained batch 626 batch loss 1.31159163 epoch total loss 1.23402452\n",
      "Trained batch 627 batch loss 1.43787 epoch total loss 1.23434973\n",
      "Trained batch 628 batch loss 1.3700937 epoch total loss 1.23456585\n",
      "Trained batch 629 batch loss 1.31300795 epoch total loss 1.23469055\n",
      "Trained batch 630 batch loss 1.28298283 epoch total loss 1.2347672\n",
      "Trained batch 631 batch loss 1.33574796 epoch total loss 1.23492718\n",
      "Trained batch 632 batch loss 1.18231738 epoch total loss 1.23484397\n",
      "Trained batch 633 batch loss 1.09518278 epoch total loss 1.23462331\n",
      "Trained batch 634 batch loss 1.14314401 epoch total loss 1.23447895\n",
      "Trained batch 635 batch loss 1.2132982 epoch total loss 1.23444569\n",
      "Trained batch 636 batch loss 1.24204195 epoch total loss 1.23445761\n",
      "Trained batch 637 batch loss 1.20069349 epoch total loss 1.23440456\n",
      "Trained batch 638 batch loss 1.30837321 epoch total loss 1.23452055\n",
      "Trained batch 639 batch loss 1.14472318 epoch total loss 1.23438\n",
      "Trained batch 640 batch loss 1.25932646 epoch total loss 1.23441899\n",
      "Trained batch 641 batch loss 1.19103503 epoch total loss 1.23435128\n",
      "Trained batch 642 batch loss 1.33475721 epoch total loss 1.23450768\n",
      "Trained batch 643 batch loss 1.27271938 epoch total loss 1.23456717\n",
      "Trained batch 644 batch loss 1.26459241 epoch total loss 1.23461378\n",
      "Trained batch 645 batch loss 1.13573325 epoch total loss 1.23446047\n",
      "Trained batch 646 batch loss 1.13234568 epoch total loss 1.2343024\n",
      "Trained batch 647 batch loss 1.10284948 epoch total loss 1.23409915\n",
      "Trained batch 648 batch loss 1.26333606 epoch total loss 1.23414421\n",
      "Trained batch 649 batch loss 1.22746611 epoch total loss 1.23413396\n",
      "Trained batch 650 batch loss 1.23854661 epoch total loss 1.23414075\n",
      "Trained batch 651 batch loss 1.17045784 epoch total loss 1.23404288\n",
      "Trained batch 652 batch loss 1.24084747 epoch total loss 1.23405337\n",
      "Trained batch 653 batch loss 1.10835886 epoch total loss 1.23386085\n",
      "Trained batch 654 batch loss 1.20315754 epoch total loss 1.23381388\n",
      "Trained batch 655 batch loss 1.15587342 epoch total loss 1.23369491\n",
      "Trained batch 656 batch loss 1.10997438 epoch total loss 1.23350632\n",
      "Trained batch 657 batch loss 1.10971165 epoch total loss 1.23331797\n",
      "Trained batch 658 batch loss 1.0109396 epoch total loss 1.23298\n",
      "Trained batch 659 batch loss 1.14243293 epoch total loss 1.23284268\n",
      "Trained batch 660 batch loss 1.127249 epoch total loss 1.2326827\n",
      "Trained batch 661 batch loss 1.23829556 epoch total loss 1.23269117\n",
      "Trained batch 662 batch loss 1.05867648 epoch total loss 1.23242819\n",
      "Trained batch 663 batch loss 1.15137148 epoch total loss 1.232306\n",
      "Trained batch 664 batch loss 1.17053115 epoch total loss 1.2322129\n",
      "Trained batch 665 batch loss 1.27661145 epoch total loss 1.23227966\n",
      "Trained batch 666 batch loss 1.28025031 epoch total loss 1.23235178\n",
      "Trained batch 667 batch loss 1.36010718 epoch total loss 1.23254335\n",
      "Trained batch 668 batch loss 1.22218335 epoch total loss 1.23252773\n",
      "Trained batch 669 batch loss 1.22524929 epoch total loss 1.23251688\n",
      "Trained batch 670 batch loss 1.21384776 epoch total loss 1.23248899\n",
      "Trained batch 671 batch loss 1.15757084 epoch total loss 1.23237741\n",
      "Trained batch 672 batch loss 1.10168791 epoch total loss 1.23218286\n",
      "Trained batch 673 batch loss 1.09526408 epoch total loss 1.23197949\n",
      "Trained batch 674 batch loss 1.04684842 epoch total loss 1.23170483\n",
      "Trained batch 675 batch loss 1.18641889 epoch total loss 1.23163772\n",
      "Trained batch 676 batch loss 1.32575428 epoch total loss 1.23177695\n",
      "Trained batch 677 batch loss 1.09943128 epoch total loss 1.23158145\n",
      "Trained batch 678 batch loss 1.13151872 epoch total loss 1.23143387\n",
      "Trained batch 679 batch loss 1.27716303 epoch total loss 1.23150122\n",
      "Trained batch 680 batch loss 1.17715359 epoch total loss 1.23142123\n",
      "Trained batch 681 batch loss 1.24048567 epoch total loss 1.23143458\n",
      "Trained batch 682 batch loss 1.20909894 epoch total loss 1.2314018\n",
      "Trained batch 683 batch loss 1.12917566 epoch total loss 1.23125207\n",
      "Trained batch 684 batch loss 1.09652352 epoch total loss 1.23105514\n",
      "Trained batch 685 batch loss 1.15425217 epoch total loss 1.23094296\n",
      "Trained batch 686 batch loss 1.20353067 epoch total loss 1.23090303\n",
      "Trained batch 687 batch loss 1.27871799 epoch total loss 1.23097265\n",
      "Trained batch 688 batch loss 1.25333381 epoch total loss 1.23100519\n",
      "Trained batch 689 batch loss 1.22432828 epoch total loss 1.23099554\n",
      "Trained batch 690 batch loss 1.05817091 epoch total loss 1.23074496\n",
      "Trained batch 691 batch loss 1.01528072 epoch total loss 1.23043311\n",
      "Trained batch 692 batch loss 0.992273688 epoch total loss 1.23008895\n",
      "Trained batch 693 batch loss 1.07129073 epoch total loss 1.22985983\n",
      "Trained batch 694 batch loss 1.06673193 epoch total loss 1.22962475\n",
      "Trained batch 695 batch loss 1.17720497 epoch total loss 1.22954929\n",
      "Trained batch 696 batch loss 1.11393809 epoch total loss 1.22938323\n",
      "Trained batch 697 batch loss 1.19912386 epoch total loss 1.22933972\n",
      "Trained batch 698 batch loss 1.17209041 epoch total loss 1.2292577\n",
      "Trained batch 699 batch loss 1.30497038 epoch total loss 1.22936606\n",
      "Trained batch 700 batch loss 1.29374909 epoch total loss 1.22945809\n",
      "Trained batch 701 batch loss 1.44271791 epoch total loss 1.22976232\n",
      "Trained batch 702 batch loss 1.3911283 epoch total loss 1.22999215\n",
      "Trained batch 703 batch loss 1.4182483 epoch total loss 1.2302599\n",
      "Trained batch 704 batch loss 1.26007724 epoch total loss 1.23030233\n",
      "Trained batch 705 batch loss 1.35708666 epoch total loss 1.23048222\n",
      "Trained batch 706 batch loss 1.21193469 epoch total loss 1.23045588\n",
      "Trained batch 707 batch loss 1.28832364 epoch total loss 1.23053777\n",
      "Trained batch 708 batch loss 1.2290113 epoch total loss 1.23053551\n",
      "Trained batch 709 batch loss 1.13252568 epoch total loss 1.23039734\n",
      "Trained batch 710 batch loss 1.29285192 epoch total loss 1.2304852\n",
      "Trained batch 711 batch loss 1.30761647 epoch total loss 1.23059368\n",
      "Trained batch 712 batch loss 1.26343167 epoch total loss 1.23063982\n",
      "Trained batch 713 batch loss 1.19987595 epoch total loss 1.23059678\n",
      "Trained batch 714 batch loss 1.12398398 epoch total loss 1.23044741\n",
      "Trained batch 715 batch loss 1.26461661 epoch total loss 1.2304951\n",
      "Trained batch 716 batch loss 1.15685296 epoch total loss 1.23039234\n",
      "Trained batch 717 batch loss 1.18638659 epoch total loss 1.23033094\n",
      "Trained batch 718 batch loss 1.0842346 epoch total loss 1.23012745\n",
      "Trained batch 719 batch loss 1.19623351 epoch total loss 1.23008025\n",
      "Trained batch 720 batch loss 1.25810707 epoch total loss 1.23011923\n",
      "Trained batch 721 batch loss 1.15895271 epoch total loss 1.23002052\n",
      "Trained batch 722 batch loss 1.18735862 epoch total loss 1.2299614\n",
      "Trained batch 723 batch loss 1.11764455 epoch total loss 1.22980607\n",
      "Trained batch 724 batch loss 1.2189939 epoch total loss 1.22979116\n",
      "Trained batch 725 batch loss 1.22352242 epoch total loss 1.22978246\n",
      "Trained batch 726 batch loss 1.2520256 epoch total loss 1.2298131\n",
      "Trained batch 727 batch loss 1.18309546 epoch total loss 1.22974885\n",
      "Trained batch 728 batch loss 1.20380116 epoch total loss 1.2297132\n",
      "Trained batch 729 batch loss 1.19170582 epoch total loss 1.22966111\n",
      "Trained batch 730 batch loss 1.22326171 epoch total loss 1.22965229\n",
      "Trained batch 731 batch loss 1.21310246 epoch total loss 1.22962964\n",
      "Trained batch 732 batch loss 1.24211156 epoch total loss 1.22964668\n",
      "Trained batch 733 batch loss 1.23136628 epoch total loss 1.22964907\n",
      "Trained batch 734 batch loss 1.08349347 epoch total loss 1.22945\n",
      "Trained batch 735 batch loss 1.06844985 epoch total loss 1.22923088\n",
      "Trained batch 736 batch loss 1.24582911 epoch total loss 1.22925341\n",
      "Trained batch 737 batch loss 1.14904773 epoch total loss 1.22914457\n",
      "Trained batch 738 batch loss 1.20028472 epoch total loss 1.22910547\n",
      "Trained batch 739 batch loss 1.1692996 epoch total loss 1.22902453\n",
      "Trained batch 740 batch loss 1.21188617 epoch total loss 1.2290014\n",
      "Trained batch 741 batch loss 1.23158622 epoch total loss 1.22900486\n",
      "Trained batch 742 batch loss 1.3251338 epoch total loss 1.22913444\n",
      "Trained batch 743 batch loss 1.2811898 epoch total loss 1.22920454\n",
      "Trained batch 744 batch loss 1.16553211 epoch total loss 1.22911894\n",
      "Trained batch 745 batch loss 1.2120173 epoch total loss 1.22909594\n",
      "Trained batch 746 batch loss 1.13799727 epoch total loss 1.22897387\n",
      "Trained batch 747 batch loss 1.13777149 epoch total loss 1.2288518\n",
      "Trained batch 748 batch loss 1.21278882 epoch total loss 1.22883022\n",
      "Trained batch 749 batch loss 1.32408237 epoch total loss 1.22895741\n",
      "Trained batch 750 batch loss 1.29746616 epoch total loss 1.22904885\n",
      "Trained batch 751 batch loss 1.17626989 epoch total loss 1.22897851\n",
      "Trained batch 752 batch loss 1.28740656 epoch total loss 1.22905624\n",
      "Trained batch 753 batch loss 1.21067309 epoch total loss 1.22903192\n",
      "Trained batch 754 batch loss 1.23027813 epoch total loss 1.22903359\n",
      "Trained batch 755 batch loss 1.21786976 epoch total loss 1.22901881\n",
      "Trained batch 756 batch loss 1.17187345 epoch total loss 1.22894323\n",
      "Trained batch 757 batch loss 1.1192 epoch total loss 1.22879827\n",
      "Trained batch 758 batch loss 1.1983484 epoch total loss 1.2287581\n",
      "Trained batch 759 batch loss 1.23194551 epoch total loss 1.22876227\n",
      "Trained batch 760 batch loss 1.24323976 epoch total loss 1.22878134\n",
      "Trained batch 761 batch loss 1.16969931 epoch total loss 1.22870362\n",
      "Trained batch 762 batch loss 1.30995274 epoch total loss 1.22881019\n",
      "Trained batch 763 batch loss 1.24756539 epoch total loss 1.22883475\n",
      "Trained batch 764 batch loss 1.25964451 epoch total loss 1.22887516\n",
      "Trained batch 765 batch loss 1.18555379 epoch total loss 1.22881854\n",
      "Trained batch 766 batch loss 1.21775866 epoch total loss 1.22880411\n",
      "Trained batch 767 batch loss 1.17345345 epoch total loss 1.22873187\n",
      "Trained batch 768 batch loss 1.31603336 epoch total loss 1.2288456\n",
      "Trained batch 769 batch loss 1.29696798 epoch total loss 1.22893417\n",
      "Trained batch 770 batch loss 1.25721359 epoch total loss 1.22897089\n",
      "Trained batch 771 batch loss 1.14425921 epoch total loss 1.22886109\n",
      "Trained batch 772 batch loss 1.15358567 epoch total loss 1.22876358\n",
      "Trained batch 773 batch loss 1.20673764 epoch total loss 1.22873509\n",
      "Trained batch 774 batch loss 1.23418355 epoch total loss 1.22874212\n",
      "Trained batch 775 batch loss 1.25924993 epoch total loss 1.22878146\n",
      "Trained batch 776 batch loss 1.38960385 epoch total loss 1.22898877\n",
      "Trained batch 777 batch loss 1.3508507 epoch total loss 1.22914553\n",
      "Trained batch 778 batch loss 1.28140783 epoch total loss 1.22921276\n",
      "Trained batch 779 batch loss 1.32028437 epoch total loss 1.22932971\n",
      "Trained batch 780 batch loss 1.14861441 epoch total loss 1.22922623\n",
      "Trained batch 781 batch loss 1.23976278 epoch total loss 1.2292397\n",
      "Trained batch 782 batch loss 1.27695465 epoch total loss 1.22930074\n",
      "Trained batch 783 batch loss 1.29010749 epoch total loss 1.22937834\n",
      "Trained batch 784 batch loss 1.2626462 epoch total loss 1.22942078\n",
      "Trained batch 785 batch loss 1.21471846 epoch total loss 1.22940207\n",
      "Trained batch 786 batch loss 1.33457768 epoch total loss 1.22953594\n",
      "Trained batch 787 batch loss 1.24557781 epoch total loss 1.22955632\n",
      "Trained batch 788 batch loss 1.26697659 epoch total loss 1.22960377\n",
      "Trained batch 789 batch loss 1.20454097 epoch total loss 1.22957206\n",
      "Trained batch 790 batch loss 1.21710622 epoch total loss 1.2295562\n",
      "Trained batch 791 batch loss 1.20632184 epoch total loss 1.22952688\n",
      "Trained batch 792 batch loss 1.24432564 epoch total loss 1.22954547\n",
      "Trained batch 793 batch loss 1.19650018 epoch total loss 1.22950375\n",
      "Trained batch 794 batch loss 1.14516973 epoch total loss 1.22939754\n",
      "Trained batch 795 batch loss 1.15641356 epoch total loss 1.22930574\n",
      "Trained batch 796 batch loss 1.081599 epoch total loss 1.22912025\n",
      "Trained batch 797 batch loss 1.0806222 epoch total loss 1.22893393\n",
      "Trained batch 798 batch loss 1.23417544 epoch total loss 1.22894049\n",
      "Trained batch 799 batch loss 1.28628707 epoch total loss 1.22901225\n",
      "Trained batch 800 batch loss 1.19402373 epoch total loss 1.22896862\n",
      "Trained batch 801 batch loss 1.32954705 epoch total loss 1.22909415\n",
      "Trained batch 802 batch loss 1.21628797 epoch total loss 1.22907817\n",
      "Trained batch 803 batch loss 1.08555865 epoch total loss 1.22889948\n",
      "Trained batch 804 batch loss 1.02827501 epoch total loss 1.22865\n",
      "Trained batch 805 batch loss 1.19125962 epoch total loss 1.22860348\n",
      "Trained batch 806 batch loss 1.18795991 epoch total loss 1.22855306\n",
      "Trained batch 807 batch loss 1.13695693 epoch total loss 1.22843957\n",
      "Trained batch 808 batch loss 1.13057208 epoch total loss 1.22831845\n",
      "Trained batch 809 batch loss 1.09603739 epoch total loss 1.2281549\n",
      "Trained batch 810 batch loss 1.1025964 epoch total loss 1.22799993\n",
      "Trained batch 811 batch loss 1.2488308 epoch total loss 1.22802556\n",
      "Trained batch 812 batch loss 1.23640466 epoch total loss 1.22803593\n",
      "Trained batch 813 batch loss 1.26329088 epoch total loss 1.22807932\n",
      "Trained batch 814 batch loss 1.03566194 epoch total loss 1.22784293\n",
      "Trained batch 815 batch loss 1.16322351 epoch total loss 1.22776353\n",
      "Trained batch 816 batch loss 1.07690644 epoch total loss 1.22757876\n",
      "Trained batch 817 batch loss 1.23385155 epoch total loss 1.22758639\n",
      "Trained batch 818 batch loss 1.19192493 epoch total loss 1.22754276\n",
      "Trained batch 819 batch loss 1.4251852 epoch total loss 1.22778404\n",
      "Trained batch 820 batch loss 1.50019491 epoch total loss 1.22811627\n",
      "Trained batch 821 batch loss 1.36611032 epoch total loss 1.22828424\n",
      "Trained batch 822 batch loss 1.13851452 epoch total loss 1.22817504\n",
      "Trained batch 823 batch loss 1.08957255 epoch total loss 1.2280066\n",
      "Trained batch 824 batch loss 1.2166034 epoch total loss 1.22799277\n",
      "Trained batch 825 batch loss 1.27186155 epoch total loss 1.22804594\n",
      "Trained batch 826 batch loss 1.20088971 epoch total loss 1.22801304\n",
      "Trained batch 827 batch loss 1.27801967 epoch total loss 1.2280736\n",
      "Trained batch 828 batch loss 1.2718643 epoch total loss 1.22812641\n",
      "Trained batch 829 batch loss 1.38835132 epoch total loss 1.22831976\n",
      "Trained batch 830 batch loss 1.26819098 epoch total loss 1.22836769\n",
      "Trained batch 831 batch loss 1.28460908 epoch total loss 1.2284354\n",
      "Trained batch 832 batch loss 1.27757311 epoch total loss 1.22849452\n",
      "Trained batch 833 batch loss 1.23866141 epoch total loss 1.22850668\n",
      "Trained batch 834 batch loss 1.29513562 epoch total loss 1.22858655\n",
      "Trained batch 835 batch loss 1.35993528 epoch total loss 1.22874403\n",
      "Trained batch 836 batch loss 1.2252574 epoch total loss 1.22873974\n",
      "Trained batch 837 batch loss 1.33956814 epoch total loss 1.22887218\n",
      "Trained batch 838 batch loss 1.36835074 epoch total loss 1.22903872\n",
      "Trained batch 839 batch loss 1.23463714 epoch total loss 1.22904539\n",
      "Trained batch 840 batch loss 1.18185401 epoch total loss 1.22898924\n",
      "Trained batch 841 batch loss 1.22442412 epoch total loss 1.22898376\n",
      "Trained batch 842 batch loss 1.28078473 epoch total loss 1.22904527\n",
      "Trained batch 843 batch loss 1.14276302 epoch total loss 1.22894299\n",
      "Trained batch 844 batch loss 1.21244574 epoch total loss 1.22892332\n",
      "Trained batch 845 batch loss 1.23869538 epoch total loss 1.22893488\n",
      "Trained batch 846 batch loss 1.30657268 epoch total loss 1.22902656\n",
      "Trained batch 847 batch loss 1.21683538 epoch total loss 1.22901213\n",
      "Trained batch 848 batch loss 1.1831789 epoch total loss 1.22895813\n",
      "Trained batch 849 batch loss 1.26442838 epoch total loss 1.22899985\n",
      "Trained batch 850 batch loss 1.27667391 epoch total loss 1.229056\n",
      "Trained batch 851 batch loss 1.10154533 epoch total loss 1.22890627\n",
      "Trained batch 852 batch loss 1.09651804 epoch total loss 1.22875082\n",
      "Trained batch 853 batch loss 1.26238751 epoch total loss 1.22879028\n",
      "Trained batch 854 batch loss 1.29588318 epoch total loss 1.22886884\n",
      "Trained batch 855 batch loss 1.15321231 epoch total loss 1.22878027\n",
      "Trained batch 856 batch loss 1.19232941 epoch total loss 1.22873783\n",
      "Trained batch 857 batch loss 1.32451463 epoch total loss 1.22884953\n",
      "Trained batch 858 batch loss 1.18760943 epoch total loss 1.22880149\n",
      "Trained batch 859 batch loss 1.17586493 epoch total loss 1.22873986\n",
      "Trained batch 860 batch loss 1.19095659 epoch total loss 1.22869587\n",
      "Trained batch 861 batch loss 1.26988029 epoch total loss 1.22874379\n",
      "Trained batch 862 batch loss 1.22741485 epoch total loss 1.22874224\n",
      "Trained batch 863 batch loss 1.30446506 epoch total loss 1.22883\n",
      "Trained batch 864 batch loss 1.20366299 epoch total loss 1.22880077\n",
      "Trained batch 865 batch loss 1.11628509 epoch total loss 1.22867072\n",
      "Trained batch 866 batch loss 1.17912602 epoch total loss 1.2286135\n",
      "Trained batch 867 batch loss 1.22776675 epoch total loss 1.22861254\n",
      "Trained batch 868 batch loss 1.26525617 epoch total loss 1.22865474\n",
      "Trained batch 869 batch loss 1.29545963 epoch total loss 1.22873151\n",
      "Trained batch 870 batch loss 1.23197973 epoch total loss 1.22873521\n",
      "Trained batch 871 batch loss 1.307356 epoch total loss 1.22882545\n",
      "Trained batch 872 batch loss 1.197065 epoch total loss 1.22878897\n",
      "Trained batch 873 batch loss 1.12868977 epoch total loss 1.22867429\n",
      "Trained batch 874 batch loss 1.07016456 epoch total loss 1.22849298\n",
      "Trained batch 875 batch loss 1.25957394 epoch total loss 1.2285285\n",
      "Trained batch 876 batch loss 1.29300785 epoch total loss 1.22860205\n",
      "Trained batch 877 batch loss 1.25123668 epoch total loss 1.2286278\n",
      "Trained batch 878 batch loss 1.27497339 epoch total loss 1.22868061\n",
      "Trained batch 879 batch loss 1.40370154 epoch total loss 1.22887981\n",
      "Trained batch 880 batch loss 1.35632467 epoch total loss 1.22902453\n",
      "Trained batch 881 batch loss 1.24191761 epoch total loss 1.22903919\n",
      "Trained batch 882 batch loss 1.20550764 epoch total loss 1.22901261\n",
      "Trained batch 883 batch loss 1.25071573 epoch total loss 1.22903717\n",
      "Trained batch 884 batch loss 1.36110294 epoch total loss 1.22918665\n",
      "Trained batch 885 batch loss 1.34610605 epoch total loss 1.22931862\n",
      "Trained batch 886 batch loss 1.29150045 epoch total loss 1.22938883\n",
      "Trained batch 887 batch loss 1.33919239 epoch total loss 1.22951269\n",
      "Trained batch 888 batch loss 1.30580819 epoch total loss 1.22959864\n",
      "Trained batch 889 batch loss 1.20966864 epoch total loss 1.22957623\n",
      "Trained batch 890 batch loss 1.18275023 epoch total loss 1.22952354\n",
      "Trained batch 891 batch loss 1.09158576 epoch total loss 1.22936881\n",
      "Trained batch 892 batch loss 1.18391919 epoch total loss 1.22931778\n",
      "Trained batch 893 batch loss 1.13141751 epoch total loss 1.22920823\n",
      "Trained batch 894 batch loss 1.11401129 epoch total loss 1.22907937\n",
      "Trained batch 895 batch loss 1.31133091 epoch total loss 1.22917128\n",
      "Trained batch 896 batch loss 1.08442152 epoch total loss 1.22900975\n",
      "Trained batch 897 batch loss 1.07664108 epoch total loss 1.22883987\n",
      "Trained batch 898 batch loss 1.16486335 epoch total loss 1.22876871\n",
      "Trained batch 899 batch loss 1.16890609 epoch total loss 1.22870219\n",
      "Trained batch 900 batch loss 1.10494268 epoch total loss 1.22856474\n",
      "Trained batch 901 batch loss 1.12213528 epoch total loss 1.22844672\n",
      "Trained batch 902 batch loss 1.15624607 epoch total loss 1.22836661\n",
      "Trained batch 903 batch loss 1.13282979 epoch total loss 1.22826076\n",
      "Trained batch 904 batch loss 1.2035917 epoch total loss 1.22823358\n",
      "Trained batch 905 batch loss 1.18353212 epoch total loss 1.2281841\n",
      "Trained batch 906 batch loss 1.23071051 epoch total loss 1.22818685\n",
      "Trained batch 907 batch loss 1.27314472 epoch total loss 1.22823644\n",
      "Trained batch 908 batch loss 1.28845763 epoch total loss 1.22830284\n",
      "Trained batch 909 batch loss 1.31468284 epoch total loss 1.22839785\n",
      "Trained batch 910 batch loss 1.35548544 epoch total loss 1.22853744\n",
      "Trained batch 911 batch loss 1.16877246 epoch total loss 1.22847199\n",
      "Trained batch 912 batch loss 1.17528319 epoch total loss 1.22841358\n",
      "Trained batch 913 batch loss 1.10908663 epoch total loss 1.22828293\n",
      "Trained batch 914 batch loss 1.09825182 epoch total loss 1.22814071\n",
      "Trained batch 915 batch loss 1.29834676 epoch total loss 1.22821748\n",
      "Trained batch 916 batch loss 1.18380129 epoch total loss 1.22816896\n",
      "Trained batch 917 batch loss 1.27244043 epoch total loss 1.22821736\n",
      "Trained batch 918 batch loss 1.28101254 epoch total loss 1.22827482\n",
      "Trained batch 919 batch loss 1.15727615 epoch total loss 1.22819746\n",
      "Trained batch 920 batch loss 1.12263966 epoch total loss 1.22808278\n",
      "Trained batch 921 batch loss 1.10447621 epoch total loss 1.22794867\n",
      "Trained batch 922 batch loss 1.1738373 epoch total loss 1.2278899\n",
      "Trained batch 923 batch loss 1.15510762 epoch total loss 1.2278111\n",
      "Trained batch 924 batch loss 1.26968575 epoch total loss 1.2278564\n",
      "Trained batch 925 batch loss 1.26065695 epoch total loss 1.2278918\n",
      "Trained batch 926 batch loss 1.17483568 epoch total loss 1.22783446\n",
      "Trained batch 927 batch loss 1.2473129 epoch total loss 1.22785544\n",
      "Trained batch 928 batch loss 1.16780591 epoch total loss 1.22779083\n",
      "Trained batch 929 batch loss 1.13875043 epoch total loss 1.22769499\n",
      "Trained batch 930 batch loss 1.22654915 epoch total loss 1.2276938\n",
      "Trained batch 931 batch loss 1.18087554 epoch total loss 1.22764361\n",
      "Trained batch 932 batch loss 1.27560222 epoch total loss 1.22769511\n",
      "Trained batch 933 batch loss 1.25786 epoch total loss 1.22772729\n",
      "Trained batch 934 batch loss 1.31731343 epoch total loss 1.22782314\n",
      "Trained batch 935 batch loss 1.11171579 epoch total loss 1.22769904\n",
      "Trained batch 936 batch loss 1.22875 epoch total loss 1.22770011\n",
      "Trained batch 937 batch loss 1.25338209 epoch total loss 1.22772753\n",
      "Trained batch 938 batch loss 1.16721606 epoch total loss 1.22766304\n",
      "Trained batch 939 batch loss 1.08894932 epoch total loss 1.22751546\n",
      "Trained batch 940 batch loss 1.05581403 epoch total loss 1.22733271\n",
      "Trained batch 941 batch loss 1.15484965 epoch total loss 1.2272557\n",
      "Trained batch 942 batch loss 1.11809444 epoch total loss 1.22713983\n",
      "Trained batch 943 batch loss 1.0544939 epoch total loss 1.22695673\n",
      "Trained batch 944 batch loss 1.07007885 epoch total loss 1.22679043\n",
      "Trained batch 945 batch loss 1.15967822 epoch total loss 1.2267195\n",
      "Trained batch 946 batch loss 1.23348331 epoch total loss 1.22672665\n",
      "Trained batch 947 batch loss 1.31178403 epoch total loss 1.22681642\n",
      "Trained batch 948 batch loss 1.15445065 epoch total loss 1.22674012\n",
      "Trained batch 949 batch loss 1.31173372 epoch total loss 1.22682965\n",
      "Trained batch 950 batch loss 1.13644564 epoch total loss 1.22673452\n",
      "Trained batch 951 batch loss 1.13974726 epoch total loss 1.22664309\n",
      "Trained batch 952 batch loss 1.17416441 epoch total loss 1.22658801\n",
      "Trained batch 953 batch loss 1.29774785 epoch total loss 1.22666264\n",
      "Trained batch 954 batch loss 1.26825202 epoch total loss 1.22670627\n",
      "Trained batch 955 batch loss 1.21372032 epoch total loss 1.2266928\n",
      "Trained batch 956 batch loss 1.21055388 epoch total loss 1.22667587\n",
      "Trained batch 957 batch loss 1.24248445 epoch total loss 1.22669232\n",
      "Trained batch 958 batch loss 1.20681739 epoch total loss 1.22667158\n",
      "Trained batch 959 batch loss 1.43647516 epoch total loss 1.22689044\n",
      "Trained batch 960 batch loss 1.37315762 epoch total loss 1.22704279\n",
      "Trained batch 961 batch loss 1.29932284 epoch total loss 1.22711802\n",
      "Trained batch 962 batch loss 1.28337896 epoch total loss 1.22717643\n",
      "Trained batch 963 batch loss 1.07103503 epoch total loss 1.2270143\n",
      "Trained batch 964 batch loss 1.076033 epoch total loss 1.22685766\n",
      "Trained batch 965 batch loss 1.0804584 epoch total loss 1.22670591\n",
      "Trained batch 966 batch loss 1.19008684 epoch total loss 1.226668\n",
      "Trained batch 967 batch loss 0.967232287 epoch total loss 1.22639978\n",
      "Trained batch 968 batch loss 0.996330678 epoch total loss 1.22616208\n",
      "Trained batch 969 batch loss 0.947767 epoch total loss 1.22587478\n",
      "Trained batch 970 batch loss 1.12186527 epoch total loss 1.22576749\n",
      "Trained batch 971 batch loss 1.1218009 epoch total loss 1.22566044\n",
      "Trained batch 972 batch loss 1.26852798 epoch total loss 1.22570467\n",
      "Trained batch 973 batch loss 1.19087315 epoch total loss 1.22566891\n",
      "Trained batch 974 batch loss 1.23569632 epoch total loss 1.22567916\n",
      "Trained batch 975 batch loss 1.28094268 epoch total loss 1.22573578\n",
      "Trained batch 976 batch loss 1.17594719 epoch total loss 1.22568476\n",
      "Trained batch 977 batch loss 1.19858527 epoch total loss 1.22565699\n",
      "Trained batch 978 batch loss 1.13665533 epoch total loss 1.22556591\n",
      "Trained batch 979 batch loss 1.20363438 epoch total loss 1.2255435\n",
      "Trained batch 980 batch loss 1.14216185 epoch total loss 1.2254585\n",
      "Trained batch 981 batch loss 1.11969924 epoch total loss 1.22535074\n",
      "Trained batch 982 batch loss 1.20861721 epoch total loss 1.22533369\n",
      "Trained batch 983 batch loss 1.408319 epoch total loss 1.2255199\n",
      "Trained batch 984 batch loss 1.328475 epoch total loss 1.22562456\n",
      "Trained batch 985 batch loss 1.28611267 epoch total loss 1.22568595\n",
      "Trained batch 986 batch loss 1.19483757 epoch total loss 1.2256546\n",
      "Trained batch 987 batch loss 1.20477891 epoch total loss 1.2256335\n",
      "Trained batch 988 batch loss 1.13653708 epoch total loss 1.22554338\n",
      "Trained batch 989 batch loss 1.08295679 epoch total loss 1.22539926\n",
      "Trained batch 990 batch loss 1.19823134 epoch total loss 1.22537184\n",
      "Trained batch 991 batch loss 1.34322751 epoch total loss 1.22549081\n",
      "Trained batch 992 batch loss 1.29851627 epoch total loss 1.22556436\n",
      "Trained batch 993 batch loss 1.23198044 epoch total loss 1.2255708\n",
      "Trained batch 994 batch loss 1.17041922 epoch total loss 1.22551537\n",
      "Trained batch 995 batch loss 1.20539117 epoch total loss 1.2254951\n",
      "Trained batch 996 batch loss 1.21968341 epoch total loss 1.22548938\n",
      "Trained batch 997 batch loss 1.04436922 epoch total loss 1.22530758\n",
      "Trained batch 998 batch loss 1.18504286 epoch total loss 1.22526729\n",
      "Trained batch 999 batch loss 1.12781847 epoch total loss 1.22516978\n",
      "Trained batch 1000 batch loss 1.26148272 epoch total loss 1.22520602\n",
      "Trained batch 1001 batch loss 1.21287 epoch total loss 1.22519374\n",
      "Trained batch 1002 batch loss 1.45123756 epoch total loss 1.2254194\n",
      "Trained batch 1003 batch loss 1.38808048 epoch total loss 1.22558153\n",
      "Trained batch 1004 batch loss 1.52516949 epoch total loss 1.22587991\n",
      "Trained batch 1005 batch loss 1.46540117 epoch total loss 1.22611833\n",
      "Trained batch 1006 batch loss 1.12596941 epoch total loss 1.22601879\n",
      "Trained batch 1007 batch loss 1.16653419 epoch total loss 1.22595966\n",
      "Trained batch 1008 batch loss 1.22736609 epoch total loss 1.22596109\n",
      "Trained batch 1009 batch loss 1.14254189 epoch total loss 1.22587848\n",
      "Trained batch 1010 batch loss 1.07397318 epoch total loss 1.22572803\n",
      "Trained batch 1011 batch loss 1.13040793 epoch total loss 1.22563374\n",
      "Trained batch 1012 batch loss 1.13800526 epoch total loss 1.22554719\n",
      "Trained batch 1013 batch loss 1.07755542 epoch total loss 1.22540104\n",
      "Trained batch 1014 batch loss 1.05480409 epoch total loss 1.22523284\n",
      "Trained batch 1015 batch loss 1.06816435 epoch total loss 1.22507811\n",
      "Trained batch 1016 batch loss 1.17917085 epoch total loss 1.22503293\n",
      "Trained batch 1017 batch loss 1.23103988 epoch total loss 1.22503889\n",
      "Trained batch 1018 batch loss 1.11006391 epoch total loss 1.22492599\n",
      "Trained batch 1019 batch loss 1.10883856 epoch total loss 1.22481203\n",
      "Trained batch 1020 batch loss 1.20780897 epoch total loss 1.22479534\n",
      "Trained batch 1021 batch loss 1.12779129 epoch total loss 1.22470033\n",
      "Trained batch 1022 batch loss 1.16953182 epoch total loss 1.22464645\n",
      "Trained batch 1023 batch loss 1.05086851 epoch total loss 1.22447658\n",
      "Trained batch 1024 batch loss 1.03022408 epoch total loss 1.22428691\n",
      "Trained batch 1025 batch loss 1.21498728 epoch total loss 1.22427785\n",
      "Trained batch 1026 batch loss 1.27730143 epoch total loss 1.22432959\n",
      "Trained batch 1027 batch loss 1.3294189 epoch total loss 1.22443187\n",
      "Trained batch 1028 batch loss 1.36838508 epoch total loss 1.22457194\n",
      "Trained batch 1029 batch loss 1.20509362 epoch total loss 1.22455299\n",
      "Trained batch 1030 batch loss 1.21998048 epoch total loss 1.22454858\n",
      "Trained batch 1031 batch loss 1.21685338 epoch total loss 1.22454107\n",
      "Trained batch 1032 batch loss 1.26948917 epoch total loss 1.2245847\n",
      "Trained batch 1033 batch loss 1.1987884 epoch total loss 1.22455966\n",
      "Trained batch 1034 batch loss 1.19647539 epoch total loss 1.22453249\n",
      "Trained batch 1035 batch loss 1.38609719 epoch total loss 1.22468865\n",
      "Trained batch 1036 batch loss 1.41674495 epoch total loss 1.22487402\n",
      "Trained batch 1037 batch loss 1.26351917 epoch total loss 1.22491133\n",
      "Trained batch 1038 batch loss 1.28577089 epoch total loss 1.22497\n",
      "Trained batch 1039 batch loss 1.14641333 epoch total loss 1.22489429\n",
      "Trained batch 1040 batch loss 1.16849053 epoch total loss 1.22484\n",
      "Trained batch 1041 batch loss 1.17774141 epoch total loss 1.22479475\n",
      "Trained batch 1042 batch loss 1.1009593 epoch total loss 1.22467589\n",
      "Trained batch 1043 batch loss 1.16472673 epoch total loss 1.22461843\n",
      "Trained batch 1044 batch loss 1.16454375 epoch total loss 1.22456086\n",
      "Trained batch 1045 batch loss 1.16947079 epoch total loss 1.22450805\n",
      "Trained batch 1046 batch loss 1.31774199 epoch total loss 1.22459722\n",
      "Trained batch 1047 batch loss 1.21455657 epoch total loss 1.22458768\n",
      "Trained batch 1048 batch loss 1.36630237 epoch total loss 1.22472298\n",
      "Trained batch 1049 batch loss 1.28600645 epoch total loss 1.22478139\n",
      "Trained batch 1050 batch loss 1.45526433 epoch total loss 1.22500098\n",
      "Trained batch 1051 batch loss 1.41300905 epoch total loss 1.22517979\n",
      "Trained batch 1052 batch loss 1.26679587 epoch total loss 1.22521937\n",
      "Trained batch 1053 batch loss 1.18734896 epoch total loss 1.22518349\n",
      "Trained batch 1054 batch loss 1.31424451 epoch total loss 1.22526789\n",
      "Trained batch 1055 batch loss 1.28548837 epoch total loss 1.22532499\n",
      "Trained batch 1056 batch loss 1.17041111 epoch total loss 1.22527301\n",
      "Trained batch 1057 batch loss 1.19964409 epoch total loss 1.22524869\n",
      "Trained batch 1058 batch loss 1.08638811 epoch total loss 1.22511744\n",
      "Trained batch 1059 batch loss 1.16294146 epoch total loss 1.22505879\n",
      "Trained batch 1060 batch loss 1.15237427 epoch total loss 1.22499025\n",
      "Trained batch 1061 batch loss 1.24116516 epoch total loss 1.22500551\n",
      "Trained batch 1062 batch loss 1.17378116 epoch total loss 1.22495735\n",
      "Trained batch 1063 batch loss 1.16492546 epoch total loss 1.22490084\n",
      "Trained batch 1064 batch loss 1.24299359 epoch total loss 1.22491789\n",
      "Trained batch 1065 batch loss 1.22597551 epoch total loss 1.22491884\n",
      "Trained batch 1066 batch loss 1.17036319 epoch total loss 1.2248677\n",
      "Trained batch 1067 batch loss 1.11010528 epoch total loss 1.22476017\n",
      "Trained batch 1068 batch loss 1.17886448 epoch total loss 1.22471714\n",
      "Trained batch 1069 batch loss 1.20615721 epoch total loss 1.22469985\n",
      "Trained batch 1070 batch loss 1.25003266 epoch total loss 1.22472346\n",
      "Trained batch 1071 batch loss 1.22423697 epoch total loss 1.22472298\n",
      "Trained batch 1072 batch loss 1.36964607 epoch total loss 1.22485816\n",
      "Trained batch 1073 batch loss 1.36091244 epoch total loss 1.224985\n",
      "Trained batch 1074 batch loss 1.24538851 epoch total loss 1.22500396\n",
      "Trained batch 1075 batch loss 1.34808707 epoch total loss 1.22511852\n",
      "Trained batch 1076 batch loss 1.27693808 epoch total loss 1.2251668\n",
      "Trained batch 1077 batch loss 1.37188709 epoch total loss 1.22530293\n",
      "Trained batch 1078 batch loss 1.17479253 epoch total loss 1.22525609\n",
      "Trained batch 1079 batch loss 1.11714256 epoch total loss 1.22515595\n",
      "Trained batch 1080 batch loss 1.27006269 epoch total loss 1.22519743\n",
      "Trained batch 1081 batch loss 1.15403032 epoch total loss 1.22513163\n",
      "Trained batch 1082 batch loss 1.0011301 epoch total loss 1.22492456\n",
      "Trained batch 1083 batch loss 1.23804188 epoch total loss 1.22493672\n",
      "Trained batch 1084 batch loss 1.2219243 epoch total loss 1.22493386\n",
      "Trained batch 1085 batch loss 1.23350346 epoch total loss 1.22494185\n",
      "Trained batch 1086 batch loss 1.25980127 epoch total loss 1.22497392\n",
      "Trained batch 1087 batch loss 1.43659282 epoch total loss 1.22516859\n",
      "Trained batch 1088 batch loss 1.36951649 epoch total loss 1.22530127\n",
      "Trained batch 1089 batch loss 1.33985758 epoch total loss 1.22540653\n",
      "Trained batch 1090 batch loss 1.27997327 epoch total loss 1.2254566\n",
      "Trained batch 1091 batch loss 1.23912168 epoch total loss 1.22546911\n",
      "Trained batch 1092 batch loss 1.23237979 epoch total loss 1.22547555\n",
      "Trained batch 1093 batch loss 1.12240314 epoch total loss 1.22538126\n",
      "Trained batch 1094 batch loss 1.20646262 epoch total loss 1.22536385\n",
      "Trained batch 1095 batch loss 1.20484686 epoch total loss 1.22534513\n",
      "Trained batch 1096 batch loss 1.18595278 epoch total loss 1.22530913\n",
      "Trained batch 1097 batch loss 1.02876 epoch total loss 1.22513008\n",
      "Trained batch 1098 batch loss 0.99767828 epoch total loss 1.2249229\n",
      "Trained batch 1099 batch loss 1.05923152 epoch total loss 1.2247721\n",
      "Trained batch 1100 batch loss 1.45793724 epoch total loss 1.22498405\n",
      "Trained batch 1101 batch loss 1.3496629 epoch total loss 1.22509718\n",
      "Trained batch 1102 batch loss 1.44294846 epoch total loss 1.22529495\n",
      "Trained batch 1103 batch loss 1.37790537 epoch total loss 1.22543335\n",
      "Trained batch 1104 batch loss 1.30254102 epoch total loss 1.22550309\n",
      "Trained batch 1105 batch loss 1.42550159 epoch total loss 1.22568417\n",
      "Trained batch 1106 batch loss 1.34956598 epoch total loss 1.22579622\n",
      "Trained batch 1107 batch loss 1.32419491 epoch total loss 1.22588515\n",
      "Trained batch 1108 batch loss 1.26849365 epoch total loss 1.22592354\n",
      "Trained batch 1109 batch loss 1.4084549 epoch total loss 1.22608805\n",
      "Trained batch 1110 batch loss 1.25356197 epoch total loss 1.22611284\n",
      "Trained batch 1111 batch loss 1.41564405 epoch total loss 1.22628343\n",
      "Trained batch 1112 batch loss 1.28239357 epoch total loss 1.22633386\n",
      "Trained batch 1113 batch loss 1.17481875 epoch total loss 1.22628748\n",
      "Trained batch 1114 batch loss 1.21700704 epoch total loss 1.22627926\n",
      "Trained batch 1115 batch loss 1.10925567 epoch total loss 1.22617424\n",
      "Trained batch 1116 batch loss 1.03771329 epoch total loss 1.22600543\n",
      "Trained batch 1117 batch loss 1.1440686 epoch total loss 1.225932\n",
      "Trained batch 1118 batch loss 0.970088661 epoch total loss 1.22570324\n",
      "Trained batch 1119 batch loss 1.05728149 epoch total loss 1.22555268\n",
      "Trained batch 1120 batch loss 0.983220577 epoch total loss 1.22533631\n",
      "Trained batch 1121 batch loss 1.00603211 epoch total loss 1.22514069\n",
      "Trained batch 1122 batch loss 0.94128716 epoch total loss 1.22488773\n",
      "Trained batch 1123 batch loss 1.11463642 epoch total loss 1.2247895\n",
      "Trained batch 1124 batch loss 1.17951703 epoch total loss 1.22474921\n",
      "Trained batch 1125 batch loss 1.21705639 epoch total loss 1.22474241\n",
      "Trained batch 1126 batch loss 1.13226008 epoch total loss 1.22466028\n",
      "Trained batch 1127 batch loss 1.26345694 epoch total loss 1.22469461\n",
      "Trained batch 1128 batch loss 1.48503518 epoch total loss 1.2249254\n",
      "Trained batch 1129 batch loss 1.40597117 epoch total loss 1.22508574\n",
      "Trained batch 1130 batch loss 1.25480199 epoch total loss 1.22511208\n",
      "Trained batch 1131 batch loss 1.25529289 epoch total loss 1.22513866\n",
      "Trained batch 1132 batch loss 1.40916622 epoch total loss 1.22530127\n",
      "Trained batch 1133 batch loss 1.27729774 epoch total loss 1.22534716\n",
      "Trained batch 1134 batch loss 1.2891227 epoch total loss 1.22540343\n",
      "Trained batch 1135 batch loss 1.16933429 epoch total loss 1.22535396\n",
      "Trained batch 1136 batch loss 1.14621663 epoch total loss 1.22528434\n",
      "Trained batch 1137 batch loss 1.24764454 epoch total loss 1.22530401\n",
      "Trained batch 1138 batch loss 1.33685219 epoch total loss 1.225402\n",
      "Trained batch 1139 batch loss 1.24076188 epoch total loss 1.22541547\n",
      "Trained batch 1140 batch loss 1.29778409 epoch total loss 1.22547889\n",
      "Trained batch 1141 batch loss 1.28871226 epoch total loss 1.22553432\n",
      "Trained batch 1142 batch loss 1.30871868 epoch total loss 1.22560716\n",
      "Trained batch 1143 batch loss 1.30860829 epoch total loss 1.22567976\n",
      "Trained batch 1144 batch loss 1.14890838 epoch total loss 1.22561264\n",
      "Trained batch 1145 batch loss 1.12521267 epoch total loss 1.2255249\n",
      "Trained batch 1146 batch loss 1.06395268 epoch total loss 1.225384\n",
      "Trained batch 1147 batch loss 1.32970822 epoch total loss 1.22547495\n",
      "Trained batch 1148 batch loss 1.1941818 epoch total loss 1.22544765\n",
      "Trained batch 1149 batch loss 1.11615753 epoch total loss 1.22535264\n",
      "Trained batch 1150 batch loss 1.50806165 epoch total loss 1.22559845\n",
      "Trained batch 1151 batch loss 1.48586702 epoch total loss 1.22582459\n",
      "Trained batch 1152 batch loss 1.30635273 epoch total loss 1.22589445\n",
      "Trained batch 1153 batch loss 1.22709596 epoch total loss 1.22589552\n",
      "Trained batch 1154 batch loss 1.16386938 epoch total loss 1.22584176\n",
      "Trained batch 1155 batch loss 1.1688776 epoch total loss 1.22579229\n",
      "Trained batch 1156 batch loss 1.3170948 epoch total loss 1.22587132\n",
      "Trained batch 1157 batch loss 1.18096066 epoch total loss 1.22583246\n",
      "Trained batch 1158 batch loss 1.25467467 epoch total loss 1.22585738\n",
      "Trained batch 1159 batch loss 1.26422119 epoch total loss 1.22589052\n",
      "Trained batch 1160 batch loss 1.31103885 epoch total loss 1.22596395\n",
      "Trained batch 1161 batch loss 1.42320287 epoch total loss 1.22613382\n",
      "Trained batch 1162 batch loss 1.3563329 epoch total loss 1.22624588\n",
      "Trained batch 1163 batch loss 1.29986155 epoch total loss 1.22630918\n",
      "Trained batch 1164 batch loss 1.21687579 epoch total loss 1.22630107\n",
      "Trained batch 1165 batch loss 1.14748061 epoch total loss 1.22623336\n",
      "Trained batch 1166 batch loss 1.24448967 epoch total loss 1.2262491\n",
      "Trained batch 1167 batch loss 1.19883037 epoch total loss 1.22622561\n",
      "Trained batch 1168 batch loss 1.16931796 epoch total loss 1.22617686\n",
      "Trained batch 1169 batch loss 1.17251945 epoch total loss 1.22613096\n",
      "Trained batch 1170 batch loss 1.15570211 epoch total loss 1.22607076\n",
      "Trained batch 1171 batch loss 1.21112669 epoch total loss 1.22605801\n",
      "Trained batch 1172 batch loss 1.15498424 epoch total loss 1.22599745\n",
      "Trained batch 1173 batch loss 1.26750219 epoch total loss 1.22603285\n",
      "Trained batch 1174 batch loss 1.16474676 epoch total loss 1.22598064\n",
      "Trained batch 1175 batch loss 1.27386165 epoch total loss 1.22602129\n",
      "Trained batch 1176 batch loss 1.33450627 epoch total loss 1.22611356\n",
      "Trained batch 1177 batch loss 1.32288122 epoch total loss 1.22619581\n",
      "Trained batch 1178 batch loss 1.35567498 epoch total loss 1.22630572\n",
      "Trained batch 1179 batch loss 1.19504333 epoch total loss 1.22627926\n",
      "Trained batch 1180 batch loss 1.21524405 epoch total loss 1.22626984\n",
      "Trained batch 1181 batch loss 1.19445837 epoch total loss 1.2262429\n",
      "Trained batch 1182 batch loss 1.25358367 epoch total loss 1.22626603\n",
      "Trained batch 1183 batch loss 1.36855054 epoch total loss 1.22638631\n",
      "Trained batch 1184 batch loss 1.38994455 epoch total loss 1.22652435\n",
      "Trained batch 1185 batch loss 1.52065349 epoch total loss 1.22677255\n",
      "Trained batch 1186 batch loss 1.23307502 epoch total loss 1.22677779\n",
      "Trained batch 1187 batch loss 1.43501806 epoch total loss 1.22695327\n",
      "Trained batch 1188 batch loss 1.13083112 epoch total loss 1.22687244\n",
      "Trained batch 1189 batch loss 1.33142769 epoch total loss 1.2269603\n",
      "Trained batch 1190 batch loss 1.41631627 epoch total loss 1.22711945\n",
      "Trained batch 1191 batch loss 1.29249883 epoch total loss 1.22717428\n",
      "Trained batch 1192 batch loss 1.07463932 epoch total loss 1.22704625\n",
      "Trained batch 1193 batch loss 1.17868912 epoch total loss 1.22700572\n",
      "Trained batch 1194 batch loss 1.18573105 epoch total loss 1.22697127\n",
      "Trained batch 1195 batch loss 1.14564276 epoch total loss 1.2269032\n",
      "Trained batch 1196 batch loss 1.05994344 epoch total loss 1.22676361\n",
      "Trained batch 1197 batch loss 1.03232169 epoch total loss 1.22660112\n",
      "Trained batch 1198 batch loss 1.27849758 epoch total loss 1.2266444\n",
      "Trained batch 1199 batch loss 1.20082068 epoch total loss 1.22662282\n",
      "Trained batch 1200 batch loss 1.22948253 epoch total loss 1.22662532\n",
      "Trained batch 1201 batch loss 1.24233198 epoch total loss 1.22663832\n",
      "Trained batch 1202 batch loss 1.27284575 epoch total loss 1.2266767\n",
      "Trained batch 1203 batch loss 1.19512868 epoch total loss 1.22665048\n",
      "Trained batch 1204 batch loss 1.22296441 epoch total loss 1.2266475\n",
      "Trained batch 1205 batch loss 1.19359255 epoch total loss 1.22662008\n",
      "Trained batch 1206 batch loss 1.22615588 epoch total loss 1.22661972\n",
      "Trained batch 1207 batch loss 1.16746926 epoch total loss 1.22657073\n",
      "Trained batch 1208 batch loss 1.07246459 epoch total loss 1.22644317\n",
      "Trained batch 1209 batch loss 0.90480876 epoch total loss 1.2261771\n",
      "Trained batch 1210 batch loss 1.1175915 epoch total loss 1.22608733\n",
      "Trained batch 1211 batch loss 1.19091547 epoch total loss 1.22605824\n",
      "Trained batch 1212 batch loss 1.37760091 epoch total loss 1.2261833\n",
      "Trained batch 1213 batch loss 1.36699045 epoch total loss 1.22629929\n",
      "Trained batch 1214 batch loss 1.37174678 epoch total loss 1.22641909\n",
      "Trained batch 1215 batch loss 1.21826243 epoch total loss 1.22641242\n",
      "Trained batch 1216 batch loss 1.26366389 epoch total loss 1.22644305\n",
      "Trained batch 1217 batch loss 1.24339676 epoch total loss 1.226457\n",
      "Trained batch 1218 batch loss 1.2261858 epoch total loss 1.22645676\n",
      "Trained batch 1219 batch loss 1.17760766 epoch total loss 1.22641671\n",
      "Trained batch 1220 batch loss 1.18342578 epoch total loss 1.22638154\n",
      "Trained batch 1221 batch loss 1.14504707 epoch total loss 1.2263149\n",
      "Trained batch 1222 batch loss 1.13315225 epoch total loss 1.22623861\n",
      "Trained batch 1223 batch loss 1.26995134 epoch total loss 1.22627437\n",
      "Trained batch 1224 batch loss 1.21138358 epoch total loss 1.22626221\n",
      "Trained batch 1225 batch loss 1.24882936 epoch total loss 1.22628057\n",
      "Trained batch 1226 batch loss 1.1455543 epoch total loss 1.22621477\n",
      "Trained batch 1227 batch loss 1.13356447 epoch total loss 1.22613919\n",
      "Trained batch 1228 batch loss 1.15776682 epoch total loss 1.22608352\n",
      "Trained batch 1229 batch loss 1.2074337 epoch total loss 1.22606826\n",
      "Trained batch 1230 batch loss 1.13551044 epoch total loss 1.22599459\n",
      "Trained batch 1231 batch loss 1.18925798 epoch total loss 1.22596478\n",
      "Trained batch 1232 batch loss 1.15636778 epoch total loss 1.22590828\n",
      "Trained batch 1233 batch loss 1.21143937 epoch total loss 1.22589648\n",
      "Trained batch 1234 batch loss 1.19580054 epoch total loss 1.22587216\n",
      "Trained batch 1235 batch loss 1.26879942 epoch total loss 1.22590685\n",
      "Trained batch 1236 batch loss 1.3040123 epoch total loss 1.22597\n",
      "Trained batch 1237 batch loss 1.39855623 epoch total loss 1.2261095\n",
      "Trained batch 1238 batch loss 1.2190659 epoch total loss 1.2261039\n",
      "Trained batch 1239 batch loss 1.14107549 epoch total loss 1.22603536\n",
      "Trained batch 1240 batch loss 1.05522406 epoch total loss 1.22589755\n",
      "Trained batch 1241 batch loss 1.25984573 epoch total loss 1.22592485\n",
      "Trained batch 1242 batch loss 1.31183064 epoch total loss 1.22599411\n",
      "Trained batch 1243 batch loss 1.32122183 epoch total loss 1.22607064\n",
      "Trained batch 1244 batch loss 1.26552355 epoch total loss 1.22610235\n",
      "Trained batch 1245 batch loss 1.2167064 epoch total loss 1.22609484\n",
      "Trained batch 1246 batch loss 1.26789117 epoch total loss 1.22612834\n",
      "Trained batch 1247 batch loss 1.18202758 epoch total loss 1.22609305\n",
      "Trained batch 1248 batch loss 1.26081622 epoch total loss 1.22612083\n",
      "Trained batch 1249 batch loss 1.27365875 epoch total loss 1.22615898\n",
      "Trained batch 1250 batch loss 1.19795609 epoch total loss 1.22613645\n",
      "Trained batch 1251 batch loss 1.14685297 epoch total loss 1.22607303\n",
      "Trained batch 1252 batch loss 1.26723051 epoch total loss 1.22610593\n",
      "Trained batch 1253 batch loss 1.26656199 epoch total loss 1.22613823\n",
      "Trained batch 1254 batch loss 1.26464272 epoch total loss 1.22616899\n",
      "Trained batch 1255 batch loss 1.18260312 epoch total loss 1.22613418\n",
      "Trained batch 1256 batch loss 1.29524148 epoch total loss 1.22618926\n",
      "Trained batch 1257 batch loss 1.28876352 epoch total loss 1.22623909\n",
      "Trained batch 1258 batch loss 1.30104947 epoch total loss 1.22629857\n",
      "Trained batch 1259 batch loss 1.2975924 epoch total loss 1.2263552\n",
      "Trained batch 1260 batch loss 1.17963386 epoch total loss 1.22631812\n",
      "Trained batch 1261 batch loss 1.24304032 epoch total loss 1.22633147\n",
      "Trained batch 1262 batch loss 1.14276743 epoch total loss 1.22626531\n",
      "Trained batch 1263 batch loss 1.22746396 epoch total loss 1.22626615\n",
      "Trained batch 1264 batch loss 1.35128784 epoch total loss 1.22636509\n",
      "Trained batch 1265 batch loss 1.36813211 epoch total loss 1.22647715\n",
      "Trained batch 1266 batch loss 1.24054694 epoch total loss 1.22648835\n",
      "Trained batch 1267 batch loss 1.1225841 epoch total loss 1.22640634\n",
      "Trained batch 1268 batch loss 1.25644994 epoch total loss 1.22643\n",
      "Trained batch 1269 batch loss 1.15173745 epoch total loss 1.22637117\n",
      "Trained batch 1270 batch loss 1.19264734 epoch total loss 1.22634459\n",
      "Trained batch 1271 batch loss 1.21625078 epoch total loss 1.22633672\n",
      "Trained batch 1272 batch loss 1.25757146 epoch total loss 1.22636127\n",
      "Trained batch 1273 batch loss 1.24664068 epoch total loss 1.22637713\n",
      "Trained batch 1274 batch loss 1.29085505 epoch total loss 1.22642779\n",
      "Trained batch 1275 batch loss 1.26407361 epoch total loss 1.22645724\n",
      "Trained batch 1276 batch loss 1.31769478 epoch total loss 1.22652888\n",
      "Trained batch 1277 batch loss 1.22684479 epoch total loss 1.226529\n",
      "Trained batch 1278 batch loss 1.28438365 epoch total loss 1.2265743\n",
      "Trained batch 1279 batch loss 1.26593113 epoch total loss 1.22660518\n",
      "Trained batch 1280 batch loss 0.986419678 epoch total loss 1.22641754\n",
      "Trained batch 1281 batch loss 0.979426682 epoch total loss 1.22622466\n",
      "Trained batch 1282 batch loss 1.08266592 epoch total loss 1.22611272\n",
      "Trained batch 1283 batch loss 1.21996951 epoch total loss 1.22610795\n",
      "Trained batch 1284 batch loss 1.33898675 epoch total loss 1.22619581\n",
      "Trained batch 1285 batch loss 1.46719933 epoch total loss 1.22638333\n",
      "Trained batch 1286 batch loss 1.30098295 epoch total loss 1.22644138\n",
      "Trained batch 1287 batch loss 1.13662577 epoch total loss 1.22637153\n",
      "Trained batch 1288 batch loss 1.22234774 epoch total loss 1.22636843\n",
      "Trained batch 1289 batch loss 1.27175319 epoch total loss 1.22640359\n",
      "Trained batch 1290 batch loss 1.31142402 epoch total loss 1.22646952\n",
      "Trained batch 1291 batch loss 1.207775 epoch total loss 1.22645497\n",
      "Trained batch 1292 batch loss 1.22420859 epoch total loss 1.2264533\n",
      "Trained batch 1293 batch loss 1.29420066 epoch total loss 1.22650564\n",
      "Trained batch 1294 batch loss 1.16712153 epoch total loss 1.22645974\n",
      "Trained batch 1295 batch loss 1.24462557 epoch total loss 1.22647381\n",
      "Trained batch 1296 batch loss 1.16606569 epoch total loss 1.2264272\n",
      "Trained batch 1297 batch loss 1.1163708 epoch total loss 1.22634232\n",
      "Trained batch 1298 batch loss 1.21260941 epoch total loss 1.22633171\n",
      "Trained batch 1299 batch loss 1.20368385 epoch total loss 1.22631431\n",
      "Trained batch 1300 batch loss 1.01658702 epoch total loss 1.22615302\n",
      "Trained batch 1301 batch loss 1.02558684 epoch total loss 1.22599888\n",
      "Trained batch 1302 batch loss 1.0956037 epoch total loss 1.22589874\n",
      "Trained batch 1303 batch loss 1.21072841 epoch total loss 1.22588706\n",
      "Trained batch 1304 batch loss 1.38506365 epoch total loss 1.22600901\n",
      "Trained batch 1305 batch loss 1.19940817 epoch total loss 1.22598875\n",
      "Trained batch 1306 batch loss 1.25349367 epoch total loss 1.22600985\n",
      "Trained batch 1307 batch loss 1.16729069 epoch total loss 1.2259649\n",
      "Trained batch 1308 batch loss 1.30847287 epoch total loss 1.22602797\n",
      "Trained batch 1309 batch loss 1.2829504 epoch total loss 1.22607148\n",
      "Trained batch 1310 batch loss 1.35687661 epoch total loss 1.22617137\n",
      "Trained batch 1311 batch loss 1.37628484 epoch total loss 1.22628582\n",
      "Trained batch 1312 batch loss 1.13612366 epoch total loss 1.22621715\n",
      "Trained batch 1313 batch loss 1.32810068 epoch total loss 1.22629476\n",
      "Trained batch 1314 batch loss 1.29471838 epoch total loss 1.22634673\n",
      "Trained batch 1315 batch loss 1.18461287 epoch total loss 1.22631502\n",
      "Trained batch 1316 batch loss 1.26005697 epoch total loss 1.22634065\n",
      "Trained batch 1317 batch loss 1.19095898 epoch total loss 1.22631371\n",
      "Trained batch 1318 batch loss 1.17490113 epoch total loss 1.22627473\n",
      "Trained batch 1319 batch loss 1.26970303 epoch total loss 1.22630763\n",
      "Trained batch 1320 batch loss 1.20535302 epoch total loss 1.22629178\n",
      "Trained batch 1321 batch loss 1.3953625 epoch total loss 1.22641969\n",
      "Trained batch 1322 batch loss 1.20735276 epoch total loss 1.22640538\n",
      "Trained batch 1323 batch loss 1.10349631 epoch total loss 1.2263124\n",
      "Trained batch 1324 batch loss 1.20790696 epoch total loss 1.22629857\n",
      "Trained batch 1325 batch loss 1.27782774 epoch total loss 1.22633743\n",
      "Trained batch 1326 batch loss 1.2098453 epoch total loss 1.22632504\n",
      "Trained batch 1327 batch loss 1.25343335 epoch total loss 1.22634542\n",
      "Trained batch 1328 batch loss 1.25927889 epoch total loss 1.22637022\n",
      "Trained batch 1329 batch loss 1.14891338 epoch total loss 1.22631192\n",
      "Trained batch 1330 batch loss 1.21931922 epoch total loss 1.22630668\n",
      "Trained batch 1331 batch loss 1.19999146 epoch total loss 1.22628689\n",
      "Trained batch 1332 batch loss 1.17800903 epoch total loss 1.22625065\n",
      "Trained batch 1333 batch loss 1.15647876 epoch total loss 1.22619832\n",
      "Trained batch 1334 batch loss 1.13491559 epoch total loss 1.22612989\n",
      "Trained batch 1335 batch loss 1.22587371 epoch total loss 1.22612965\n",
      "Trained batch 1336 batch loss 1.24976897 epoch total loss 1.22614729\n",
      "Trained batch 1337 batch loss 1.49333823 epoch total loss 1.22634709\n",
      "Trained batch 1338 batch loss 1.37667036 epoch total loss 1.2264595\n",
      "Trained batch 1339 batch loss 1.32220817 epoch total loss 1.22653103\n",
      "Trained batch 1340 batch loss 1.15562391 epoch total loss 1.2264781\n",
      "Trained batch 1341 batch loss 1.12504315 epoch total loss 1.22640252\n",
      "Trained batch 1342 batch loss 1.04492247 epoch total loss 1.22626722\n",
      "Trained batch 1343 batch loss 0.969228804 epoch total loss 1.22607589\n",
      "Trained batch 1344 batch loss 1.00621438 epoch total loss 1.22591233\n",
      "Trained batch 1345 batch loss 1.12679029 epoch total loss 1.22583866\n",
      "Trained batch 1346 batch loss 1.33274758 epoch total loss 1.22591805\n",
      "Trained batch 1347 batch loss 1.28819895 epoch total loss 1.22596431\n",
      "Trained batch 1348 batch loss 1.34250116 epoch total loss 1.22605073\n",
      "Trained batch 1349 batch loss 1.17612195 epoch total loss 1.22601378\n",
      "Trained batch 1350 batch loss 1.282794 epoch total loss 1.22605586\n",
      "Trained batch 1351 batch loss 1.2062813 epoch total loss 1.2260412\n",
      "Trained batch 1352 batch loss 1.14290476 epoch total loss 1.2259798\n",
      "Trained batch 1353 batch loss 1.19187903 epoch total loss 1.22595453\n",
      "Trained batch 1354 batch loss 1.35219407 epoch total loss 1.22604775\n",
      "Trained batch 1355 batch loss 1.16151333 epoch total loss 1.22600019\n",
      "Trained batch 1356 batch loss 1.23304617 epoch total loss 1.22600532\n",
      "Trained batch 1357 batch loss 1.24197602 epoch total loss 1.22601712\n",
      "Trained batch 1358 batch loss 1.16695511 epoch total loss 1.22597361\n",
      "Trained batch 1359 batch loss 1.09354472 epoch total loss 1.22587621\n",
      "Trained batch 1360 batch loss 1.17459607 epoch total loss 1.22583842\n",
      "Trained batch 1361 batch loss 1.20425403 epoch total loss 1.22582257\n",
      "Trained batch 1362 batch loss 1.10911775 epoch total loss 1.22573686\n",
      "Trained batch 1363 batch loss 1.11950994 epoch total loss 1.22565889\n",
      "Trained batch 1364 batch loss 1.04961908 epoch total loss 1.22552979\n",
      "Trained batch 1365 batch loss 1.19006276 epoch total loss 1.2255038\n",
      "Trained batch 1366 batch loss 1.32812595 epoch total loss 1.22557902\n",
      "Trained batch 1367 batch loss 1.25138402 epoch total loss 1.22559786\n",
      "Trained batch 1368 batch loss 1.16499496 epoch total loss 1.22555351\n",
      "Trained batch 1369 batch loss 1.2162025 epoch total loss 1.22554672\n",
      "Trained batch 1370 batch loss 1.21491289 epoch total loss 1.22553897\n",
      "Trained batch 1371 batch loss 1.33409429 epoch total loss 1.22561812\n",
      "Trained batch 1372 batch loss 1.23559976 epoch total loss 1.2256254\n",
      "Trained batch 1373 batch loss 1.09780347 epoch total loss 1.22553229\n",
      "Trained batch 1374 batch loss 1.15163803 epoch total loss 1.22547853\n",
      "Trained batch 1375 batch loss 1.1433146 epoch total loss 1.22541881\n",
      "Trained batch 1376 batch loss 1.06483388 epoch total loss 1.2253021\n",
      "Trained batch 1377 batch loss 1.08106327 epoch total loss 1.22519732\n",
      "Trained batch 1378 batch loss 1.18419242 epoch total loss 1.22516751\n",
      "Trained batch 1379 batch loss 1.15871024 epoch total loss 1.22511935\n",
      "Trained batch 1380 batch loss 1.13719404 epoch total loss 1.22505569\n",
      "Trained batch 1381 batch loss 1.11703885 epoch total loss 1.22497749\n",
      "Trained batch 1382 batch loss 1.13627231 epoch total loss 1.22491324\n",
      "Trained batch 1383 batch loss 1.15632176 epoch total loss 1.22486365\n",
      "Trained batch 1384 batch loss 1.22853088 epoch total loss 1.22486627\n",
      "Trained batch 1385 batch loss 1.24860847 epoch total loss 1.22488344\n",
      "Trained batch 1386 batch loss 1.23899102 epoch total loss 1.22489369\n",
      "Trained batch 1387 batch loss 1.17116117 epoch total loss 1.22485495\n",
      "Trained batch 1388 batch loss 1.22145987 epoch total loss 1.22485244\n",
      "Epoch 4 train loss 1.224852442741394\n",
      "Validated batch 1 batch loss 1.14208794\n",
      "Validated batch 2 batch loss 1.10178018\n",
      "Validated batch 3 batch loss 1.15497601\n",
      "Validated batch 4 batch loss 1.12401927\n",
      "Validated batch 5 batch loss 1.1417836\n",
      "Validated batch 6 batch loss 1.24023485\n",
      "Validated batch 7 batch loss 1.22277725\n",
      "Validated batch 8 batch loss 1.19399941\n",
      "Validated batch 9 batch loss 1.19528937\n",
      "Validated batch 10 batch loss 1.22061682\n",
      "Validated batch 11 batch loss 1.16601229\n",
      "Validated batch 12 batch loss 1.19298911\n",
      "Validated batch 13 batch loss 1.1536392\n",
      "Validated batch 14 batch loss 1.14098668\n",
      "Validated batch 15 batch loss 1.20284963\n",
      "Validated batch 16 batch loss 1.20056868\n",
      "Validated batch 17 batch loss 1.34236217\n",
      "Validated batch 18 batch loss 1.26806927\n",
      "Validated batch 19 batch loss 1.09086215\n",
      "Validated batch 20 batch loss 1.2970047\n",
      "Validated batch 21 batch loss 1.17112315\n",
      "Validated batch 22 batch loss 1.12684476\n",
      "Validated batch 23 batch loss 1.23059833\n",
      "Validated batch 24 batch loss 1.33394313\n",
      "Validated batch 25 batch loss 1.27144921\n",
      "Validated batch 26 batch loss 1.17237973\n",
      "Validated batch 27 batch loss 1.17304039\n",
      "Validated batch 28 batch loss 1.15016961\n",
      "Validated batch 29 batch loss 1.25821948\n",
      "Validated batch 30 batch loss 1.28469801\n",
      "Validated batch 31 batch loss 1.08162117\n",
      "Validated batch 32 batch loss 1.23816133\n",
      "Validated batch 33 batch loss 1.19683695\n",
      "Validated batch 34 batch loss 1.2766118\n",
      "Validated batch 35 batch loss 1.23296213\n",
      "Validated batch 36 batch loss 1.23054588\n",
      "Validated batch 37 batch loss 1.16551483\n",
      "Validated batch 38 batch loss 1.16090035\n",
      "Validated batch 39 batch loss 1.19679296\n",
      "Validated batch 40 batch loss 1.1768074\n",
      "Validated batch 41 batch loss 1.24277592\n",
      "Validated batch 42 batch loss 1.2419939\n",
      "Validated batch 43 batch loss 1.47895515\n",
      "Validated batch 44 batch loss 1.24225569\n",
      "Validated batch 45 batch loss 1.22453511\n",
      "Validated batch 46 batch loss 1.10145068\n",
      "Validated batch 47 batch loss 1.1628437\n",
      "Validated batch 48 batch loss 1.16435838\n",
      "Validated batch 49 batch loss 1.21575689\n",
      "Validated batch 50 batch loss 1.08607805\n",
      "Validated batch 51 batch loss 1.19984555\n",
      "Validated batch 52 batch loss 1.21582174\n",
      "Validated batch 53 batch loss 1.26066208\n",
      "Validated batch 54 batch loss 1.31837153\n",
      "Validated batch 55 batch loss 1.25711501\n",
      "Validated batch 56 batch loss 1.21871853\n",
      "Validated batch 57 batch loss 1.25849891\n",
      "Validated batch 58 batch loss 1.37982619\n",
      "Validated batch 59 batch loss 1.29462183\n",
      "Validated batch 60 batch loss 1.28916049\n",
      "Validated batch 61 batch loss 1.28806913\n",
      "Validated batch 62 batch loss 1.31178784\n",
      "Validated batch 63 batch loss 1.30754375\n",
      "Validated batch 64 batch loss 1.09565496\n",
      "Validated batch 65 batch loss 1.19964671\n",
      "Validated batch 66 batch loss 1.29892528\n",
      "Validated batch 67 batch loss 1.25873351\n",
      "Validated batch 68 batch loss 1.18846786\n",
      "Validated batch 69 batch loss 1.22086096\n",
      "Validated batch 70 batch loss 1.21021283\n",
      "Validated batch 71 batch loss 1.2437104\n",
      "Validated batch 72 batch loss 1.14247131\n",
      "Validated batch 73 batch loss 1.17431235\n",
      "Validated batch 74 batch loss 1.19603777\n",
      "Validated batch 75 batch loss 1.30403924\n",
      "Validated batch 76 batch loss 1.29091394\n",
      "Validated batch 77 batch loss 1.32219326\n",
      "Validated batch 78 batch loss 1.2224499\n",
      "Validated batch 79 batch loss 1.15481627\n",
      "Validated batch 80 batch loss 1.25658643\n",
      "Validated batch 81 batch loss 1.20092893\n",
      "Validated batch 82 batch loss 1.24630141\n",
      "Validated batch 83 batch loss 1.33942962\n",
      "Validated batch 84 batch loss 1.2932893\n",
      "Validated batch 85 batch loss 1.23442566\n",
      "Validated batch 86 batch loss 1.38003373\n",
      "Validated batch 87 batch loss 1.09987164\n",
      "Validated batch 88 batch loss 1.2482723\n",
      "Validated batch 89 batch loss 1.05409515\n",
      "Validated batch 90 batch loss 1.11903107\n",
      "Validated batch 91 batch loss 1.31571305\n",
      "Validated batch 92 batch loss 1.10805464\n",
      "Validated batch 93 batch loss 1.09741664\n",
      "Validated batch 94 batch loss 1.18242896\n",
      "Validated batch 95 batch loss 1.27498782\n",
      "Validated batch 96 batch loss 1.12238264\n",
      "Validated batch 97 batch loss 1.28186727\n",
      "Validated batch 98 batch loss 1.34536409\n",
      "Validated batch 99 batch loss 1.08387923\n",
      "Validated batch 100 batch loss 1.18479538\n",
      "Validated batch 101 batch loss 1.20849395\n",
      "Validated batch 102 batch loss 1.2917794\n",
      "Validated batch 103 batch loss 1.26604164\n",
      "Validated batch 104 batch loss 1.1292268\n",
      "Validated batch 105 batch loss 1.0177803\n",
      "Validated batch 106 batch loss 1.15638411\n",
      "Validated batch 107 batch loss 1.11236024\n",
      "Validated batch 108 batch loss 1.16853189\n",
      "Validated batch 109 batch loss 1.2344954\n",
      "Validated batch 110 batch loss 1.06955588\n",
      "Validated batch 111 batch loss 1.19198763\n",
      "Validated batch 112 batch loss 1.27679694\n",
      "Validated batch 113 batch loss 1.2673223\n",
      "Validated batch 114 batch loss 1.21480489\n",
      "Validated batch 115 batch loss 1.07820892\n",
      "Validated batch 116 batch loss 1.26796293\n",
      "Validated batch 117 batch loss 1.25956047\n",
      "Validated batch 118 batch loss 1.12042522\n",
      "Validated batch 119 batch loss 1.162606\n",
      "Validated batch 120 batch loss 1.19988203\n",
      "Validated batch 121 batch loss 1.20834708\n",
      "Validated batch 122 batch loss 1.25818896\n",
      "Validated batch 123 batch loss 1.15953958\n",
      "Validated batch 124 batch loss 1.15097213\n",
      "Validated batch 125 batch loss 1.31664765\n",
      "Validated batch 126 batch loss 1.12729299\n",
      "Validated batch 127 batch loss 1.12069237\n",
      "Validated batch 128 batch loss 1.15799689\n",
      "Validated batch 129 batch loss 1.35450315\n",
      "Validated batch 130 batch loss 1.34144092\n",
      "Validated batch 131 batch loss 1.42188227\n",
      "Validated batch 132 batch loss 1.22120488\n",
      "Validated batch 133 batch loss 1.34858346\n",
      "Validated batch 134 batch loss 1.21401632\n",
      "Validated batch 135 batch loss 1.29346931\n",
      "Validated batch 136 batch loss 1.30269539\n",
      "Validated batch 137 batch loss 0.97663641\n",
      "Validated batch 138 batch loss 1.16267586\n",
      "Validated batch 139 batch loss 1.20788765\n",
      "Validated batch 140 batch loss 1.21951675\n",
      "Validated batch 141 batch loss 1.193753\n",
      "Validated batch 142 batch loss 1.07940948\n",
      "Validated batch 143 batch loss 1.15316808\n",
      "Validated batch 144 batch loss 1.30242395\n",
      "Validated batch 145 batch loss 1.10540199\n",
      "Validated batch 146 batch loss 1.07816505\n",
      "Validated batch 147 batch loss 1.13629389\n",
      "Validated batch 148 batch loss 1.1894\n",
      "Validated batch 149 batch loss 1.04896569\n",
      "Validated batch 150 batch loss 1.22089219\n",
      "Validated batch 151 batch loss 1.07002842\n",
      "Validated batch 152 batch loss 1.20445633\n",
      "Validated batch 153 batch loss 1.28811347\n",
      "Validated batch 154 batch loss 1.33797467\n",
      "Validated batch 155 batch loss 1.14316821\n",
      "Validated batch 156 batch loss 1.31946492\n",
      "Validated batch 157 batch loss 1.02824175\n",
      "Validated batch 158 batch loss 1.06960011\n",
      "Validated batch 159 batch loss 1.18251681\n",
      "Validated batch 160 batch loss 1.15678692\n",
      "Validated batch 161 batch loss 1.30380249\n",
      "Validated batch 162 batch loss 1.25829792\n",
      "Validated batch 163 batch loss 1.23334539\n",
      "Validated batch 164 batch loss 1.21424425\n",
      "Validated batch 165 batch loss 1.18453801\n",
      "Validated batch 166 batch loss 1.28238761\n",
      "Validated batch 167 batch loss 1.40446723\n",
      "Validated batch 168 batch loss 1.18832445\n",
      "Validated batch 169 batch loss 1.26115608\n",
      "Validated batch 170 batch loss 1.16482842\n",
      "Validated batch 171 batch loss 1.28211904\n",
      "Validated batch 172 batch loss 1.21041179\n",
      "Validated batch 173 batch loss 1.22137845\n",
      "Validated batch 174 batch loss 1.26023173\n",
      "Validated batch 175 batch loss 1.30064666\n",
      "Validated batch 176 batch loss 1.24751139\n",
      "Validated batch 177 batch loss 1.25041175\n",
      "Validated batch 178 batch loss 1.28190553\n",
      "Validated batch 179 batch loss 1.19960403\n",
      "Validated batch 180 batch loss 1.13074124\n",
      "Validated batch 181 batch loss 1.23492372\n",
      "Validated batch 182 batch loss 1.14491153\n",
      "Validated batch 183 batch loss 1.20408463\n",
      "Validated batch 184 batch loss 1.25235009\n",
      "Validated batch 185 batch loss 1.34121156\n",
      "Epoch 4 val loss 1.2124340534210205\n",
      "Model /aiffel/aiffel/mpii/models/model-epoch-4-loss-1.2124.h5 saved.\n",
      "Start epoch 5 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.27179694 epoch total loss 1.27179694\n",
      "Trained batch 2 batch loss 1.16702485 epoch total loss 1.2194109\n",
      "Trained batch 3 batch loss 1.25583375 epoch total loss 1.23155177\n",
      "Trained batch 4 batch loss 1.2406199 epoch total loss 1.23381877\n",
      "Trained batch 5 batch loss 1.35679412 epoch total loss 1.25841391\n",
      "Trained batch 6 batch loss 1.2437582 epoch total loss 1.25597131\n",
      "Trained batch 7 batch loss 1.27814496 epoch total loss 1.25913894\n",
      "Trained batch 8 batch loss 1.21539497 epoch total loss 1.25367093\n",
      "Trained batch 9 batch loss 1.20078528 epoch total loss 1.24779475\n",
      "Trained batch 10 batch loss 1.11927438 epoch total loss 1.23494267\n",
      "Trained batch 11 batch loss 1.22505522 epoch total loss 1.23404384\n",
      "Trained batch 12 batch loss 1.15373933 epoch total loss 1.22735178\n",
      "Trained batch 13 batch loss 1.17224991 epoch total loss 1.22311318\n",
      "Trained batch 14 batch loss 1.11126781 epoch total loss 1.21512413\n",
      "Trained batch 15 batch loss 1.09053624 epoch total loss 1.20681822\n",
      "Trained batch 16 batch loss 1.04831147 epoch total loss 1.19691157\n",
      "Trained batch 17 batch loss 1.1252811 epoch total loss 1.192698\n",
      "Trained batch 18 batch loss 1.21771371 epoch total loss 1.19408774\n",
      "Trained batch 19 batch loss 1.21828365 epoch total loss 1.19536126\n",
      "Trained batch 20 batch loss 1.23055458 epoch total loss 1.1971209\n",
      "Trained batch 21 batch loss 1.16653728 epoch total loss 1.19566453\n",
      "Trained batch 22 batch loss 1.28388464 epoch total loss 1.19967449\n",
      "Trained batch 23 batch loss 1.09315968 epoch total loss 1.19504344\n",
      "Trained batch 24 batch loss 1.32946622 epoch total loss 1.20064437\n",
      "Trained batch 25 batch loss 1.289 epoch total loss 1.20417857\n",
      "Trained batch 26 batch loss 1.20450664 epoch total loss 1.20419121\n",
      "Trained batch 27 batch loss 1.16029954 epoch total loss 1.20256567\n",
      "Trained batch 28 batch loss 1.15869236 epoch total loss 1.20099866\n",
      "Trained batch 29 batch loss 1.25935793 epoch total loss 1.20301104\n",
      "Trained batch 30 batch loss 1.32790506 epoch total loss 1.20717418\n",
      "Trained batch 31 batch loss 1.19052219 epoch total loss 1.20663702\n",
      "Trained batch 32 batch loss 0.900759041 epoch total loss 1.19707835\n",
      "Trained batch 33 batch loss 1.16395128 epoch total loss 1.19607449\n",
      "Trained batch 34 batch loss 1.16035879 epoch total loss 1.19502401\n",
      "Trained batch 35 batch loss 1.06805396 epoch total loss 1.19139636\n",
      "Trained batch 36 batch loss 1.11452925 epoch total loss 1.18926108\n",
      "Trained batch 37 batch loss 1.21898496 epoch total loss 1.19006455\n",
      "Trained batch 38 batch loss 1.21478271 epoch total loss 1.19071496\n",
      "Trained batch 39 batch loss 1.27556634 epoch total loss 1.19289064\n",
      "Trained batch 40 batch loss 1.24847484 epoch total loss 1.19428027\n",
      "Trained batch 41 batch loss 1.23163688 epoch total loss 1.19519138\n",
      "Trained batch 42 batch loss 1.43002737 epoch total loss 1.20078266\n",
      "Trained batch 43 batch loss 1.43501842 epoch total loss 1.20623\n",
      "Trained batch 44 batch loss 1.41414309 epoch total loss 1.21095526\n",
      "Trained batch 45 batch loss 1.37508559 epoch total loss 1.21460259\n",
      "Trained batch 46 batch loss 1.28935659 epoch total loss 1.21622765\n",
      "Trained batch 47 batch loss 1.11771011 epoch total loss 1.21413159\n",
      "Trained batch 48 batch loss 1.10596836 epoch total loss 1.21187818\n",
      "Trained batch 49 batch loss 1.19782817 epoch total loss 1.21159136\n",
      "Trained batch 50 batch loss 1.2488327 epoch total loss 1.21233618\n",
      "Trained batch 51 batch loss 1.12876046 epoch total loss 1.21069753\n",
      "Trained batch 52 batch loss 0.983350813 epoch total loss 1.20632541\n",
      "Trained batch 53 batch loss 0.997858286 epoch total loss 1.2023921\n",
      "Trained batch 54 batch loss 1.06851518 epoch total loss 1.19991291\n",
      "Trained batch 55 batch loss 1.16125464 epoch total loss 1.19921\n",
      "Trained batch 56 batch loss 1.07442784 epoch total loss 1.19698167\n",
      "Trained batch 57 batch loss 1.11801505 epoch total loss 1.19559622\n",
      "Trained batch 58 batch loss 1.13343024 epoch total loss 1.19452441\n",
      "Trained batch 59 batch loss 1.21062279 epoch total loss 1.19479728\n",
      "Trained batch 60 batch loss 1.14786041 epoch total loss 1.19401503\n",
      "Trained batch 61 batch loss 1.02297056 epoch total loss 1.19121099\n",
      "Trained batch 62 batch loss 1.22585773 epoch total loss 1.19176984\n",
      "Trained batch 63 batch loss 1.31476 epoch total loss 1.19372213\n",
      "Trained batch 64 batch loss 1.2708298 epoch total loss 1.19492686\n",
      "Trained batch 65 batch loss 1.14337873 epoch total loss 1.19413376\n",
      "Trained batch 66 batch loss 1.28445148 epoch total loss 1.19550228\n",
      "Trained batch 67 batch loss 1.12765 epoch total loss 1.1944896\n",
      "Trained batch 68 batch loss 1.1245954 epoch total loss 1.19346166\n",
      "Trained batch 69 batch loss 1.03184915 epoch total loss 1.19111955\n",
      "Trained batch 70 batch loss 1.02147794 epoch total loss 1.18869603\n",
      "Trained batch 71 batch loss 1.10994887 epoch total loss 1.1875869\n",
      "Trained batch 72 batch loss 1.12580967 epoch total loss 1.18672895\n",
      "Trained batch 73 batch loss 1.14703691 epoch total loss 1.18618512\n",
      "Trained batch 74 batch loss 1.07673502 epoch total loss 1.18470609\n",
      "Trained batch 75 batch loss 1.26457691 epoch total loss 1.18577111\n",
      "Trained batch 76 batch loss 1.34816778 epoch total loss 1.18790781\n",
      "Trained batch 77 batch loss 1.21564293 epoch total loss 1.18826807\n",
      "Trained batch 78 batch loss 1.23925388 epoch total loss 1.18892169\n",
      "Trained batch 79 batch loss 1.13232267 epoch total loss 1.18820524\n",
      "Trained batch 80 batch loss 1.13983727 epoch total loss 1.18760073\n",
      "Trained batch 81 batch loss 1.14951205 epoch total loss 1.18713045\n",
      "Trained batch 82 batch loss 1.21859455 epoch total loss 1.18751419\n",
      "Trained batch 83 batch loss 1.25661373 epoch total loss 1.18834674\n",
      "Trained batch 84 batch loss 1.23497963 epoch total loss 1.1889019\n",
      "Trained batch 85 batch loss 1.23537326 epoch total loss 1.1894486\n",
      "Trained batch 86 batch loss 1.14828753 epoch total loss 1.18897\n",
      "Trained batch 87 batch loss 1.14633131 epoch total loss 1.1884799\n",
      "Trained batch 88 batch loss 1.19987452 epoch total loss 1.18860936\n",
      "Trained batch 89 batch loss 1.20877934 epoch total loss 1.18883598\n",
      "Trained batch 90 batch loss 1.16198862 epoch total loss 1.18853772\n",
      "Trained batch 91 batch loss 1.24700785 epoch total loss 1.18918025\n",
      "Trained batch 92 batch loss 1.28378105 epoch total loss 1.19020855\n",
      "Trained batch 93 batch loss 1.17216468 epoch total loss 1.19001448\n",
      "Trained batch 94 batch loss 1.18803322 epoch total loss 1.18999338\n",
      "Trained batch 95 batch loss 1.04670453 epoch total loss 1.18848515\n",
      "Trained batch 96 batch loss 1.15391576 epoch total loss 1.18812501\n",
      "Trained batch 97 batch loss 1.13860416 epoch total loss 1.18761456\n",
      "Trained batch 98 batch loss 1.0917089 epoch total loss 1.18663585\n",
      "Trained batch 99 batch loss 1.1825881 epoch total loss 1.18659496\n",
      "Trained batch 100 batch loss 1.27821255 epoch total loss 1.18751109\n",
      "Trained batch 101 batch loss 1.36594594 epoch total loss 1.18927777\n",
      "Trained batch 102 batch loss 1.33781648 epoch total loss 1.19073403\n",
      "Trained batch 103 batch loss 1.32870746 epoch total loss 1.19207358\n",
      "Trained batch 104 batch loss 1.1471051 epoch total loss 1.19164109\n",
      "Trained batch 105 batch loss 1.04061806 epoch total loss 1.19020283\n",
      "Trained batch 106 batch loss 0.970053 epoch total loss 1.18812597\n",
      "Trained batch 107 batch loss 0.954619825 epoch total loss 1.18594372\n",
      "Trained batch 108 batch loss 0.940197349 epoch total loss 1.18366826\n",
      "Trained batch 109 batch loss 1.231668 epoch total loss 1.18410861\n",
      "Trained batch 110 batch loss 1.20889795 epoch total loss 1.18433392\n",
      "Trained batch 111 batch loss 1.13827598 epoch total loss 1.18391895\n",
      "Trained batch 112 batch loss 1.13689125 epoch total loss 1.1834991\n",
      "Trained batch 113 batch loss 1.12061334 epoch total loss 1.18294263\n",
      "Trained batch 114 batch loss 1.20341969 epoch total loss 1.18312216\n",
      "Trained batch 115 batch loss 1.20145786 epoch total loss 1.18328166\n",
      "Trained batch 116 batch loss 1.2242105 epoch total loss 1.18363452\n",
      "Trained batch 117 batch loss 1.27805007 epoch total loss 1.18444145\n",
      "Trained batch 118 batch loss 1.25573683 epoch total loss 1.18504572\n",
      "Trained batch 119 batch loss 1.28695548 epoch total loss 1.185902\n",
      "Trained batch 120 batch loss 1.2454437 epoch total loss 1.18639815\n",
      "Trained batch 121 batch loss 1.26079702 epoch total loss 1.18701315\n",
      "Trained batch 122 batch loss 1.36281562 epoch total loss 1.18845403\n",
      "Trained batch 123 batch loss 1.23928118 epoch total loss 1.18886733\n",
      "Trained batch 124 batch loss 1.27029848 epoch total loss 1.18952405\n",
      "Trained batch 125 batch loss 1.13165987 epoch total loss 1.18906105\n",
      "Trained batch 126 batch loss 1.15064096 epoch total loss 1.18875611\n",
      "Trained batch 127 batch loss 1.13442492 epoch total loss 1.18832827\n",
      "Trained batch 128 batch loss 1.03370702 epoch total loss 1.18712032\n",
      "Trained batch 129 batch loss 1.20461929 epoch total loss 1.18725598\n",
      "Trained batch 130 batch loss 1.0509125 epoch total loss 1.18620718\n",
      "Trained batch 131 batch loss 1.04899633 epoch total loss 1.1851598\n",
      "Trained batch 132 batch loss 1.08222473 epoch total loss 1.18438\n",
      "Trained batch 133 batch loss 1.12957728 epoch total loss 1.18396795\n",
      "Trained batch 134 batch loss 1.16360974 epoch total loss 1.18381608\n",
      "Trained batch 135 batch loss 1.20996296 epoch total loss 1.18400967\n",
      "Trained batch 136 batch loss 1.18620074 epoch total loss 1.18402576\n",
      "Trained batch 137 batch loss 1.27839851 epoch total loss 1.18471467\n",
      "Trained batch 138 batch loss 1.2807641 epoch total loss 1.18541062\n",
      "Trained batch 139 batch loss 1.20331287 epoch total loss 1.18553936\n",
      "Trained batch 140 batch loss 1.2529428 epoch total loss 1.18602085\n",
      "Trained batch 141 batch loss 1.20766985 epoch total loss 1.18617439\n",
      "Trained batch 142 batch loss 1.13432193 epoch total loss 1.18580925\n",
      "Trained batch 143 batch loss 1.30529904 epoch total loss 1.18664491\n",
      "Trained batch 144 batch loss 1.25132525 epoch total loss 1.18709409\n",
      "Trained batch 145 batch loss 1.24391162 epoch total loss 1.18748593\n",
      "Trained batch 146 batch loss 1.16281271 epoch total loss 1.18731689\n",
      "Trained batch 147 batch loss 1.18321443 epoch total loss 1.187289\n",
      "Trained batch 148 batch loss 1.29561162 epoch total loss 1.18802083\n",
      "Trained batch 149 batch loss 1.38477826 epoch total loss 1.18934143\n",
      "Trained batch 150 batch loss 1.14489937 epoch total loss 1.18904507\n",
      "Trained batch 151 batch loss 1.23768377 epoch total loss 1.18936718\n",
      "Trained batch 152 batch loss 1.22813714 epoch total loss 1.18962228\n",
      "Trained batch 153 batch loss 1.14176464 epoch total loss 1.18930948\n",
      "Trained batch 154 batch loss 1.42499197 epoch total loss 1.19083989\n",
      "Trained batch 155 batch loss 1.36806011 epoch total loss 1.19198322\n",
      "Trained batch 156 batch loss 1.28704321 epoch total loss 1.19259262\n",
      "Trained batch 157 batch loss 1.3391372 epoch total loss 1.19352603\n",
      "Trained batch 158 batch loss 1.26151419 epoch total loss 1.19395638\n",
      "Trained batch 159 batch loss 1.24347949 epoch total loss 1.19426787\n",
      "Trained batch 160 batch loss 1.20837975 epoch total loss 1.19435608\n",
      "Trained batch 161 batch loss 1.22314095 epoch total loss 1.1945349\n",
      "Trained batch 162 batch loss 1.17351866 epoch total loss 1.1944052\n",
      "Trained batch 163 batch loss 1.25566149 epoch total loss 1.19478095\n",
      "Trained batch 164 batch loss 1.19844925 epoch total loss 1.19480336\n",
      "Trained batch 165 batch loss 1.23401093 epoch total loss 1.19504094\n",
      "Trained batch 166 batch loss 1.19624269 epoch total loss 1.19504821\n",
      "Trained batch 167 batch loss 1.04228258 epoch total loss 1.19413352\n",
      "Trained batch 168 batch loss 1.05078137 epoch total loss 1.19328022\n",
      "Trained batch 169 batch loss 1.10336494 epoch total loss 1.19274807\n",
      "Trained batch 170 batch loss 1.19623578 epoch total loss 1.19276869\n",
      "Trained batch 171 batch loss 1.26144207 epoch total loss 1.19317031\n",
      "Trained batch 172 batch loss 1.38263273 epoch total loss 1.1942718\n",
      "Trained batch 173 batch loss 1.30782413 epoch total loss 1.19492817\n",
      "Trained batch 174 batch loss 1.28867924 epoch total loss 1.195467\n",
      "Trained batch 175 batch loss 1.21228409 epoch total loss 1.19556308\n",
      "Trained batch 176 batch loss 1.22814298 epoch total loss 1.19574821\n",
      "Trained batch 177 batch loss 1.23132122 epoch total loss 1.1959492\n",
      "Trained batch 178 batch loss 1.18094373 epoch total loss 1.19586492\n",
      "Trained batch 179 batch loss 1.06644344 epoch total loss 1.19514179\n",
      "Trained batch 180 batch loss 1.21263945 epoch total loss 1.19523907\n",
      "Trained batch 181 batch loss 1.44237649 epoch total loss 1.19660449\n",
      "Trained batch 182 batch loss 1.27963376 epoch total loss 1.1970607\n",
      "Trained batch 183 batch loss 1.1329906 epoch total loss 1.19671059\n",
      "Trained batch 184 batch loss 1.00459111 epoch total loss 1.19566655\n",
      "Trained batch 185 batch loss 1.07393968 epoch total loss 1.19500852\n",
      "Trained batch 186 batch loss 1.06846058 epoch total loss 1.19432819\n",
      "Trained batch 187 batch loss 1.03212416 epoch total loss 1.19346082\n",
      "Trained batch 188 batch loss 1.11022377 epoch total loss 1.19301808\n",
      "Trained batch 189 batch loss 1.04290044 epoch total loss 1.19222379\n",
      "Trained batch 190 batch loss 1.15722466 epoch total loss 1.19203961\n",
      "Trained batch 191 batch loss 1.07470036 epoch total loss 1.19142532\n",
      "Trained batch 192 batch loss 1.20033348 epoch total loss 1.1914717\n",
      "Trained batch 193 batch loss 1.22948194 epoch total loss 1.19166863\n",
      "Trained batch 194 batch loss 1.24825549 epoch total loss 1.19196033\n",
      "Trained batch 195 batch loss 1.21316874 epoch total loss 1.19206905\n",
      "Trained batch 196 batch loss 1.13182557 epoch total loss 1.19176173\n",
      "Trained batch 197 batch loss 1.12479222 epoch total loss 1.19142175\n",
      "Trained batch 198 batch loss 1.24207425 epoch total loss 1.19167757\n",
      "Trained batch 199 batch loss 1.31105518 epoch total loss 1.19227743\n",
      "Trained batch 200 batch loss 1.16120076 epoch total loss 1.19212198\n",
      "Trained batch 201 batch loss 1.19180679 epoch total loss 1.19212043\n",
      "Trained batch 202 batch loss 1.15254414 epoch total loss 1.19192457\n",
      "Trained batch 203 batch loss 1.15852714 epoch total loss 1.19176\n",
      "Trained batch 204 batch loss 1.2734828 epoch total loss 1.19216061\n",
      "Trained batch 205 batch loss 1.26172614 epoch total loss 1.19249988\n",
      "Trained batch 206 batch loss 1.09655368 epoch total loss 1.19203413\n",
      "Trained batch 207 batch loss 1.15447879 epoch total loss 1.19185269\n",
      "Trained batch 208 batch loss 1.19245291 epoch total loss 1.19185567\n",
      "Trained batch 209 batch loss 1.16954231 epoch total loss 1.19174886\n",
      "Trained batch 210 batch loss 1.21348286 epoch total loss 1.19185233\n",
      "Trained batch 211 batch loss 1.28762746 epoch total loss 1.19230628\n",
      "Trained batch 212 batch loss 1.36948109 epoch total loss 1.19314206\n",
      "Trained batch 213 batch loss 1.36397779 epoch total loss 1.1939441\n",
      "Trained batch 214 batch loss 1.30677819 epoch total loss 1.19447136\n",
      "Trained batch 215 batch loss 1.27411389 epoch total loss 1.19484186\n",
      "Trained batch 216 batch loss 1.10442019 epoch total loss 1.1944232\n",
      "Trained batch 217 batch loss 1.1854887 epoch total loss 1.19438207\n",
      "Trained batch 218 batch loss 1.19488907 epoch total loss 1.19438434\n",
      "Trained batch 219 batch loss 1.20859706 epoch total loss 1.19444919\n",
      "Trained batch 220 batch loss 1.27979636 epoch total loss 1.19483709\n",
      "Trained batch 221 batch loss 1.18940139 epoch total loss 1.19481254\n",
      "Trained batch 222 batch loss 1.38887811 epoch total loss 1.1956867\n",
      "Trained batch 223 batch loss 1.20709419 epoch total loss 1.19573784\n",
      "Trained batch 224 batch loss 1.24146056 epoch total loss 1.19594193\n",
      "Trained batch 225 batch loss 1.34672964 epoch total loss 1.19661212\n",
      "Trained batch 226 batch loss 1.3676368 epoch total loss 1.19736898\n",
      "Trained batch 227 batch loss 1.25565505 epoch total loss 1.19762564\n",
      "Trained batch 228 batch loss 1.18703496 epoch total loss 1.19757926\n",
      "Trained batch 229 batch loss 1.17544985 epoch total loss 1.19748259\n",
      "Trained batch 230 batch loss 1.1441052 epoch total loss 1.19725049\n",
      "Trained batch 231 batch loss 1.20288157 epoch total loss 1.19727492\n",
      "Trained batch 232 batch loss 1.24394345 epoch total loss 1.19747615\n",
      "Trained batch 233 batch loss 1.2108767 epoch total loss 1.19753361\n",
      "Trained batch 234 batch loss 1.20579636 epoch total loss 1.19756901\n",
      "Trained batch 235 batch loss 1.22749209 epoch total loss 1.19769621\n",
      "Trained batch 236 batch loss 1.09447 epoch total loss 1.19725895\n",
      "Trained batch 237 batch loss 1.17885137 epoch total loss 1.19718122\n",
      "Trained batch 238 batch loss 1.18904066 epoch total loss 1.19714701\n",
      "Trained batch 239 batch loss 1.17594612 epoch total loss 1.1970582\n",
      "Trained batch 240 batch loss 1.33710802 epoch total loss 1.19764173\n",
      "Trained batch 241 batch loss 1.31901217 epoch total loss 1.19814527\n",
      "Trained batch 242 batch loss 1.1093502 epoch total loss 1.19777834\n",
      "Trained batch 243 batch loss 1.05628228 epoch total loss 1.19719601\n",
      "Trained batch 244 batch loss 1.11447597 epoch total loss 1.19685698\n",
      "Trained batch 245 batch loss 1.12318993 epoch total loss 1.19655633\n",
      "Trained batch 246 batch loss 1.09460509 epoch total loss 1.19614196\n",
      "Trained batch 247 batch loss 1.27447248 epoch total loss 1.19645905\n",
      "Trained batch 248 batch loss 1.15347672 epoch total loss 1.19628572\n",
      "Trained batch 249 batch loss 1.17298758 epoch total loss 1.19619215\n",
      "Trained batch 250 batch loss 1.30769324 epoch total loss 1.19663811\n",
      "Trained batch 251 batch loss 1.23851502 epoch total loss 1.196805\n",
      "Trained batch 252 batch loss 1.36828899 epoch total loss 1.19748545\n",
      "Trained batch 253 batch loss 1.50380778 epoch total loss 1.19869626\n",
      "Trained batch 254 batch loss 1.42244375 epoch total loss 1.19957709\n",
      "Trained batch 255 batch loss 1.17658055 epoch total loss 1.19948697\n",
      "Trained batch 256 batch loss 1.17532778 epoch total loss 1.19939256\n",
      "Trained batch 257 batch loss 1.25153375 epoch total loss 1.19959545\n",
      "Trained batch 258 batch loss 1.31548619 epoch total loss 1.20004463\n",
      "Trained batch 259 batch loss 1.40229321 epoch total loss 1.20082545\n",
      "Trained batch 260 batch loss 1.19369876 epoch total loss 1.20079803\n",
      "Trained batch 261 batch loss 1.32881975 epoch total loss 1.20128858\n",
      "Trained batch 262 batch loss 1.24770641 epoch total loss 1.20146573\n",
      "Trained batch 263 batch loss 1.20383871 epoch total loss 1.20147479\n",
      "Trained batch 264 batch loss 1.14964545 epoch total loss 1.20127845\n",
      "Trained batch 265 batch loss 1.20572782 epoch total loss 1.20129526\n",
      "Trained batch 266 batch loss 1.28948379 epoch total loss 1.20162678\n",
      "Trained batch 267 batch loss 1.20109844 epoch total loss 1.20162487\n",
      "Trained batch 268 batch loss 1.22599518 epoch total loss 1.20171571\n",
      "Trained batch 269 batch loss 1.14283013 epoch total loss 1.20149684\n",
      "Trained batch 270 batch loss 1.13734651 epoch total loss 1.20125926\n",
      "Trained batch 271 batch loss 1.20359671 epoch total loss 1.20126784\n",
      "Trained batch 272 batch loss 1.34859705 epoch total loss 1.20180953\n",
      "Trained batch 273 batch loss 1.18605042 epoch total loss 1.20175183\n",
      "Trained batch 274 batch loss 1.23640633 epoch total loss 1.20187831\n",
      "Trained batch 275 batch loss 1.17784345 epoch total loss 1.20179105\n",
      "Trained batch 276 batch loss 1.17266655 epoch total loss 1.20168543\n",
      "Trained batch 277 batch loss 1.14429247 epoch total loss 1.20147824\n",
      "Trained batch 278 batch loss 1.14129877 epoch total loss 1.20126176\n",
      "Trained batch 279 batch loss 1.2140553 epoch total loss 1.20130765\n",
      "Trained batch 280 batch loss 1.27042782 epoch total loss 1.20155442\n",
      "Trained batch 281 batch loss 1.21967065 epoch total loss 1.20161891\n",
      "Trained batch 282 batch loss 1.24418283 epoch total loss 1.20176983\n",
      "Trained batch 283 batch loss 1.30509269 epoch total loss 1.20213485\n",
      "Trained batch 284 batch loss 1.22810149 epoch total loss 1.20222628\n",
      "Trained batch 285 batch loss 1.29395413 epoch total loss 1.20254803\n",
      "Trained batch 286 batch loss 1.15433431 epoch total loss 1.20237947\n",
      "Trained batch 287 batch loss 1.14859939 epoch total loss 1.20219207\n",
      "Trained batch 288 batch loss 0.969520748 epoch total loss 1.20138407\n",
      "Trained batch 289 batch loss 1.12474799 epoch total loss 1.20111895\n",
      "Trained batch 290 batch loss 1.21160519 epoch total loss 1.20115519\n",
      "Trained batch 291 batch loss 1.28943467 epoch total loss 1.20145845\n",
      "Trained batch 292 batch loss 1.31378233 epoch total loss 1.20184314\n",
      "Trained batch 293 batch loss 1.32512891 epoch total loss 1.20226395\n",
      "Trained batch 294 batch loss 1.31403899 epoch total loss 1.20264411\n",
      "Trained batch 295 batch loss 1.25578225 epoch total loss 1.20282412\n",
      "Trained batch 296 batch loss 1.23713672 epoch total loss 1.20294\n",
      "Trained batch 297 batch loss 1.21127129 epoch total loss 1.20296812\n",
      "Trained batch 298 batch loss 1.27787983 epoch total loss 1.20321953\n",
      "Trained batch 299 batch loss 1.19544899 epoch total loss 1.20319355\n",
      "Trained batch 300 batch loss 1.2774961 epoch total loss 1.20344114\n",
      "Trained batch 301 batch loss 1.24865568 epoch total loss 1.20359135\n",
      "Trained batch 302 batch loss 1.25610828 epoch total loss 1.20376527\n",
      "Trained batch 303 batch loss 1.21430326 epoch total loss 1.2038\n",
      "Trained batch 304 batch loss 1.20667565 epoch total loss 1.20380938\n",
      "Trained batch 305 batch loss 1.19829917 epoch total loss 1.20379138\n",
      "Trained batch 306 batch loss 1.19581497 epoch total loss 1.20376527\n",
      "Trained batch 307 batch loss 1.05829537 epoch total loss 1.20329142\n",
      "Trained batch 308 batch loss 1.06770861 epoch total loss 1.20285118\n",
      "Trained batch 309 batch loss 0.998843 epoch total loss 1.202191\n",
      "Trained batch 310 batch loss 1.21508741 epoch total loss 1.2022326\n",
      "Trained batch 311 batch loss 1.12198734 epoch total loss 1.20197451\n",
      "Trained batch 312 batch loss 1.33138013 epoch total loss 1.20238936\n",
      "Trained batch 313 batch loss 1.39803815 epoch total loss 1.20301449\n",
      "Trained batch 314 batch loss 1.42594147 epoch total loss 1.20372438\n",
      "Trained batch 315 batch loss 1.24924099 epoch total loss 1.20386887\n",
      "Trained batch 316 batch loss 1.29466546 epoch total loss 1.20415628\n",
      "Trained batch 317 batch loss 1.18763399 epoch total loss 1.20410407\n",
      "Trained batch 318 batch loss 0.991411805 epoch total loss 1.2034353\n",
      "Trained batch 319 batch loss 1.15167749 epoch total loss 1.20327294\n",
      "Trained batch 320 batch loss 1.20104384 epoch total loss 1.20326602\n",
      "Trained batch 321 batch loss 1.23401475 epoch total loss 1.20336187\n",
      "Trained batch 322 batch loss 1.22283959 epoch total loss 1.20342231\n",
      "Trained batch 323 batch loss 1.1626873 epoch total loss 1.20329618\n",
      "Trained batch 324 batch loss 1.22580588 epoch total loss 1.20336568\n",
      "Trained batch 325 batch loss 1.2850256 epoch total loss 1.20361698\n",
      "Trained batch 326 batch loss 1.3152715 epoch total loss 1.20395947\n",
      "Trained batch 327 batch loss 1.20258617 epoch total loss 1.20395517\n",
      "Trained batch 328 batch loss 1.2159555 epoch total loss 1.20399177\n",
      "Trained batch 329 batch loss 1.26541638 epoch total loss 1.20417845\n",
      "Trained batch 330 batch loss 1.07450426 epoch total loss 1.20378542\n",
      "Trained batch 331 batch loss 1.09524632 epoch total loss 1.20345759\n",
      "Trained batch 332 batch loss 1.24822533 epoch total loss 1.20359242\n",
      "Trained batch 333 batch loss 1.20298505 epoch total loss 1.20359051\n",
      "Trained batch 334 batch loss 1.22652376 epoch total loss 1.2036593\n",
      "Trained batch 335 batch loss 1.13084733 epoch total loss 1.20344198\n",
      "Trained batch 336 batch loss 1.16983032 epoch total loss 1.20334184\n",
      "Trained batch 337 batch loss 1.16667318 epoch total loss 1.20323312\n",
      "Trained batch 338 batch loss 1.18849075 epoch total loss 1.20318949\n",
      "Trained batch 339 batch loss 1.13717508 epoch total loss 1.2029947\n",
      "Trained batch 340 batch loss 1.21030796 epoch total loss 1.20301616\n",
      "Trained batch 341 batch loss 1.17294419 epoch total loss 1.20292807\n",
      "Trained batch 342 batch loss 1.20629668 epoch total loss 1.20293784\n",
      "Trained batch 343 batch loss 1.23056436 epoch total loss 1.20301843\n",
      "Trained batch 344 batch loss 1.05283237 epoch total loss 1.20258176\n",
      "Trained batch 345 batch loss 1.13151169 epoch total loss 1.20237577\n",
      "Trained batch 346 batch loss 1.20061159 epoch total loss 1.20237076\n",
      "Trained batch 347 batch loss 1.18676162 epoch total loss 1.2023257\n",
      "Trained batch 348 batch loss 1.26631939 epoch total loss 1.20250964\n",
      "Trained batch 349 batch loss 1.31322217 epoch total loss 1.20282686\n",
      "Trained batch 350 batch loss 1.23032141 epoch total loss 1.20290542\n",
      "Trained batch 351 batch loss 1.1359961 epoch total loss 1.2027148\n",
      "Trained batch 352 batch loss 1.25832415 epoch total loss 1.20287275\n",
      "Trained batch 353 batch loss 1.08467 epoch total loss 1.20253789\n",
      "Trained batch 354 batch loss 1.10217333 epoch total loss 1.20225441\n",
      "Trained batch 355 batch loss 1.1122632 epoch total loss 1.20200098\n",
      "Trained batch 356 batch loss 1.25874305 epoch total loss 1.20216024\n",
      "Trained batch 357 batch loss 1.2705493 epoch total loss 1.20235181\n",
      "Trained batch 358 batch loss 1.1964159 epoch total loss 1.20233524\n",
      "Trained batch 359 batch loss 1.41887259 epoch total loss 1.20293844\n",
      "Trained batch 360 batch loss 1.2065717 epoch total loss 1.20294845\n",
      "Trained batch 361 batch loss 1.18035877 epoch total loss 1.20288599\n",
      "Trained batch 362 batch loss 1.31163108 epoch total loss 1.20318639\n",
      "Trained batch 363 batch loss 1.21940231 epoch total loss 1.20323098\n",
      "Trained batch 364 batch loss 1.24120164 epoch total loss 1.2033354\n",
      "Trained batch 365 batch loss 1.23612404 epoch total loss 1.20342517\n",
      "Trained batch 366 batch loss 1.12472677 epoch total loss 1.20321012\n",
      "Trained batch 367 batch loss 1.06847239 epoch total loss 1.20284307\n",
      "Trained batch 368 batch loss 1.12004304 epoch total loss 1.202618\n",
      "Trained batch 369 batch loss 1.07084632 epoch total loss 1.20226097\n",
      "Trained batch 370 batch loss 1.12213755 epoch total loss 1.20204437\n",
      "Trained batch 371 batch loss 1.22656834 epoch total loss 1.20211041\n",
      "Trained batch 372 batch loss 1.13834751 epoch total loss 1.20193899\n",
      "Trained batch 373 batch loss 1.27920175 epoch total loss 1.20214617\n",
      "Trained batch 374 batch loss 1.42838049 epoch total loss 1.20275104\n",
      "Trained batch 375 batch loss 1.34160829 epoch total loss 1.2031213\n",
      "Trained batch 376 batch loss 1.25168371 epoch total loss 1.20325053\n",
      "Trained batch 377 batch loss 1.12027287 epoch total loss 1.20303035\n",
      "Trained batch 378 batch loss 0.93151021 epoch total loss 1.20231211\n",
      "Trained batch 379 batch loss 1.05717409 epoch total loss 1.20192909\n",
      "Trained batch 380 batch loss 1.12538517 epoch total loss 1.20172775\n",
      "Trained batch 381 batch loss 1.05961859 epoch total loss 1.20135474\n",
      "Trained batch 382 batch loss 0.917966604 epoch total loss 1.2006129\n",
      "Trained batch 383 batch loss 0.900767326 epoch total loss 1.19982994\n",
      "Trained batch 384 batch loss 0.963431954 epoch total loss 1.19921434\n",
      "Trained batch 385 batch loss 1.08732152 epoch total loss 1.19892371\n",
      "Trained batch 386 batch loss 1.11183846 epoch total loss 1.19869816\n",
      "Trained batch 387 batch loss 1.1889869 epoch total loss 1.19867301\n",
      "Trained batch 388 batch loss 1.14537215 epoch total loss 1.19853568\n",
      "Trained batch 389 batch loss 1.21311224 epoch total loss 1.19857323\n",
      "Trained batch 390 batch loss 1.30122113 epoch total loss 1.19883633\n",
      "Trained batch 391 batch loss 1.2715075 epoch total loss 1.19902217\n",
      "Trained batch 392 batch loss 1.12499726 epoch total loss 1.19883335\n",
      "Trained batch 393 batch loss 1.17226827 epoch total loss 1.19876575\n",
      "Trained batch 394 batch loss 1.26246095 epoch total loss 1.1989274\n",
      "Trained batch 395 batch loss 1.12477803 epoch total loss 1.19873977\n",
      "Trained batch 396 batch loss 1.03821051 epoch total loss 1.19833434\n",
      "Trained batch 397 batch loss 1.18323946 epoch total loss 1.19829631\n",
      "Trained batch 398 batch loss 1.13520634 epoch total loss 1.19813776\n",
      "Trained batch 399 batch loss 1.07434082 epoch total loss 1.19782746\n",
      "Trained batch 400 batch loss 0.97297 epoch total loss 1.19726527\n",
      "Trained batch 401 batch loss 0.983730197 epoch total loss 1.19673288\n",
      "Trained batch 402 batch loss 1.0971601 epoch total loss 1.19648516\n",
      "Trained batch 403 batch loss 1.129722 epoch total loss 1.19631946\n",
      "Trained batch 404 batch loss 1.12151241 epoch total loss 1.19613433\n",
      "Trained batch 405 batch loss 1.02296102 epoch total loss 1.19570673\n",
      "Trained batch 406 batch loss 1.11697412 epoch total loss 1.19551277\n",
      "Trained batch 407 batch loss 1.12179375 epoch total loss 1.19533169\n",
      "Trained batch 408 batch loss 1.34158182 epoch total loss 1.19569016\n",
      "Trained batch 409 batch loss 1.17145383 epoch total loss 1.19563091\n",
      "Trained batch 410 batch loss 1.33000016 epoch total loss 1.19595861\n",
      "Trained batch 411 batch loss 1.29604435 epoch total loss 1.19620216\n",
      "Trained batch 412 batch loss 1.20767832 epoch total loss 1.19622993\n",
      "Trained batch 413 batch loss 1.26506019 epoch total loss 1.19639659\n",
      "Trained batch 414 batch loss 1.07492328 epoch total loss 1.19610322\n",
      "Trained batch 415 batch loss 1.07969689 epoch total loss 1.19582272\n",
      "Trained batch 416 batch loss 0.975397587 epoch total loss 1.19529283\n",
      "Trained batch 417 batch loss 0.981847227 epoch total loss 1.19478095\n",
      "Trained batch 418 batch loss 1.09190583 epoch total loss 1.1945349\n",
      "Trained batch 419 batch loss 1.23225856 epoch total loss 1.1946249\n",
      "Trained batch 420 batch loss 1.34814143 epoch total loss 1.19499052\n",
      "Trained batch 421 batch loss 1.27184534 epoch total loss 1.19517303\n",
      "Trained batch 422 batch loss 1.10569012 epoch total loss 1.19496095\n",
      "Trained batch 423 batch loss 1.11202562 epoch total loss 1.19476497\n",
      "Trained batch 424 batch loss 1.18210602 epoch total loss 1.19473505\n",
      "Trained batch 425 batch loss 1.18769836 epoch total loss 1.19471848\n",
      "Trained batch 426 batch loss 1.2300365 epoch total loss 1.19480145\n",
      "Trained batch 427 batch loss 1.19334459 epoch total loss 1.19479811\n",
      "Trained batch 428 batch loss 1.33351219 epoch total loss 1.19512224\n",
      "Trained batch 429 batch loss 1.36415875 epoch total loss 1.19551623\n",
      "Trained batch 430 batch loss 1.21235478 epoch total loss 1.19555533\n",
      "Trained batch 431 batch loss 1.23880661 epoch total loss 1.19565582\n",
      "Trained batch 432 batch loss 1.13256836 epoch total loss 1.19550979\n",
      "Trained batch 433 batch loss 1.06314087 epoch total loss 1.19520402\n",
      "Trained batch 434 batch loss 1.15981579 epoch total loss 1.19512236\n",
      "Trained batch 435 batch loss 1.17396343 epoch total loss 1.19507372\n",
      "Trained batch 436 batch loss 1.20448673 epoch total loss 1.19509518\n",
      "Trained batch 437 batch loss 1.25343406 epoch total loss 1.1952287\n",
      "Trained batch 438 batch loss 1.17938507 epoch total loss 1.19519258\n",
      "Trained batch 439 batch loss 1.29034734 epoch total loss 1.1954093\n",
      "Trained batch 440 batch loss 1.06798959 epoch total loss 1.19511974\n",
      "Trained batch 441 batch loss 1.05252457 epoch total loss 1.19479644\n",
      "Trained batch 442 batch loss 1.11324096 epoch total loss 1.19461179\n",
      "Trained batch 443 batch loss 1.13930571 epoch total loss 1.19448698\n",
      "Trained batch 444 batch loss 1.15857887 epoch total loss 1.19440603\n",
      "Trained batch 445 batch loss 1.30796146 epoch total loss 1.19466126\n",
      "Trained batch 446 batch loss 1.11009037 epoch total loss 1.19447172\n",
      "Trained batch 447 batch loss 1.0075866 epoch total loss 1.19405353\n",
      "Trained batch 448 batch loss 1.1396625 epoch total loss 1.19393218\n",
      "Trained batch 449 batch loss 1.28753555 epoch total loss 1.19414055\n",
      "Trained batch 450 batch loss 1.26717973 epoch total loss 1.1943028\n",
      "Trained batch 451 batch loss 1.14563489 epoch total loss 1.19419491\n",
      "Trained batch 452 batch loss 1.10267973 epoch total loss 1.19399238\n",
      "Trained batch 453 batch loss 0.974201918 epoch total loss 1.19350719\n",
      "Trained batch 454 batch loss 0.85965836 epoch total loss 1.19277191\n",
      "Trained batch 455 batch loss 0.89319694 epoch total loss 1.1921134\n",
      "Trained batch 456 batch loss 1.10985625 epoch total loss 1.19193304\n",
      "Trained batch 457 batch loss 1.22086024 epoch total loss 1.19199646\n",
      "Trained batch 458 batch loss 1.15172076 epoch total loss 1.19190848\n",
      "Trained batch 459 batch loss 1.08338845 epoch total loss 1.19167209\n",
      "Trained batch 460 batch loss 1.04497468 epoch total loss 1.1913532\n",
      "Trained batch 461 batch loss 1.16589105 epoch total loss 1.19129801\n",
      "Trained batch 462 batch loss 1.20539272 epoch total loss 1.19132841\n",
      "Trained batch 463 batch loss 1.22991872 epoch total loss 1.19141173\n",
      "Trained batch 464 batch loss 1.14118445 epoch total loss 1.19130349\n",
      "Trained batch 465 batch loss 1.17554474 epoch total loss 1.19126964\n",
      "Trained batch 466 batch loss 1.16775441 epoch total loss 1.19121909\n",
      "Trained batch 467 batch loss 1.201015 epoch total loss 1.19124\n",
      "Trained batch 468 batch loss 1.21237469 epoch total loss 1.19128525\n",
      "Trained batch 469 batch loss 1.22119284 epoch total loss 1.19134903\n",
      "Trained batch 470 batch loss 1.25539339 epoch total loss 1.19148517\n",
      "Trained batch 471 batch loss 1.23888826 epoch total loss 1.1915859\n",
      "Trained batch 472 batch loss 1.2326088 epoch total loss 1.1916728\n",
      "Trained batch 473 batch loss 1.08742571 epoch total loss 1.19145226\n",
      "Trained batch 474 batch loss 1.07498741 epoch total loss 1.19120669\n",
      "Trained batch 475 batch loss 1.16128862 epoch total loss 1.19114375\n",
      "Trained batch 476 batch loss 1.19941187 epoch total loss 1.19116104\n",
      "Trained batch 477 batch loss 1.26768851 epoch total loss 1.19132149\n",
      "Trained batch 478 batch loss 1.26486278 epoch total loss 1.19147551\n",
      "Trained batch 479 batch loss 1.23899102 epoch total loss 1.19157469\n",
      "Trained batch 480 batch loss 1.25347519 epoch total loss 1.19170368\n",
      "Trained batch 481 batch loss 1.23017359 epoch total loss 1.19178367\n",
      "Trained batch 482 batch loss 1.35146451 epoch total loss 1.19211483\n",
      "Trained batch 483 batch loss 1.19449067 epoch total loss 1.19211984\n",
      "Trained batch 484 batch loss 1.11795831 epoch total loss 1.19196665\n",
      "Trained batch 485 batch loss 1.14728856 epoch total loss 1.1918745\n",
      "Trained batch 486 batch loss 1.0719986 epoch total loss 1.19162786\n",
      "Trained batch 487 batch loss 1.15341115 epoch total loss 1.19154942\n",
      "Trained batch 488 batch loss 0.987120748 epoch total loss 1.19113052\n",
      "Trained batch 489 batch loss 1.08195806 epoch total loss 1.19090724\n",
      "Trained batch 490 batch loss 1.00903249 epoch total loss 1.19053602\n",
      "Trained batch 491 batch loss 0.867023 epoch total loss 1.18987715\n",
      "Trained batch 492 batch loss 0.897423506 epoch total loss 1.18928266\n",
      "Trained batch 493 batch loss 0.878428042 epoch total loss 1.18865216\n",
      "Trained batch 494 batch loss 1.11919916 epoch total loss 1.18851149\n",
      "Trained batch 495 batch loss 1.09749067 epoch total loss 1.18832767\n",
      "Trained batch 496 batch loss 1.10867679 epoch total loss 1.1881671\n",
      "Trained batch 497 batch loss 1.17346096 epoch total loss 1.18813753\n",
      "Trained batch 498 batch loss 1.28572595 epoch total loss 1.18833339\n",
      "Trained batch 499 batch loss 1.1507045 epoch total loss 1.18825793\n",
      "Trained batch 500 batch loss 1.22137547 epoch total loss 1.18832421\n",
      "Trained batch 501 batch loss 1.0814743 epoch total loss 1.18811095\n",
      "Trained batch 502 batch loss 1.13302648 epoch total loss 1.18800128\n",
      "Trained batch 503 batch loss 1.30030131 epoch total loss 1.18822455\n",
      "Trained batch 504 batch loss 1.11775887 epoch total loss 1.18808472\n",
      "Trained batch 505 batch loss 1.16625166 epoch total loss 1.18804145\n",
      "Trained batch 506 batch loss 1.05177641 epoch total loss 1.18777215\n",
      "Trained batch 507 batch loss 1.17281294 epoch total loss 1.18774259\n",
      "Trained batch 508 batch loss 1.21954906 epoch total loss 1.18780518\n",
      "Trained batch 509 batch loss 1.23985171 epoch total loss 1.18790746\n",
      "Trained batch 510 batch loss 1.07423162 epoch total loss 1.18768454\n",
      "Trained batch 511 batch loss 1.17220902 epoch total loss 1.18765426\n",
      "Trained batch 512 batch loss 1.20246518 epoch total loss 1.18768311\n",
      "Trained batch 513 batch loss 1.13906479 epoch total loss 1.18758833\n",
      "Trained batch 514 batch loss 1.19293904 epoch total loss 1.18759871\n",
      "Trained batch 515 batch loss 1.22260833 epoch total loss 1.18766665\n",
      "Trained batch 516 batch loss 1.20567811 epoch total loss 1.18770158\n",
      "Trained batch 517 batch loss 1.07014322 epoch total loss 1.18747413\n",
      "Trained batch 518 batch loss 1.01690936 epoch total loss 1.18714488\n",
      "Trained batch 519 batch loss 1.14680934 epoch total loss 1.18706715\n",
      "Trained batch 520 batch loss 1.06970203 epoch total loss 1.18684137\n",
      "Trained batch 521 batch loss 0.978954673 epoch total loss 1.18644238\n",
      "Trained batch 522 batch loss 0.984171093 epoch total loss 1.18605494\n",
      "Trained batch 523 batch loss 1.01851583 epoch total loss 1.18573451\n",
      "Trained batch 524 batch loss 1.0955838 epoch total loss 1.18556249\n",
      "Trained batch 525 batch loss 1.11400294 epoch total loss 1.18542624\n",
      "Trained batch 526 batch loss 1.07591903 epoch total loss 1.18521798\n",
      "Trained batch 527 batch loss 1.05072737 epoch total loss 1.18496287\n",
      "Trained batch 528 batch loss 1.08988011 epoch total loss 1.18478274\n",
      "Trained batch 529 batch loss 1.24144495 epoch total loss 1.18488991\n",
      "Trained batch 530 batch loss 1.24886394 epoch total loss 1.18501055\n",
      "Trained batch 531 batch loss 1.15308535 epoch total loss 1.18495047\n",
      "Trained batch 532 batch loss 1.00228775 epoch total loss 1.18460703\n",
      "Trained batch 533 batch loss 1.06337 epoch total loss 1.18437958\n",
      "Trained batch 534 batch loss 1.09832823 epoch total loss 1.18421841\n",
      "Trained batch 535 batch loss 1.18267655 epoch total loss 1.18421555\n",
      "Trained batch 536 batch loss 1.15412748 epoch total loss 1.1841594\n",
      "Trained batch 537 batch loss 1.29370606 epoch total loss 1.18436337\n",
      "Trained batch 538 batch loss 1.41771924 epoch total loss 1.18479705\n",
      "Trained batch 539 batch loss 1.28136396 epoch total loss 1.18497622\n",
      "Trained batch 540 batch loss 1.11013925 epoch total loss 1.1848377\n",
      "Trained batch 541 batch loss 1.05437231 epoch total loss 1.18459666\n",
      "Trained batch 542 batch loss 1.16373479 epoch total loss 1.18455815\n",
      "Trained batch 543 batch loss 1.26652312 epoch total loss 1.18470907\n",
      "Trained batch 544 batch loss 1.24830699 epoch total loss 1.18482602\n",
      "Trained batch 545 batch loss 1.2065165 epoch total loss 1.18486583\n",
      "Trained batch 546 batch loss 1.22927475 epoch total loss 1.18494713\n",
      "Trained batch 547 batch loss 1.25894284 epoch total loss 1.18508244\n",
      "Trained batch 548 batch loss 1.21483576 epoch total loss 1.1851368\n",
      "Trained batch 549 batch loss 1.28284264 epoch total loss 1.18531477\n",
      "Trained batch 550 batch loss 1.29142308 epoch total loss 1.18550766\n",
      "Trained batch 551 batch loss 1.33641493 epoch total loss 1.1857816\n",
      "Trained batch 552 batch loss 1.35161817 epoch total loss 1.18608201\n",
      "Trained batch 553 batch loss 1.06798077 epoch total loss 1.1858685\n",
      "Trained batch 554 batch loss 1.27558279 epoch total loss 1.18603039\n",
      "Trained batch 555 batch loss 1.24608374 epoch total loss 1.18613863\n",
      "Trained batch 556 batch loss 1.21439314 epoch total loss 1.18618953\n",
      "Trained batch 557 batch loss 1.19232345 epoch total loss 1.1862005\n",
      "Trained batch 558 batch loss 1.2390734 epoch total loss 1.18629527\n",
      "Trained batch 559 batch loss 1.22934628 epoch total loss 1.18637228\n",
      "Trained batch 560 batch loss 1.16950703 epoch total loss 1.18634224\n",
      "Trained batch 561 batch loss 1.23794091 epoch total loss 1.18643415\n",
      "Trained batch 562 batch loss 1.18442416 epoch total loss 1.18643057\n",
      "Trained batch 563 batch loss 1.15098786 epoch total loss 1.18636763\n",
      "Trained batch 564 batch loss 1.28579164 epoch total loss 1.18654394\n",
      "Trained batch 565 batch loss 1.31023073 epoch total loss 1.18676281\n",
      "Trained batch 566 batch loss 1.28574395 epoch total loss 1.18693769\n",
      "Trained batch 567 batch loss 1.18843055 epoch total loss 1.18694031\n",
      "Trained batch 568 batch loss 1.28160143 epoch total loss 1.18710709\n",
      "Trained batch 569 batch loss 1.20405185 epoch total loss 1.18713677\n",
      "Trained batch 570 batch loss 1.1593914 epoch total loss 1.18708801\n",
      "Trained batch 571 batch loss 1.16739202 epoch total loss 1.18705356\n",
      "Trained batch 572 batch loss 1.23400748 epoch total loss 1.1871357\n",
      "Trained batch 573 batch loss 1.19641864 epoch total loss 1.18715191\n",
      "Trained batch 574 batch loss 1.22954166 epoch total loss 1.18722582\n",
      "Trained batch 575 batch loss 1.23656619 epoch total loss 1.18731153\n",
      "Trained batch 576 batch loss 1.13708377 epoch total loss 1.18722439\n",
      "Trained batch 577 batch loss 1.18245077 epoch total loss 1.18721604\n",
      "Trained batch 578 batch loss 1.01128495 epoch total loss 1.1869117\n",
      "Trained batch 579 batch loss 1.13111639 epoch total loss 1.18681538\n",
      "Trained batch 580 batch loss 1.11035395 epoch total loss 1.18668354\n",
      "Trained batch 581 batch loss 1.14307463 epoch total loss 1.18660843\n",
      "Trained batch 582 batch loss 1.03548455 epoch total loss 1.18634868\n",
      "Trained batch 583 batch loss 1.11604249 epoch total loss 1.18622816\n",
      "Trained batch 584 batch loss 1.1031822 epoch total loss 1.18608594\n",
      "Trained batch 585 batch loss 1.23944271 epoch total loss 1.18617713\n",
      "Trained batch 586 batch loss 0.97034812 epoch total loss 1.18580878\n",
      "Trained batch 587 batch loss 1.02919245 epoch total loss 1.18554199\n",
      "Trained batch 588 batch loss 1.20554924 epoch total loss 1.18557608\n",
      "Trained batch 589 batch loss 1.24266434 epoch total loss 1.185673\n",
      "Trained batch 590 batch loss 1.14673173 epoch total loss 1.18560696\n",
      "Trained batch 591 batch loss 1.24783754 epoch total loss 1.18571234\n",
      "Trained batch 592 batch loss 1.22998011 epoch total loss 1.18578708\n",
      "Trained batch 593 batch loss 1.19307876 epoch total loss 1.18579936\n",
      "Trained batch 594 batch loss 1.09793735 epoch total loss 1.18565142\n",
      "Trained batch 595 batch loss 1.01167238 epoch total loss 1.185359\n",
      "Trained batch 596 batch loss 1.15436459 epoch total loss 1.18530703\n",
      "Trained batch 597 batch loss 1.13950753 epoch total loss 1.18523037\n",
      "Trained batch 598 batch loss 1.27897036 epoch total loss 1.18538713\n",
      "Trained batch 599 batch loss 1.29976153 epoch total loss 1.18557811\n",
      "Trained batch 600 batch loss 1.19713283 epoch total loss 1.1855973\n",
      "Trained batch 601 batch loss 1.29326713 epoch total loss 1.18577647\n",
      "Trained batch 602 batch loss 1.31355143 epoch total loss 1.18598866\n",
      "Trained batch 603 batch loss 1.35115886 epoch total loss 1.18626261\n",
      "Trained batch 604 batch loss 1.27795053 epoch total loss 1.18641436\n",
      "Trained batch 605 batch loss 1.33960795 epoch total loss 1.18666756\n",
      "Trained batch 606 batch loss 1.33769858 epoch total loss 1.18691683\n",
      "Trained batch 607 batch loss 1.24876833 epoch total loss 1.18701875\n",
      "Trained batch 608 batch loss 1.13792241 epoch total loss 1.18693805\n",
      "Trained batch 609 batch loss 1.13401425 epoch total loss 1.18685114\n",
      "Trained batch 610 batch loss 1.23296714 epoch total loss 1.18692672\n",
      "Trained batch 611 batch loss 1.0782057 epoch total loss 1.18674874\n",
      "Trained batch 612 batch loss 1.1455878 epoch total loss 1.18668151\n",
      "Trained batch 613 batch loss 1.13540661 epoch total loss 1.18659794\n",
      "Trained batch 614 batch loss 1.19571102 epoch total loss 1.18661284\n",
      "Trained batch 615 batch loss 1.03307247 epoch total loss 1.1863631\n",
      "Trained batch 616 batch loss 1.18669438 epoch total loss 1.1863637\n",
      "Trained batch 617 batch loss 1.05455673 epoch total loss 1.18615007\n",
      "Trained batch 618 batch loss 1.06557536 epoch total loss 1.18595493\n",
      "Trained batch 619 batch loss 1.05751538 epoch total loss 1.18574739\n",
      "Trained batch 620 batch loss 1.16817284 epoch total loss 1.18571901\n",
      "Trained batch 621 batch loss 1.10235214 epoch total loss 1.18558478\n",
      "Trained batch 622 batch loss 1.2456398 epoch total loss 1.18568146\n",
      "Trained batch 623 batch loss 1.10998297 epoch total loss 1.18555987\n",
      "Trained batch 624 batch loss 1.21864045 epoch total loss 1.18561292\n",
      "Trained batch 625 batch loss 1.28069758 epoch total loss 1.18576503\n",
      "Trained batch 626 batch loss 1.27443385 epoch total loss 1.18590665\n",
      "Trained batch 627 batch loss 1.24818659 epoch total loss 1.18600595\n",
      "Trained batch 628 batch loss 1.25746453 epoch total loss 1.18611968\n",
      "Trained batch 629 batch loss 1.24312747 epoch total loss 1.18621027\n",
      "Trained batch 630 batch loss 1.07091451 epoch total loss 1.18602729\n",
      "Trained batch 631 batch loss 1.1010313 epoch total loss 1.18589258\n",
      "Trained batch 632 batch loss 1.31177592 epoch total loss 1.18609178\n",
      "Trained batch 633 batch loss 1.2068764 epoch total loss 1.18612456\n",
      "Trained batch 634 batch loss 1.24492776 epoch total loss 1.18621731\n",
      "Trained batch 635 batch loss 1.23787284 epoch total loss 1.18629861\n",
      "Trained batch 636 batch loss 1.29797721 epoch total loss 1.1864742\n",
      "Trained batch 637 batch loss 1.26027203 epoch total loss 1.18659008\n",
      "Trained batch 638 batch loss 1.23530674 epoch total loss 1.18666637\n",
      "Trained batch 639 batch loss 1.25219154 epoch total loss 1.18676889\n",
      "Trained batch 640 batch loss 1.30237031 epoch total loss 1.18694949\n",
      "Trained batch 641 batch loss 1.36880422 epoch total loss 1.18723321\n",
      "Trained batch 642 batch loss 1.19579065 epoch total loss 1.18724656\n",
      "Trained batch 643 batch loss 1.19878697 epoch total loss 1.18726444\n",
      "Trained batch 644 batch loss 1.20298624 epoch total loss 1.18728888\n",
      "Trained batch 645 batch loss 1.17566919 epoch total loss 1.18727088\n",
      "Trained batch 646 batch loss 1.20986366 epoch total loss 1.18730581\n",
      "Trained batch 647 batch loss 1.30429947 epoch total loss 1.18748665\n",
      "Trained batch 648 batch loss 1.08854508 epoch total loss 1.18733406\n",
      "Trained batch 649 batch loss 1.19211769 epoch total loss 1.18734145\n",
      "Trained batch 650 batch loss 1.08052492 epoch total loss 1.18717706\n",
      "Trained batch 651 batch loss 1.12973523 epoch total loss 1.18708885\n",
      "Trained batch 652 batch loss 1.15665102 epoch total loss 1.18704224\n",
      "Trained batch 653 batch loss 1.23417807 epoch total loss 1.18711448\n",
      "Trained batch 654 batch loss 1.25315714 epoch total loss 1.18721545\n",
      "Trained batch 655 batch loss 1.09934855 epoch total loss 1.18708134\n",
      "Trained batch 656 batch loss 1.17463887 epoch total loss 1.18706238\n",
      "Trained batch 657 batch loss 1.04655373 epoch total loss 1.18684852\n",
      "Trained batch 658 batch loss 1.01323128 epoch total loss 1.18658471\n",
      "Trained batch 659 batch loss 1.22480679 epoch total loss 1.18664265\n",
      "Trained batch 660 batch loss 1.11774409 epoch total loss 1.18653822\n",
      "Trained batch 661 batch loss 1.0883646 epoch total loss 1.18638968\n",
      "Trained batch 662 batch loss 1.18673515 epoch total loss 1.18639016\n",
      "Trained batch 663 batch loss 1.15918052 epoch total loss 1.18634915\n",
      "Trained batch 664 batch loss 1.09120643 epoch total loss 1.18620586\n",
      "Trained batch 665 batch loss 1.17078161 epoch total loss 1.18618262\n",
      "Trained batch 666 batch loss 1.13256681 epoch total loss 1.18610215\n",
      "Trained batch 667 batch loss 1.15391946 epoch total loss 1.18605387\n",
      "Trained batch 668 batch loss 1.05127037 epoch total loss 1.18585217\n",
      "Trained batch 669 batch loss 1.16913843 epoch total loss 1.18582714\n",
      "Trained batch 670 batch loss 1.05011451 epoch total loss 1.1856246\n",
      "Trained batch 671 batch loss 1.1771096 epoch total loss 1.18561196\n",
      "Trained batch 672 batch loss 1.28346491 epoch total loss 1.18575752\n",
      "Trained batch 673 batch loss 1.24200153 epoch total loss 1.18584108\n",
      "Trained batch 674 batch loss 1.19850099 epoch total loss 1.1858598\n",
      "Trained batch 675 batch loss 1.37876928 epoch total loss 1.18614566\n",
      "Trained batch 676 batch loss 1.34551716 epoch total loss 1.18638146\n",
      "Trained batch 677 batch loss 1.22021437 epoch total loss 1.18643141\n",
      "Trained batch 678 batch loss 1.31683981 epoch total loss 1.18662369\n",
      "Trained batch 679 batch loss 1.20496333 epoch total loss 1.18665075\n",
      "Trained batch 680 batch loss 1.15835011 epoch total loss 1.18660903\n",
      "Trained batch 681 batch loss 1.12057686 epoch total loss 1.18651211\n",
      "Trained batch 682 batch loss 1.37234688 epoch total loss 1.18678463\n",
      "Trained batch 683 batch loss 1.35383153 epoch total loss 1.18702924\n",
      "Trained batch 684 batch loss 1.20956576 epoch total loss 1.18706226\n",
      "Trained batch 685 batch loss 1.25202155 epoch total loss 1.18715703\n",
      "Trained batch 686 batch loss 1.25610399 epoch total loss 1.18725753\n",
      "Trained batch 687 batch loss 1.20805573 epoch total loss 1.18728781\n",
      "Trained batch 688 batch loss 1.24115503 epoch total loss 1.18736613\n",
      "Trained batch 689 batch loss 1.28020477 epoch total loss 1.18750083\n",
      "Trained batch 690 batch loss 1.337538 epoch total loss 1.18771827\n",
      "Trained batch 691 batch loss 1.32317233 epoch total loss 1.18791437\n",
      "Trained batch 692 batch loss 1.08577359 epoch total loss 1.18776667\n",
      "Trained batch 693 batch loss 1.10237217 epoch total loss 1.18764353\n",
      "Trained batch 694 batch loss 1.05805504 epoch total loss 1.18745673\n",
      "Trained batch 695 batch loss 1.2005024 epoch total loss 1.18747544\n",
      "Trained batch 696 batch loss 1.26407719 epoch total loss 1.18758559\n",
      "Trained batch 697 batch loss 1.42926335 epoch total loss 1.18793237\n",
      "Trained batch 698 batch loss 1.07574332 epoch total loss 1.18777156\n",
      "Trained batch 699 batch loss 1.02632833 epoch total loss 1.18754065\n",
      "Trained batch 700 batch loss 0.983986378 epoch total loss 1.1872499\n",
      "Trained batch 701 batch loss 1.1260587 epoch total loss 1.18716252\n",
      "Trained batch 702 batch loss 1.27061772 epoch total loss 1.18728137\n",
      "Trained batch 703 batch loss 1.15955734 epoch total loss 1.18724191\n",
      "Trained batch 704 batch loss 1.30593061 epoch total loss 1.18741047\n",
      "Trained batch 705 batch loss 1.24266076 epoch total loss 1.18748891\n",
      "Trained batch 706 batch loss 1.25832081 epoch total loss 1.18758917\n",
      "Trained batch 707 batch loss 1.11607933 epoch total loss 1.18748808\n",
      "Trained batch 708 batch loss 1.18770182 epoch total loss 1.18748832\n",
      "Trained batch 709 batch loss 1.14641762 epoch total loss 1.1874305\n",
      "Trained batch 710 batch loss 1.39426792 epoch total loss 1.18772185\n",
      "Trained batch 711 batch loss 1.25159514 epoch total loss 1.18781161\n",
      "Trained batch 712 batch loss 1.25351202 epoch total loss 1.18790388\n",
      "Trained batch 713 batch loss 1.09707081 epoch total loss 1.18777645\n",
      "Trained batch 714 batch loss 1.03227425 epoch total loss 1.18755877\n",
      "Trained batch 715 batch loss 1.15329254 epoch total loss 1.18751085\n",
      "Trained batch 716 batch loss 1.15831614 epoch total loss 1.18747008\n",
      "Trained batch 717 batch loss 1.22280478 epoch total loss 1.18751931\n",
      "Trained batch 718 batch loss 1.21629596 epoch total loss 1.18755937\n",
      "Trained batch 719 batch loss 1.20116901 epoch total loss 1.18757832\n",
      "Trained batch 720 batch loss 1.25207758 epoch total loss 1.18766797\n",
      "Trained batch 721 batch loss 1.27662206 epoch total loss 1.18779135\n",
      "Trained batch 722 batch loss 1.22278821 epoch total loss 1.18783975\n",
      "Trained batch 723 batch loss 1.31532931 epoch total loss 1.18801606\n",
      "Trained batch 724 batch loss 1.30381608 epoch total loss 1.18817604\n",
      "Trained batch 725 batch loss 1.34389544 epoch total loss 1.18839073\n",
      "Trained batch 726 batch loss 1.31765676 epoch total loss 1.18856883\n",
      "Trained batch 727 batch loss 1.17401206 epoch total loss 1.1885488\n",
      "Trained batch 728 batch loss 1.23107862 epoch total loss 1.18860722\n",
      "Trained batch 729 batch loss 1.30820858 epoch total loss 1.18877125\n",
      "Trained batch 730 batch loss 1.1553843 epoch total loss 1.18872559\n",
      "Trained batch 731 batch loss 1.25456429 epoch total loss 1.18881559\n",
      "Trained batch 732 batch loss 1.11985278 epoch total loss 1.18872142\n",
      "Trained batch 733 batch loss 1.12195182 epoch total loss 1.18863034\n",
      "Trained batch 734 batch loss 1.16948438 epoch total loss 1.18860424\n",
      "Trained batch 735 batch loss 1.07629085 epoch total loss 1.18845153\n",
      "Trained batch 736 batch loss 1.17434907 epoch total loss 1.18843234\n",
      "Trained batch 737 batch loss 1.18097115 epoch total loss 1.1884222\n",
      "Trained batch 738 batch loss 1.14645267 epoch total loss 1.18836534\n",
      "Trained batch 739 batch loss 1.14851332 epoch total loss 1.18831134\n",
      "Trained batch 740 batch loss 1.23861802 epoch total loss 1.18837941\n",
      "Trained batch 741 batch loss 1.21391535 epoch total loss 1.18841386\n",
      "Trained batch 742 batch loss 1.23067617 epoch total loss 1.18847084\n",
      "Trained batch 743 batch loss 1.24973059 epoch total loss 1.18855333\n",
      "Trained batch 744 batch loss 1.08050299 epoch total loss 1.18840802\n",
      "Trained batch 745 batch loss 1.1176343 epoch total loss 1.18831301\n",
      "Trained batch 746 batch loss 1.04144454 epoch total loss 1.18811619\n",
      "Trained batch 747 batch loss 1.04100025 epoch total loss 1.18791926\n",
      "Trained batch 748 batch loss 1.07986069 epoch total loss 1.18777478\n",
      "Trained batch 749 batch loss 1.10690677 epoch total loss 1.18766677\n",
      "Trained batch 750 batch loss 1.08579803 epoch total loss 1.18753099\n",
      "Trained batch 751 batch loss 1.11269724 epoch total loss 1.18743134\n",
      "Trained batch 752 batch loss 1.08848441 epoch total loss 1.18729973\n",
      "Trained batch 753 batch loss 1.13235092 epoch total loss 1.18722677\n",
      "Trained batch 754 batch loss 1.00700808 epoch total loss 1.18698776\n",
      "Trained batch 755 batch loss 1.09590721 epoch total loss 1.18686712\n",
      "Trained batch 756 batch loss 1.26500916 epoch total loss 1.18697047\n",
      "Trained batch 757 batch loss 1.32384562 epoch total loss 1.18715131\n",
      "Trained batch 758 batch loss 1.26482022 epoch total loss 1.18725371\n",
      "Trained batch 759 batch loss 1.11502349 epoch total loss 1.18715858\n",
      "Trained batch 760 batch loss 1.1229198 epoch total loss 1.18707407\n",
      "Trained batch 761 batch loss 1.29739618 epoch total loss 1.18721914\n",
      "Trained batch 762 batch loss 1.34417748 epoch total loss 1.18742514\n",
      "Trained batch 763 batch loss 1.37461078 epoch total loss 1.18767047\n",
      "Trained batch 764 batch loss 1.3580488 epoch total loss 1.18789351\n",
      "Trained batch 765 batch loss 1.31573892 epoch total loss 1.18806052\n",
      "Trained batch 766 batch loss 1.24659884 epoch total loss 1.18813694\n",
      "Trained batch 767 batch loss 1.24911642 epoch total loss 1.18821645\n",
      "Trained batch 768 batch loss 1.09156561 epoch total loss 1.18809068\n",
      "Trained batch 769 batch loss 1.08957899 epoch total loss 1.18796253\n",
      "Trained batch 770 batch loss 1.21438491 epoch total loss 1.18799686\n",
      "Trained batch 771 batch loss 1.32050896 epoch total loss 1.18816864\n",
      "Trained batch 772 batch loss 1.18558836 epoch total loss 1.18816543\n",
      "Trained batch 773 batch loss 1.22302973 epoch total loss 1.18821049\n",
      "Trained batch 774 batch loss 1.06514621 epoch total loss 1.18805146\n",
      "Trained batch 775 batch loss 1.13330221 epoch total loss 1.18798077\n",
      "Trained batch 776 batch loss 1.00978851 epoch total loss 1.18775117\n",
      "Trained batch 777 batch loss 1.1652329 epoch total loss 1.18772209\n",
      "Trained batch 778 batch loss 1.175367 epoch total loss 1.18770623\n",
      "Trained batch 779 batch loss 1.06423235 epoch total loss 1.18754768\n",
      "Trained batch 780 batch loss 1.11957443 epoch total loss 1.18746054\n",
      "Trained batch 781 batch loss 1.29829669 epoch total loss 1.1876024\n",
      "Trained batch 782 batch loss 1.13615417 epoch total loss 1.18753672\n",
      "Trained batch 783 batch loss 1.26474643 epoch total loss 1.1876353\n",
      "Trained batch 784 batch loss 1.22635162 epoch total loss 1.18768477\n",
      "Trained batch 785 batch loss 1.35736489 epoch total loss 1.1879009\n",
      "Trained batch 786 batch loss 1.41509056 epoch total loss 1.18819\n",
      "Trained batch 787 batch loss 1.37403369 epoch total loss 1.18842602\n",
      "Trained batch 788 batch loss 1.25501847 epoch total loss 1.18851054\n",
      "Trained batch 789 batch loss 1.17048824 epoch total loss 1.18848765\n",
      "Trained batch 790 batch loss 1.15969586 epoch total loss 1.18845117\n",
      "Trained batch 791 batch loss 1.18276572 epoch total loss 1.18844402\n",
      "Trained batch 792 batch loss 1.20146024 epoch total loss 1.18846047\n",
      "Trained batch 793 batch loss 1.38671088 epoch total loss 1.18871045\n",
      "Trained batch 794 batch loss 1.2616452 epoch total loss 1.18880236\n",
      "Trained batch 795 batch loss 1.38208723 epoch total loss 1.18904543\n",
      "Trained batch 796 batch loss 1.40325117 epoch total loss 1.1893146\n",
      "Trained batch 797 batch loss 1.35761154 epoch total loss 1.18952572\n",
      "Trained batch 798 batch loss 1.35464358 epoch total loss 1.18973255\n",
      "Trained batch 799 batch loss 1.31563091 epoch total loss 1.18989015\n",
      "Trained batch 800 batch loss 1.31941748 epoch total loss 1.19005203\n",
      "Trained batch 801 batch loss 1.3275032 epoch total loss 1.19022369\n",
      "Trained batch 802 batch loss 1.28316438 epoch total loss 1.19033945\n",
      "Trained batch 803 batch loss 1.23514366 epoch total loss 1.19039536\n",
      "Trained batch 804 batch loss 1.1944437 epoch total loss 1.19040036\n",
      "Trained batch 805 batch loss 1.1357857 epoch total loss 1.19033253\n",
      "Trained batch 806 batch loss 1.15648496 epoch total loss 1.19029057\n",
      "Trained batch 807 batch loss 1.16174126 epoch total loss 1.19025517\n",
      "Trained batch 808 batch loss 1.23035729 epoch total loss 1.19030488\n",
      "Trained batch 809 batch loss 1.18770254 epoch total loss 1.19030154\n",
      "Trained batch 810 batch loss 1.10669899 epoch total loss 1.1901983\n",
      "Trained batch 811 batch loss 1.08874869 epoch total loss 1.19007325\n",
      "Trained batch 812 batch loss 0.956607401 epoch total loss 1.18978572\n",
      "Trained batch 813 batch loss 0.940268278 epoch total loss 1.18947875\n",
      "Trained batch 814 batch loss 1.28768873 epoch total loss 1.18959939\n",
      "Trained batch 815 batch loss 1.18469632 epoch total loss 1.18959343\n",
      "Trained batch 816 batch loss 1.2569207 epoch total loss 1.18967593\n",
      "Trained batch 817 batch loss 1.1714313 epoch total loss 1.18965352\n",
      "Trained batch 818 batch loss 1.16771126 epoch total loss 1.18962681\n",
      "Trained batch 819 batch loss 0.995611668 epoch total loss 1.18938982\n",
      "Trained batch 820 batch loss 1.13498235 epoch total loss 1.18932354\n",
      "Trained batch 821 batch loss 1.07314491 epoch total loss 1.18918204\n",
      "Trained batch 822 batch loss 1.22625804 epoch total loss 1.1892271\n",
      "Trained batch 823 batch loss 1.16192925 epoch total loss 1.18919396\n",
      "Trained batch 824 batch loss 1.07298326 epoch total loss 1.18905294\n",
      "Trained batch 825 batch loss 1.27609611 epoch total loss 1.18915844\n",
      "Trained batch 826 batch loss 1.25106955 epoch total loss 1.18923342\n",
      "Trained batch 827 batch loss 1.29619038 epoch total loss 1.18936276\n",
      "Trained batch 828 batch loss 1.18135357 epoch total loss 1.18935311\n",
      "Trained batch 829 batch loss 1.23440695 epoch total loss 1.18940747\n",
      "Trained batch 830 batch loss 1.25723505 epoch total loss 1.18948925\n",
      "Trained batch 831 batch loss 1.23649859 epoch total loss 1.18954587\n",
      "Trained batch 832 batch loss 1.33017647 epoch total loss 1.18971491\n",
      "Trained batch 833 batch loss 1.228935 epoch total loss 1.189762\n",
      "Trained batch 834 batch loss 1.16921282 epoch total loss 1.18973732\n",
      "Trained batch 835 batch loss 1.21816647 epoch total loss 1.18977129\n",
      "Trained batch 836 batch loss 1.36025381 epoch total loss 1.18997526\n",
      "Trained batch 837 batch loss 1.39264941 epoch total loss 1.19021738\n",
      "Trained batch 838 batch loss 1.2173152 epoch total loss 1.19024968\n",
      "Trained batch 839 batch loss 1.16795039 epoch total loss 1.1902231\n",
      "Trained batch 840 batch loss 1.08820844 epoch total loss 1.19010162\n",
      "Trained batch 841 batch loss 1.04541218 epoch total loss 1.1899296\n",
      "Trained batch 842 batch loss 1.16974425 epoch total loss 1.18990564\n",
      "Trained batch 843 batch loss 1.15754843 epoch total loss 1.18986714\n",
      "Trained batch 844 batch loss 1.19519 epoch total loss 1.18987346\n",
      "Trained batch 845 batch loss 1.26322222 epoch total loss 1.18996036\n",
      "Trained batch 846 batch loss 1.4169755 epoch total loss 1.1902287\n",
      "Trained batch 847 batch loss 1.3928957 epoch total loss 1.19046795\n",
      "Trained batch 848 batch loss 1.20954466 epoch total loss 1.19049048\n",
      "Trained batch 849 batch loss 1.05780566 epoch total loss 1.1903342\n",
      "Trained batch 850 batch loss 1.18443096 epoch total loss 1.19032717\n",
      "Trained batch 851 batch loss 1.15999615 epoch total loss 1.19029152\n",
      "Trained batch 852 batch loss 1.33938396 epoch total loss 1.19046652\n",
      "Trained batch 853 batch loss 1.27698326 epoch total loss 1.19056797\n",
      "Trained batch 854 batch loss 1.22957683 epoch total loss 1.19061363\n",
      "Trained batch 855 batch loss 1.2451756 epoch total loss 1.1906774\n",
      "Trained batch 856 batch loss 1.16763783 epoch total loss 1.19065046\n",
      "Trained batch 857 batch loss 1.1692754 epoch total loss 1.19062555\n",
      "Trained batch 858 batch loss 1.30042958 epoch total loss 1.19075346\n",
      "Trained batch 859 batch loss 1.21003366 epoch total loss 1.19077599\n",
      "Trained batch 860 batch loss 1.16391015 epoch total loss 1.19074464\n",
      "Trained batch 861 batch loss 1.16363955 epoch total loss 1.19071329\n",
      "Trained batch 862 batch loss 1.21034372 epoch total loss 1.19073606\n",
      "Trained batch 863 batch loss 1.18378055 epoch total loss 1.19072795\n",
      "Trained batch 864 batch loss 1.2319299 epoch total loss 1.19077575\n",
      "Trained batch 865 batch loss 1.24291587 epoch total loss 1.19083595\n",
      "Trained batch 866 batch loss 1.29535437 epoch total loss 1.19095671\n",
      "Trained batch 867 batch loss 1.16224086 epoch total loss 1.19092357\n",
      "Trained batch 868 batch loss 1.26373649 epoch total loss 1.19100749\n",
      "Trained batch 869 batch loss 1.27062917 epoch total loss 1.19109917\n",
      "Trained batch 870 batch loss 1.15696383 epoch total loss 1.19106\n",
      "Trained batch 871 batch loss 1.18615746 epoch total loss 1.19105434\n",
      "Trained batch 872 batch loss 1.28651583 epoch total loss 1.19116378\n",
      "Trained batch 873 batch loss 1.31704378 epoch total loss 1.1913079\n",
      "Trained batch 874 batch loss 1.40710676 epoch total loss 1.1915549\n",
      "Trained batch 875 batch loss 1.21328354 epoch total loss 1.1915797\n",
      "Trained batch 876 batch loss 1.21959162 epoch total loss 1.19161165\n",
      "Trained batch 877 batch loss 1.17210627 epoch total loss 1.19158947\n",
      "Trained batch 878 batch loss 1.26975298 epoch total loss 1.19167852\n",
      "Trained batch 879 batch loss 1.24526632 epoch total loss 1.19173944\n",
      "Trained batch 880 batch loss 1.41513991 epoch total loss 1.19199336\n",
      "Trained batch 881 batch loss 1.43902624 epoch total loss 1.19227374\n",
      "Trained batch 882 batch loss 1.25898087 epoch total loss 1.19234943\n",
      "Trained batch 883 batch loss 1.34624803 epoch total loss 1.19252372\n",
      "Trained batch 884 batch loss 1.17373109 epoch total loss 1.19250238\n",
      "Trained batch 885 batch loss 1.32521415 epoch total loss 1.19265234\n",
      "Trained batch 886 batch loss 1.27423823 epoch total loss 1.19274449\n",
      "Trained batch 887 batch loss 1.2370435 epoch total loss 1.19279444\n",
      "Trained batch 888 batch loss 1.16556656 epoch total loss 1.19276369\n",
      "Trained batch 889 batch loss 1.09090066 epoch total loss 1.19264925\n",
      "Trained batch 890 batch loss 1.15011179 epoch total loss 1.19260144\n",
      "Trained batch 891 batch loss 1.06013465 epoch total loss 1.19245279\n",
      "Trained batch 892 batch loss 1.21207666 epoch total loss 1.19247472\n",
      "Trained batch 893 batch loss 1.239663 epoch total loss 1.19252753\n",
      "Trained batch 894 batch loss 1.11719275 epoch total loss 1.19244325\n",
      "Trained batch 895 batch loss 1.09292257 epoch total loss 1.19233203\n",
      "Trained batch 896 batch loss 1.17350292 epoch total loss 1.19231105\n",
      "Trained batch 897 batch loss 1.15128541 epoch total loss 1.19226527\n",
      "Trained batch 898 batch loss 1.12805247 epoch total loss 1.19219375\n",
      "Trained batch 899 batch loss 1.19227445 epoch total loss 1.19219375\n",
      "Trained batch 900 batch loss 1.23272967 epoch total loss 1.19223893\n",
      "Trained batch 901 batch loss 1.14036322 epoch total loss 1.19218135\n",
      "Trained batch 902 batch loss 1.14569449 epoch total loss 1.19212985\n",
      "Trained batch 903 batch loss 1.1521399 epoch total loss 1.1920855\n",
      "Trained batch 904 batch loss 1.24631321 epoch total loss 1.19214559\n",
      "Trained batch 905 batch loss 1.27045846 epoch total loss 1.19223213\n",
      "Trained batch 906 batch loss 1.12530696 epoch total loss 1.19215834\n",
      "Trained batch 907 batch loss 1.24394655 epoch total loss 1.19221544\n",
      "Trained batch 908 batch loss 1.23789 epoch total loss 1.19226575\n",
      "Trained batch 909 batch loss 1.10086513 epoch total loss 1.19216514\n",
      "Trained batch 910 batch loss 1.18760848 epoch total loss 1.19216013\n",
      "Trained batch 911 batch loss 1.19645178 epoch total loss 1.19216478\n",
      "Trained batch 912 batch loss 1.16558814 epoch total loss 1.19213557\n",
      "Trained batch 913 batch loss 1.04028475 epoch total loss 1.19196928\n",
      "Trained batch 914 batch loss 1.0338577 epoch total loss 1.19179618\n",
      "Trained batch 915 batch loss 1.27048779 epoch total loss 1.19188225\n",
      "Trained batch 916 batch loss 1.25803256 epoch total loss 1.19195449\n",
      "Trained batch 917 batch loss 1.27295911 epoch total loss 1.19204283\n",
      "Trained batch 918 batch loss 1.17773795 epoch total loss 1.19202721\n",
      "Trained batch 919 batch loss 1.21360958 epoch total loss 1.1920507\n",
      "Trained batch 920 batch loss 1.19462276 epoch total loss 1.19205344\n",
      "Trained batch 921 batch loss 1.23443806 epoch total loss 1.19209957\n",
      "Trained batch 922 batch loss 1.20028782 epoch total loss 1.19210851\n",
      "Trained batch 923 batch loss 1.08896744 epoch total loss 1.19199681\n",
      "Trained batch 924 batch loss 1.16568887 epoch total loss 1.1919682\n",
      "Trained batch 925 batch loss 1.16154552 epoch total loss 1.1919353\n",
      "Trained batch 926 batch loss 1.01372516 epoch total loss 1.19174278\n",
      "Trained batch 927 batch loss 1.08293223 epoch total loss 1.19162536\n",
      "Trained batch 928 batch loss 1.15620446 epoch total loss 1.19158721\n",
      "Trained batch 929 batch loss 1.05972886 epoch total loss 1.19144523\n",
      "Trained batch 930 batch loss 1.1180793 epoch total loss 1.19136631\n",
      "Trained batch 931 batch loss 1.0812583 epoch total loss 1.19124818\n",
      "Trained batch 932 batch loss 1.14785266 epoch total loss 1.19120157\n",
      "Trained batch 933 batch loss 1.06177831 epoch total loss 1.19106281\n",
      "Trained batch 934 batch loss 1.10009611 epoch total loss 1.19096541\n",
      "Trained batch 935 batch loss 1.14622092 epoch total loss 1.19091761\n",
      "Trained batch 936 batch loss 1.19822812 epoch total loss 1.19092536\n",
      "Trained batch 937 batch loss 1.28626442 epoch total loss 1.19102716\n",
      "Trained batch 938 batch loss 1.20634568 epoch total loss 1.19104338\n",
      "Trained batch 939 batch loss 1.13980508 epoch total loss 1.19098878\n",
      "Trained batch 940 batch loss 1.16401649 epoch total loss 1.19096017\n",
      "Trained batch 941 batch loss 1.21395159 epoch total loss 1.19098461\n",
      "Trained batch 942 batch loss 1.19345224 epoch total loss 1.19098735\n",
      "Trained batch 943 batch loss 1.15732372 epoch total loss 1.19095159\n",
      "Trained batch 944 batch loss 1.01801443 epoch total loss 1.19076848\n",
      "Trained batch 945 batch loss 1.1437515 epoch total loss 1.19071877\n",
      "Trained batch 946 batch loss 1.09016752 epoch total loss 1.19061255\n",
      "Trained batch 947 batch loss 1.26152217 epoch total loss 1.19068742\n",
      "Trained batch 948 batch loss 1.20189357 epoch total loss 1.19069922\n",
      "Trained batch 949 batch loss 1.16647768 epoch total loss 1.19067371\n",
      "Trained batch 950 batch loss 1.03263783 epoch total loss 1.19050729\n",
      "Trained batch 951 batch loss 1.02018356 epoch total loss 1.19032812\n",
      "Trained batch 952 batch loss 0.89047 epoch total loss 1.19001317\n",
      "Trained batch 953 batch loss 0.944194198 epoch total loss 1.18975532\n",
      "Trained batch 954 batch loss 1.05018592 epoch total loss 1.18960893\n",
      "Trained batch 955 batch loss 1.17075944 epoch total loss 1.18958926\n",
      "Trained batch 956 batch loss 1.23657703 epoch total loss 1.18963838\n",
      "Trained batch 957 batch loss 1.21329904 epoch total loss 1.18966305\n",
      "Trained batch 958 batch loss 1.27341402 epoch total loss 1.18975055\n",
      "Trained batch 959 batch loss 1.22364533 epoch total loss 1.18978584\n",
      "Trained batch 960 batch loss 1.24914908 epoch total loss 1.18984771\n",
      "Trained batch 961 batch loss 1.29665899 epoch total loss 1.18995881\n",
      "Trained batch 962 batch loss 1.20221114 epoch total loss 1.18997157\n",
      "Trained batch 963 batch loss 1.2520237 epoch total loss 1.19003606\n",
      "Trained batch 964 batch loss 1.15908909 epoch total loss 1.19000399\n",
      "Trained batch 965 batch loss 1.18195975 epoch total loss 1.18999565\n",
      "Trained batch 966 batch loss 1.23472559 epoch total loss 1.19004202\n",
      "Trained batch 967 batch loss 1.36812699 epoch total loss 1.1902262\n",
      "Trained batch 968 batch loss 1.09272695 epoch total loss 1.19012547\n",
      "Trained batch 969 batch loss 1.11216843 epoch total loss 1.19004512\n",
      "Trained batch 970 batch loss 0.992805481 epoch total loss 1.18984175\n",
      "Trained batch 971 batch loss 1.04697156 epoch total loss 1.18969464\n",
      "Trained batch 972 batch loss 1.0739634 epoch total loss 1.18957555\n",
      "Trained batch 973 batch loss 1.14427972 epoch total loss 1.18952906\n",
      "Trained batch 974 batch loss 1.08711123 epoch total loss 1.18942392\n",
      "Trained batch 975 batch loss 1.01024473 epoch total loss 1.18924022\n",
      "Trained batch 976 batch loss 1.14198184 epoch total loss 1.1891917\n",
      "Trained batch 977 batch loss 1.16034496 epoch total loss 1.18916225\n",
      "Trained batch 978 batch loss 1.09920418 epoch total loss 1.18907034\n",
      "Trained batch 979 batch loss 1.03432202 epoch total loss 1.18891227\n",
      "Trained batch 980 batch loss 1.07520759 epoch total loss 1.18879616\n",
      "Trained batch 981 batch loss 1.03717327 epoch total loss 1.18864167\n",
      "Trained batch 982 batch loss 1.26260614 epoch total loss 1.18871701\n",
      "Trained batch 983 batch loss 1.39153266 epoch total loss 1.18892324\n",
      "Trained batch 984 batch loss 1.22550118 epoch total loss 1.18896043\n",
      "Trained batch 985 batch loss 1.07597232 epoch total loss 1.18884563\n",
      "Trained batch 986 batch loss 1.28562462 epoch total loss 1.18894374\n",
      "Trained batch 987 batch loss 1.05870295 epoch total loss 1.1888119\n",
      "Trained batch 988 batch loss 1.01586 epoch total loss 1.18863678\n",
      "Trained batch 989 batch loss 1.17734265 epoch total loss 1.18862545\n",
      "Trained batch 990 batch loss 1.17052698 epoch total loss 1.1886071\n",
      "Trained batch 991 batch loss 1.162745 epoch total loss 1.18858099\n",
      "Trained batch 992 batch loss 1.17936873 epoch total loss 1.18857169\n",
      "Trained batch 993 batch loss 1.20672846 epoch total loss 1.18859\n",
      "Trained batch 994 batch loss 1.19269443 epoch total loss 1.18859422\n",
      "Trained batch 995 batch loss 1.06637681 epoch total loss 1.18847144\n",
      "Trained batch 996 batch loss 1.17884183 epoch total loss 1.18846178\n",
      "Trained batch 997 batch loss 1.15947247 epoch total loss 1.18843257\n",
      "Trained batch 998 batch loss 1.09238493 epoch total loss 1.18833637\n",
      "Trained batch 999 batch loss 1.23221183 epoch total loss 1.18838024\n",
      "Trained batch 1000 batch loss 1.22057045 epoch total loss 1.18841243\n",
      "Trained batch 1001 batch loss 1.1219418 epoch total loss 1.18834603\n",
      "Trained batch 1002 batch loss 1.33668482 epoch total loss 1.18849409\n",
      "Trained batch 1003 batch loss 1.41540575 epoch total loss 1.18872035\n",
      "Trained batch 1004 batch loss 1.46155167 epoch total loss 1.18899202\n",
      "Trained batch 1005 batch loss 1.37717438 epoch total loss 1.1891793\n",
      "Trained batch 1006 batch loss 1.13198948 epoch total loss 1.18912244\n",
      "Trained batch 1007 batch loss 1.2005446 epoch total loss 1.18913388\n",
      "Trained batch 1008 batch loss 1.09194565 epoch total loss 1.18903744\n",
      "Trained batch 1009 batch loss 1.08578861 epoch total loss 1.18893504\n",
      "Trained batch 1010 batch loss 1.0932765 epoch total loss 1.18884039\n",
      "Trained batch 1011 batch loss 1.0278393 epoch total loss 1.18868113\n",
      "Trained batch 1012 batch loss 1.1385231 epoch total loss 1.18863153\n",
      "Trained batch 1013 batch loss 1.02977085 epoch total loss 1.18847477\n",
      "Trained batch 1014 batch loss 1.0066222 epoch total loss 1.18829536\n",
      "Trained batch 1015 batch loss 1.0734756 epoch total loss 1.18818223\n",
      "Trained batch 1016 batch loss 1.09693682 epoch total loss 1.18809247\n",
      "Trained batch 1017 batch loss 1.16645992 epoch total loss 1.18807125\n",
      "Trained batch 1018 batch loss 0.962843418 epoch total loss 1.18785\n",
      "Trained batch 1019 batch loss 1.03176558 epoch total loss 1.18769681\n",
      "Trained batch 1020 batch loss 1.06660151 epoch total loss 1.1875782\n",
      "Trained batch 1021 batch loss 1.0894841 epoch total loss 1.18748212\n",
      "Trained batch 1022 batch loss 1.05702436 epoch total loss 1.18735445\n",
      "Trained batch 1023 batch loss 1.18787599 epoch total loss 1.18735492\n",
      "Trained batch 1024 batch loss 1.19173706 epoch total loss 1.18735921\n",
      "Trained batch 1025 batch loss 1.24417365 epoch total loss 1.18741465\n",
      "Trained batch 1026 batch loss 1.19213343 epoch total loss 1.18741918\n",
      "Trained batch 1027 batch loss 1.23744345 epoch total loss 1.18746793\n",
      "Trained batch 1028 batch loss 1.3043704 epoch total loss 1.18758154\n",
      "Trained batch 1029 batch loss 1.25967228 epoch total loss 1.18765163\n",
      "Trained batch 1030 batch loss 1.31791306 epoch total loss 1.187778\n",
      "Trained batch 1031 batch loss 1.11684012 epoch total loss 1.18770921\n",
      "Trained batch 1032 batch loss 1.12405336 epoch total loss 1.18764746\n",
      "Trained batch 1033 batch loss 1.13360929 epoch total loss 1.18759525\n",
      "Trained batch 1034 batch loss 1.18699706 epoch total loss 1.18759465\n",
      "Trained batch 1035 batch loss 1.1581732 epoch total loss 1.18756628\n",
      "Trained batch 1036 batch loss 1.39863324 epoch total loss 1.18777\n",
      "Trained batch 1037 batch loss 1.29701138 epoch total loss 1.18787539\n",
      "Trained batch 1038 batch loss 1.31905556 epoch total loss 1.18800175\n",
      "Trained batch 1039 batch loss 1.08678138 epoch total loss 1.18790436\n",
      "Trained batch 1040 batch loss 1.25859618 epoch total loss 1.18797231\n",
      "Trained batch 1041 batch loss 1.17099547 epoch total loss 1.18795598\n",
      "Trained batch 1042 batch loss 1.21106982 epoch total loss 1.18797815\n",
      "Trained batch 1043 batch loss 1.11110413 epoch total loss 1.18790448\n",
      "Trained batch 1044 batch loss 1.0325458 epoch total loss 1.1877557\n",
      "Trained batch 1045 batch loss 1.02656186 epoch total loss 1.18760157\n",
      "Trained batch 1046 batch loss 1.14977694 epoch total loss 1.18756533\n",
      "Trained batch 1047 batch loss 1.24045706 epoch total loss 1.18761587\n",
      "Trained batch 1048 batch loss 1.14784813 epoch total loss 1.18757796\n",
      "Trained batch 1049 batch loss 1.2557373 epoch total loss 1.18764293\n",
      "Trained batch 1050 batch loss 1.34762323 epoch total loss 1.18779528\n",
      "Trained batch 1051 batch loss 1.24274588 epoch total loss 1.18784761\n",
      "Trained batch 1052 batch loss 1.11681509 epoch total loss 1.18778014\n",
      "Trained batch 1053 batch loss 1.14247525 epoch total loss 1.18773711\n",
      "Trained batch 1054 batch loss 1.237216 epoch total loss 1.18778396\n",
      "Trained batch 1055 batch loss 1.23163128 epoch total loss 1.18782556\n",
      "Trained batch 1056 batch loss 0.898769617 epoch total loss 1.18755186\n",
      "Trained batch 1057 batch loss 0.952495575 epoch total loss 1.18732953\n",
      "Trained batch 1058 batch loss 0.998675108 epoch total loss 1.18715119\n",
      "Trained batch 1059 batch loss 1.16312444 epoch total loss 1.18712854\n",
      "Trained batch 1060 batch loss 1.36498141 epoch total loss 1.18729627\n",
      "Trained batch 1061 batch loss 1.43011761 epoch total loss 1.18752515\n",
      "Trained batch 1062 batch loss 1.26001191 epoch total loss 1.18759346\n",
      "Trained batch 1063 batch loss 1.16824412 epoch total loss 1.18757522\n",
      "Trained batch 1064 batch loss 1.13004291 epoch total loss 1.1875211\n",
      "Trained batch 1065 batch loss 1.10661459 epoch total loss 1.18744504\n",
      "Trained batch 1066 batch loss 1.07041883 epoch total loss 1.18733537\n",
      "Trained batch 1067 batch loss 1.25101936 epoch total loss 1.18739498\n",
      "Trained batch 1068 batch loss 1.22313952 epoch total loss 1.18742847\n",
      "Trained batch 1069 batch loss 1.1801604 epoch total loss 1.18742168\n",
      "Trained batch 1070 batch loss 1.19945335 epoch total loss 1.18743289\n",
      "Trained batch 1071 batch loss 1.02991593 epoch total loss 1.18728578\n",
      "Trained batch 1072 batch loss 1.08966267 epoch total loss 1.18719482\n",
      "Trained batch 1073 batch loss 1.11828303 epoch total loss 1.18713057\n",
      "Trained batch 1074 batch loss 1.12514472 epoch total loss 1.18707287\n",
      "Trained batch 1075 batch loss 1.14571917 epoch total loss 1.18703449\n",
      "Trained batch 1076 batch loss 1.26519108 epoch total loss 1.18710697\n",
      "Trained batch 1077 batch loss 1.2261759 epoch total loss 1.18714333\n",
      "Trained batch 1078 batch loss 1.20565474 epoch total loss 1.18716049\n",
      "Trained batch 1079 batch loss 1.1988256 epoch total loss 1.18717134\n",
      "Trained batch 1080 batch loss 1.14722562 epoch total loss 1.18713439\n",
      "Trained batch 1081 batch loss 1.11375856 epoch total loss 1.18706644\n",
      "Trained batch 1082 batch loss 1.13653231 epoch total loss 1.18701971\n",
      "Trained batch 1083 batch loss 1.1637702 epoch total loss 1.18699825\n",
      "Trained batch 1084 batch loss 1.18337202 epoch total loss 1.18699491\n",
      "Trained batch 1085 batch loss 1.18429053 epoch total loss 1.18699253\n",
      "Trained batch 1086 batch loss 1.26341963 epoch total loss 1.18706286\n",
      "Trained batch 1087 batch loss 1.30626547 epoch total loss 1.18717253\n",
      "Trained batch 1088 batch loss 1.31939542 epoch total loss 1.18729401\n",
      "Trained batch 1089 batch loss 1.33796477 epoch total loss 1.18743241\n",
      "Trained batch 1090 batch loss 1.2386353 epoch total loss 1.18747938\n",
      "Trained batch 1091 batch loss 1.17477703 epoch total loss 1.18746781\n",
      "Trained batch 1092 batch loss 1.16515255 epoch total loss 1.18744731\n",
      "Trained batch 1093 batch loss 1.13997042 epoch total loss 1.18740392\n",
      "Trained batch 1094 batch loss 1.24307048 epoch total loss 1.18745482\n",
      "Trained batch 1095 batch loss 1.12060428 epoch total loss 1.18739378\n",
      "Trained batch 1096 batch loss 1.14575851 epoch total loss 1.18735576\n",
      "Trained batch 1097 batch loss 1.00317121 epoch total loss 1.18718791\n",
      "Trained batch 1098 batch loss 0.988451958 epoch total loss 1.18700683\n",
      "Trained batch 1099 batch loss 0.931603789 epoch total loss 1.18677449\n",
      "Trained batch 1100 batch loss 1.21792436 epoch total loss 1.18680274\n",
      "Trained batch 1101 batch loss 1.41679692 epoch total loss 1.1870116\n",
      "Trained batch 1102 batch loss 1.47198391 epoch total loss 1.18727016\n",
      "Trained batch 1103 batch loss 1.34201074 epoch total loss 1.18741047\n",
      "Trained batch 1104 batch loss 1.31515646 epoch total loss 1.18752623\n",
      "Trained batch 1105 batch loss 1.32574129 epoch total loss 1.18765128\n",
      "Trained batch 1106 batch loss 1.38979149 epoch total loss 1.18783402\n",
      "Trained batch 1107 batch loss 1.3781749 epoch total loss 1.18800592\n",
      "Trained batch 1108 batch loss 1.4207629 epoch total loss 1.18821597\n",
      "Trained batch 1109 batch loss 1.22239685 epoch total loss 1.18824685\n",
      "Trained batch 1110 batch loss 1.34904087 epoch total loss 1.18839169\n",
      "Trained batch 1111 batch loss 1.36416209 epoch total loss 1.18854988\n",
      "Trained batch 1112 batch loss 1.22658658 epoch total loss 1.18858409\n",
      "Trained batch 1113 batch loss 1.16006279 epoch total loss 1.18855834\n",
      "Trained batch 1114 batch loss 1.21537733 epoch total loss 1.18858242\n",
      "Trained batch 1115 batch loss 1.12354553 epoch total loss 1.18852413\n",
      "Trained batch 1116 batch loss 1.18565607 epoch total loss 1.1885215\n",
      "Trained batch 1117 batch loss 1.23578644 epoch total loss 1.18856382\n",
      "Trained batch 1118 batch loss 1.27274621 epoch total loss 1.18863916\n",
      "Trained batch 1119 batch loss 1.3247087 epoch total loss 1.18876076\n",
      "Trained batch 1120 batch loss 1.25335717 epoch total loss 1.18881845\n",
      "Trained batch 1121 batch loss 1.26975322 epoch total loss 1.1888907\n",
      "Trained batch 1122 batch loss 1.17861152 epoch total loss 1.18888152\n",
      "Trained batch 1123 batch loss 1.11940408 epoch total loss 1.18881965\n",
      "Trained batch 1124 batch loss 1.0527246 epoch total loss 1.18869853\n",
      "Trained batch 1125 batch loss 1.05324697 epoch total loss 1.18857813\n",
      "Trained batch 1126 batch loss 1.20428014 epoch total loss 1.18859208\n",
      "Trained batch 1127 batch loss 1.18959284 epoch total loss 1.18859291\n",
      "Trained batch 1128 batch loss 1.30503058 epoch total loss 1.18869615\n",
      "Trained batch 1129 batch loss 1.1613251 epoch total loss 1.18867195\n",
      "Trained batch 1130 batch loss 1.16845906 epoch total loss 1.18865407\n",
      "Trained batch 1131 batch loss 1.21092224 epoch total loss 1.18867373\n",
      "Trained batch 1132 batch loss 1.12387204 epoch total loss 1.18861651\n",
      "Trained batch 1133 batch loss 1.1027348 epoch total loss 1.18854082\n",
      "Trained batch 1134 batch loss 1.08177257 epoch total loss 1.18844664\n",
      "Trained batch 1135 batch loss 1.09201407 epoch total loss 1.18836164\n",
      "Trained batch 1136 batch loss 1.02605891 epoch total loss 1.18821883\n",
      "Trained batch 1137 batch loss 1.07227385 epoch total loss 1.18811679\n",
      "Trained batch 1138 batch loss 1.00790155 epoch total loss 1.18795848\n",
      "Trained batch 1139 batch loss 1.11072814 epoch total loss 1.18789065\n",
      "Trained batch 1140 batch loss 1.00264847 epoch total loss 1.18772817\n",
      "Trained batch 1141 batch loss 1.17305911 epoch total loss 1.18771541\n",
      "Trained batch 1142 batch loss 1.31026483 epoch total loss 1.1878227\n",
      "Trained batch 1143 batch loss 1.08365262 epoch total loss 1.1877315\n",
      "Trained batch 1144 batch loss 1.23406148 epoch total loss 1.18777204\n",
      "Trained batch 1145 batch loss 1.20877886 epoch total loss 1.18779027\n",
      "Trained batch 1146 batch loss 1.06307578 epoch total loss 1.18768156\n",
      "Trained batch 1147 batch loss 1.13382125 epoch total loss 1.18763447\n",
      "Trained batch 1148 batch loss 1.23912537 epoch total loss 1.18767941\n",
      "Trained batch 1149 batch loss 1.2317071 epoch total loss 1.18771768\n",
      "Trained batch 1150 batch loss 0.934465408 epoch total loss 1.1874975\n",
      "Trained batch 1151 batch loss 0.992677629 epoch total loss 1.18732822\n",
      "Trained batch 1152 batch loss 1.07066262 epoch total loss 1.18722689\n",
      "Trained batch 1153 batch loss 1.15963733 epoch total loss 1.18720305\n",
      "Trained batch 1154 batch loss 1.26016796 epoch total loss 1.18726623\n",
      "Trained batch 1155 batch loss 1.41321504 epoch total loss 1.18746185\n",
      "Trained batch 1156 batch loss 1.29922616 epoch total loss 1.18755853\n",
      "Trained batch 1157 batch loss 1.15573633 epoch total loss 1.18753099\n",
      "Trained batch 1158 batch loss 1.1700145 epoch total loss 1.18751597\n",
      "Trained batch 1159 batch loss 1.25701237 epoch total loss 1.18757582\n",
      "Trained batch 1160 batch loss 1.20842433 epoch total loss 1.18759382\n",
      "Trained batch 1161 batch loss 1.17096055 epoch total loss 1.18757951\n",
      "Trained batch 1162 batch loss 1.24830461 epoch total loss 1.18763173\n",
      "Trained batch 1163 batch loss 1.27443612 epoch total loss 1.18770635\n",
      "Trained batch 1164 batch loss 1.15173674 epoch total loss 1.18767548\n",
      "Trained batch 1165 batch loss 1.16764951 epoch total loss 1.18765819\n",
      "Trained batch 1166 batch loss 1.0329845 epoch total loss 1.18752551\n",
      "Trained batch 1167 batch loss 1.11995316 epoch total loss 1.18746769\n",
      "Trained batch 1168 batch loss 1.23461556 epoch total loss 1.18750811\n",
      "Trained batch 1169 batch loss 1.13592458 epoch total loss 1.18746388\n",
      "Trained batch 1170 batch loss 1.11374104 epoch total loss 1.18740094\n",
      "Trained batch 1171 batch loss 0.966023564 epoch total loss 1.18721187\n",
      "Trained batch 1172 batch loss 1.23426008 epoch total loss 1.18725204\n",
      "Trained batch 1173 batch loss 1.04710865 epoch total loss 1.1871326\n",
      "Trained batch 1174 batch loss 1.11013269 epoch total loss 1.18706691\n",
      "Trained batch 1175 batch loss 1.32621813 epoch total loss 1.18718529\n",
      "Trained batch 1176 batch loss 1.22518301 epoch total loss 1.18721771\n",
      "Trained batch 1177 batch loss 1.15373981 epoch total loss 1.18718922\n",
      "Trained batch 1178 batch loss 1.14554942 epoch total loss 1.18715382\n",
      "Trained batch 1179 batch loss 1.09114265 epoch total loss 1.1870724\n",
      "Trained batch 1180 batch loss 1.02731168 epoch total loss 1.18693697\n",
      "Trained batch 1181 batch loss 1.14961028 epoch total loss 1.1869055\n",
      "Trained batch 1182 batch loss 1.15248859 epoch total loss 1.1868763\n",
      "Trained batch 1183 batch loss 1.23927522 epoch total loss 1.18692064\n",
      "Trained batch 1184 batch loss 1.24658406 epoch total loss 1.18697095\n",
      "Trained batch 1185 batch loss 1.19859171 epoch total loss 1.18698084\n",
      "Trained batch 1186 batch loss 1.15041566 epoch total loss 1.18695\n",
      "Trained batch 1187 batch loss 1.20615196 epoch total loss 1.18696618\n",
      "Trained batch 1188 batch loss 1.22688425 epoch total loss 1.1869998\n",
      "Trained batch 1189 batch loss 1.03759873 epoch total loss 1.18687415\n",
      "Trained batch 1190 batch loss 1.15748918 epoch total loss 1.18684947\n",
      "Trained batch 1191 batch loss 1.14315271 epoch total loss 1.18681276\n",
      "Trained batch 1192 batch loss 1.07866478 epoch total loss 1.18672204\n",
      "Trained batch 1193 batch loss 1.33031535 epoch total loss 1.18684232\n",
      "Trained batch 1194 batch loss 1.44111478 epoch total loss 1.18705535\n",
      "Trained batch 1195 batch loss 1.39150202 epoch total loss 1.18722641\n",
      "Trained batch 1196 batch loss 1.20980966 epoch total loss 1.18724537\n",
      "Trained batch 1197 batch loss 1.04650271 epoch total loss 1.18712783\n",
      "Trained batch 1198 batch loss 1.08239079 epoch total loss 1.18704033\n",
      "Trained batch 1199 batch loss 1.32186699 epoch total loss 1.18715286\n",
      "Trained batch 1200 batch loss 1.23795652 epoch total loss 1.18719518\n",
      "Trained batch 1201 batch loss 1.17314446 epoch total loss 1.18718338\n",
      "Trained batch 1202 batch loss 1.30815947 epoch total loss 1.18728399\n",
      "Trained batch 1203 batch loss 1.30735552 epoch total loss 1.18738377\n",
      "Trained batch 1204 batch loss 1.3680985 epoch total loss 1.18753386\n",
      "Trained batch 1205 batch loss 1.24322104 epoch total loss 1.18758\n",
      "Trained batch 1206 batch loss 1.29375648 epoch total loss 1.18766797\n",
      "Trained batch 1207 batch loss 1.21609735 epoch total loss 1.18769157\n",
      "Trained batch 1208 batch loss 1.16644955 epoch total loss 1.18767405\n",
      "Trained batch 1209 batch loss 1.27871823 epoch total loss 1.18774927\n",
      "Trained batch 1210 batch loss 1.23442125 epoch total loss 1.18778777\n",
      "Trained batch 1211 batch loss 1.17053926 epoch total loss 1.18777359\n",
      "Trained batch 1212 batch loss 1.12766039 epoch total loss 1.18772399\n",
      "Trained batch 1213 batch loss 1.12729883 epoch total loss 1.18767416\n",
      "Trained batch 1214 batch loss 1.12010169 epoch total loss 1.18761849\n",
      "Trained batch 1215 batch loss 1.22775912 epoch total loss 1.18765163\n",
      "Trained batch 1216 batch loss 1.14846396 epoch total loss 1.18761933\n",
      "Trained batch 1217 batch loss 1.24360824 epoch total loss 1.18766534\n",
      "Trained batch 1218 batch loss 1.1113323 epoch total loss 1.18760276\n",
      "Trained batch 1219 batch loss 1.06737137 epoch total loss 1.18750405\n",
      "Trained batch 1220 batch loss 1.09129906 epoch total loss 1.18742526\n",
      "Trained batch 1221 batch loss 1.0877378 epoch total loss 1.1873436\n",
      "Trained batch 1222 batch loss 0.996165931 epoch total loss 1.18718719\n",
      "Trained batch 1223 batch loss 1.10473597 epoch total loss 1.18711984\n",
      "Trained batch 1224 batch loss 1.02337182 epoch total loss 1.18698597\n",
      "Trained batch 1225 batch loss 1.14406943 epoch total loss 1.18695092\n",
      "Trained batch 1226 batch loss 1.04115498 epoch total loss 1.18683195\n",
      "Trained batch 1227 batch loss 1.18441939 epoch total loss 1.18683\n",
      "Trained batch 1228 batch loss 1.18380165 epoch total loss 1.18682766\n",
      "Trained batch 1229 batch loss 1.12209213 epoch total loss 1.18677497\n",
      "Trained batch 1230 batch loss 1.23747849 epoch total loss 1.1868161\n",
      "Trained batch 1231 batch loss 1.26892078 epoch total loss 1.18688285\n",
      "Trained batch 1232 batch loss 1.12942505 epoch total loss 1.18683612\n",
      "Trained batch 1233 batch loss 1.24275756 epoch total loss 1.18688154\n",
      "Trained batch 1234 batch loss 1.18973482 epoch total loss 1.18688381\n",
      "Trained batch 1235 batch loss 1.07246971 epoch total loss 1.18679118\n",
      "Trained batch 1236 batch loss 1.06178939 epoch total loss 1.18669009\n",
      "Trained batch 1237 batch loss 0.957007766 epoch total loss 1.18650436\n",
      "Trained batch 1238 batch loss 1.01993406 epoch total loss 1.18636978\n",
      "Trained batch 1239 batch loss 1.2291652 epoch total loss 1.18640435\n",
      "Trained batch 1240 batch loss 1.29054463 epoch total loss 1.18648827\n",
      "Trained batch 1241 batch loss 1.26066053 epoch total loss 1.18654799\n",
      "Trained batch 1242 batch loss 1.22162199 epoch total loss 1.18657637\n",
      "Trained batch 1243 batch loss 1.31143272 epoch total loss 1.18667674\n",
      "Trained batch 1244 batch loss 1.28884685 epoch total loss 1.18675888\n",
      "Trained batch 1245 batch loss 1.11888671 epoch total loss 1.18670428\n",
      "Trained batch 1246 batch loss 1.32016063 epoch total loss 1.18681145\n",
      "Trained batch 1247 batch loss 1.31947422 epoch total loss 1.18691778\n",
      "Trained batch 1248 batch loss 1.15962422 epoch total loss 1.18689597\n",
      "Trained batch 1249 batch loss 1.04792333 epoch total loss 1.18678474\n",
      "Trained batch 1250 batch loss 1.01548696 epoch total loss 1.18664777\n",
      "Trained batch 1251 batch loss 1.00282884 epoch total loss 1.18650079\n",
      "Trained batch 1252 batch loss 1.05760598 epoch total loss 1.18639791\n",
      "Trained batch 1253 batch loss 1.23667932 epoch total loss 1.18643796\n",
      "Trained batch 1254 batch loss 1.13765943 epoch total loss 1.1863991\n",
      "Trained batch 1255 batch loss 1.16053474 epoch total loss 1.18637848\n",
      "Trained batch 1256 batch loss 1.37136769 epoch total loss 1.18652582\n",
      "Trained batch 1257 batch loss 1.25514507 epoch total loss 1.1865803\n",
      "Trained batch 1258 batch loss 1.16024756 epoch total loss 1.18655944\n",
      "Trained batch 1259 batch loss 1.06340551 epoch total loss 1.18646157\n",
      "Trained batch 1260 batch loss 1.04422033 epoch total loss 1.18634868\n",
      "Trained batch 1261 batch loss 1.14144087 epoch total loss 1.18631303\n",
      "Trained batch 1262 batch loss 1.03035545 epoch total loss 1.18618953\n",
      "Trained batch 1263 batch loss 1.18356097 epoch total loss 1.18618751\n",
      "Trained batch 1264 batch loss 1.2154727 epoch total loss 1.18621063\n",
      "Trained batch 1265 batch loss 1.1851449 epoch total loss 1.1862098\n",
      "Trained batch 1266 batch loss 1.2573489 epoch total loss 1.18626595\n",
      "Trained batch 1267 batch loss 1.35368717 epoch total loss 1.18639815\n",
      "Trained batch 1268 batch loss 1.1752497 epoch total loss 1.18638933\n",
      "Trained batch 1269 batch loss 1.17868757 epoch total loss 1.18638325\n",
      "Trained batch 1270 batch loss 1.02064919 epoch total loss 1.18625271\n",
      "Trained batch 1271 batch loss 1.01045752 epoch total loss 1.18611443\n",
      "Trained batch 1272 batch loss 1.3026135 epoch total loss 1.1862061\n",
      "Trained batch 1273 batch loss 1.25927591 epoch total loss 1.18626344\n",
      "Trained batch 1274 batch loss 1.27915502 epoch total loss 1.1863364\n",
      "Trained batch 1275 batch loss 1.24685931 epoch total loss 1.18638384\n",
      "Trained batch 1276 batch loss 1.17833078 epoch total loss 1.18637753\n",
      "Trained batch 1277 batch loss 1.09979987 epoch total loss 1.18630981\n",
      "Trained batch 1278 batch loss 1.16249144 epoch total loss 1.1862911\n",
      "Trained batch 1279 batch loss 1.08336973 epoch total loss 1.18621063\n",
      "Trained batch 1280 batch loss 1.11759281 epoch total loss 1.18615699\n",
      "Trained batch 1281 batch loss 1.28179717 epoch total loss 1.18623161\n",
      "Trained batch 1282 batch loss 1.29264569 epoch total loss 1.18631458\n",
      "Trained batch 1283 batch loss 1.06868851 epoch total loss 1.18622303\n",
      "Trained batch 1284 batch loss 1.11833858 epoch total loss 1.1861701\n",
      "Trained batch 1285 batch loss 1.12617648 epoch total loss 1.18612337\n",
      "Trained batch 1286 batch loss 1.1527729 epoch total loss 1.1860975\n",
      "Trained batch 1287 batch loss 1.16800165 epoch total loss 1.18608344\n",
      "Trained batch 1288 batch loss 1.14776683 epoch total loss 1.18605375\n",
      "Trained batch 1289 batch loss 1.20590019 epoch total loss 1.18606913\n",
      "Trained batch 1290 batch loss 1.15977371 epoch total loss 1.18604875\n",
      "Trained batch 1291 batch loss 1.11993384 epoch total loss 1.18599749\n",
      "Trained batch 1292 batch loss 1.13178611 epoch total loss 1.18595564\n",
      "Trained batch 1293 batch loss 1.16072118 epoch total loss 1.18593609\n",
      "Trained batch 1294 batch loss 1.18290472 epoch total loss 1.18593371\n",
      "Trained batch 1295 batch loss 1.04800725 epoch total loss 1.18582726\n",
      "Trained batch 1296 batch loss 1.21900523 epoch total loss 1.18585277\n",
      "Trained batch 1297 batch loss 1.10653138 epoch total loss 1.18579173\n",
      "Trained batch 1298 batch loss 1.23152399 epoch total loss 1.1858269\n",
      "Trained batch 1299 batch loss 1.15889049 epoch total loss 1.18580627\n",
      "Trained batch 1300 batch loss 1.07252431 epoch total loss 1.18571913\n",
      "Trained batch 1301 batch loss 1.07329476 epoch total loss 1.18563259\n",
      "Trained batch 1302 batch loss 1.11254954 epoch total loss 1.18557656\n",
      "Trained batch 1303 batch loss 0.957857847 epoch total loss 1.1854018\n",
      "Trained batch 1304 batch loss 1.09420156 epoch total loss 1.18533182\n",
      "Trained batch 1305 batch loss 1.07754922 epoch total loss 1.18524921\n",
      "Trained batch 1306 batch loss 1.05168688 epoch total loss 1.18514693\n",
      "Trained batch 1307 batch loss 0.934520423 epoch total loss 1.18495524\n",
      "Trained batch 1308 batch loss 0.990777 epoch total loss 1.1848067\n",
      "Trained batch 1309 batch loss 1.10332346 epoch total loss 1.18474448\n",
      "Trained batch 1310 batch loss 1.07858872 epoch total loss 1.18466341\n",
      "Trained batch 1311 batch loss 1.11909068 epoch total loss 1.18461347\n",
      "Trained batch 1312 batch loss 1.06076813 epoch total loss 1.18451905\n",
      "Trained batch 1313 batch loss 1.07416701 epoch total loss 1.18443501\n",
      "Trained batch 1314 batch loss 1.11312437 epoch total loss 1.18438077\n",
      "Trained batch 1315 batch loss 1.41562617 epoch total loss 1.18455672\n",
      "Trained batch 1316 batch loss 1.36678052 epoch total loss 1.18469512\n",
      "Trained batch 1317 batch loss 1.0899806 epoch total loss 1.18462324\n",
      "Trained batch 1318 batch loss 1.30540383 epoch total loss 1.18471491\n",
      "Trained batch 1319 batch loss 1.3520757 epoch total loss 1.18484175\n",
      "Trained batch 1320 batch loss 1.27463257 epoch total loss 1.18490982\n",
      "Trained batch 1321 batch loss 1.1685586 epoch total loss 1.18489742\n",
      "Trained batch 1322 batch loss 1.10279775 epoch total loss 1.18483531\n",
      "Trained batch 1323 batch loss 1.1908375 epoch total loss 1.18483984\n",
      "Trained batch 1324 batch loss 1.32700133 epoch total loss 1.18494725\n",
      "Trained batch 1325 batch loss 1.15006864 epoch total loss 1.18492091\n",
      "Trained batch 1326 batch loss 1.16059947 epoch total loss 1.18490255\n",
      "Trained batch 1327 batch loss 1.23382151 epoch total loss 1.18493938\n",
      "Trained batch 1328 batch loss 1.24219847 epoch total loss 1.18498254\n",
      "Trained batch 1329 batch loss 1.31816876 epoch total loss 1.18508267\n",
      "Trained batch 1330 batch loss 1.21359074 epoch total loss 1.18510413\n",
      "Trained batch 1331 batch loss 1.21833253 epoch total loss 1.18512917\n",
      "Trained batch 1332 batch loss 1.28584373 epoch total loss 1.18520474\n",
      "Trained batch 1333 batch loss 1.39826298 epoch total loss 1.1853646\n",
      "Trained batch 1334 batch loss 1.23951447 epoch total loss 1.18540525\n",
      "Trained batch 1335 batch loss 1.31455219 epoch total loss 1.18550193\n",
      "Trained batch 1336 batch loss 1.19442439 epoch total loss 1.18550873\n",
      "Trained batch 1337 batch loss 1.1178031 epoch total loss 1.18545806\n",
      "Trained batch 1338 batch loss 1.26547241 epoch total loss 1.18551791\n",
      "Trained batch 1339 batch loss 1.22580481 epoch total loss 1.18554795\n",
      "Trained batch 1340 batch loss 1.17507958 epoch total loss 1.18554008\n",
      "Trained batch 1341 batch loss 1.21493435 epoch total loss 1.18556213\n",
      "Trained batch 1342 batch loss 1.22138786 epoch total loss 1.18558884\n",
      "Trained batch 1343 batch loss 1.1747427 epoch total loss 1.18558073\n",
      "Trained batch 1344 batch loss 1.21126688 epoch total loss 1.1855998\n",
      "Trained batch 1345 batch loss 1.29572403 epoch total loss 1.1856817\n",
      "Trained batch 1346 batch loss 1.16367853 epoch total loss 1.18566537\n",
      "Trained batch 1347 batch loss 1.20248008 epoch total loss 1.18567789\n",
      "Trained batch 1348 batch loss 1.20723164 epoch total loss 1.18569398\n",
      "Trained batch 1349 batch loss 1.2031219 epoch total loss 1.18570685\n",
      "Trained batch 1350 batch loss 1.26265705 epoch total loss 1.18576384\n",
      "Trained batch 1351 batch loss 1.25300491 epoch total loss 1.18581367\n",
      "Trained batch 1352 batch loss 1.30204761 epoch total loss 1.18589962\n",
      "Trained batch 1353 batch loss 1.15808487 epoch total loss 1.18587911\n",
      "Trained batch 1354 batch loss 1.18880367 epoch total loss 1.18588126\n",
      "Trained batch 1355 batch loss 1.24455976 epoch total loss 1.18592453\n",
      "Trained batch 1356 batch loss 1.18501306 epoch total loss 1.18592393\n",
      "Trained batch 1357 batch loss 1.20192909 epoch total loss 1.18593562\n",
      "Trained batch 1358 batch loss 1.23004472 epoch total loss 1.18596816\n",
      "Trained batch 1359 batch loss 1.13102329 epoch total loss 1.18592775\n",
      "Trained batch 1360 batch loss 1.29276407 epoch total loss 1.18600631\n",
      "Trained batch 1361 batch loss 1.2747364 epoch total loss 1.18607152\n",
      "Trained batch 1362 batch loss 1.25479138 epoch total loss 1.18612194\n",
      "Trained batch 1363 batch loss 1.20952082 epoch total loss 1.18613899\n",
      "Trained batch 1364 batch loss 1.27292931 epoch total loss 1.18620265\n",
      "Trained batch 1365 batch loss 1.27213383 epoch total loss 1.18626559\n",
      "Trained batch 1366 batch loss 1.16215324 epoch total loss 1.18624794\n",
      "Trained batch 1367 batch loss 1.13321543 epoch total loss 1.18620908\n",
      "Trained batch 1368 batch loss 1.16021907 epoch total loss 1.18619013\n",
      "Trained batch 1369 batch loss 1.10665143 epoch total loss 1.18613207\n",
      "Trained batch 1370 batch loss 1.05312109 epoch total loss 1.18603504\n",
      "Trained batch 1371 batch loss 1.19858384 epoch total loss 1.1860441\n",
      "Trained batch 1372 batch loss 1.21735549 epoch total loss 1.18606699\n",
      "Trained batch 1373 batch loss 1.38544393 epoch total loss 1.1862123\n",
      "Trained batch 1374 batch loss 1.35637414 epoch total loss 1.18633604\n",
      "Trained batch 1375 batch loss 1.3217653 epoch total loss 1.18643463\n",
      "Trained batch 1376 batch loss 1.24129593 epoch total loss 1.18647444\n",
      "Trained batch 1377 batch loss 1.25871813 epoch total loss 1.18652689\n",
      "Trained batch 1378 batch loss 1.19088352 epoch total loss 1.18653011\n",
      "Trained batch 1379 batch loss 1.16369236 epoch total loss 1.18651354\n",
      "Trained batch 1380 batch loss 1.17947614 epoch total loss 1.18650842\n",
      "Trained batch 1381 batch loss 1.16486311 epoch total loss 1.1864928\n",
      "Trained batch 1382 batch loss 1.14685833 epoch total loss 1.18646407\n",
      "Trained batch 1383 batch loss 1.19791818 epoch total loss 1.1864723\n",
      "Trained batch 1384 batch loss 1.12436962 epoch total loss 1.18642747\n",
      "Trained batch 1385 batch loss 1.19149041 epoch total loss 1.18643117\n",
      "Trained batch 1386 batch loss 1.07433343 epoch total loss 1.18635023\n",
      "Trained batch 1387 batch loss 1.15652883 epoch total loss 1.18632877\n",
      "Trained batch 1388 batch loss 1.12446034 epoch total loss 1.18628418\n",
      "Epoch 5 train loss 1.1862841844558716\n",
      "Validated batch 1 batch loss 1.27908993\n",
      "Validated batch 2 batch loss 1.17688823\n",
      "Validated batch 3 batch loss 1.16974258\n",
      "Validated batch 4 batch loss 1.17445338\n",
      "Validated batch 5 batch loss 1.30817091\n",
      "Validated batch 6 batch loss 1.33921432\n",
      "Validated batch 7 batch loss 1.16386223\n",
      "Validated batch 8 batch loss 1.24645364\n",
      "Validated batch 9 batch loss 1.20961142\n",
      "Validated batch 10 batch loss 1.2071079\n",
      "Validated batch 11 batch loss 1.25077009\n",
      "Validated batch 12 batch loss 1.11785197\n",
      "Validated batch 13 batch loss 1.35548139\n",
      "Validated batch 14 batch loss 1.10693026\n",
      "Validated batch 15 batch loss 1.27950513\n",
      "Validated batch 16 batch loss 1.25649655\n",
      "Validated batch 17 batch loss 1.28242803\n",
      "Validated batch 18 batch loss 1.05334222\n",
      "Validated batch 19 batch loss 1.25264418\n",
      "Validated batch 20 batch loss 1.12624288\n",
      "Validated batch 21 batch loss 1.23426068\n",
      "Validated batch 22 batch loss 1.18635333\n",
      "Validated batch 23 batch loss 1.19662631\n",
      "Validated batch 24 batch loss 1.31980729\n",
      "Validated batch 25 batch loss 1.23874199\n",
      "Validated batch 26 batch loss 1.11549962\n",
      "Validated batch 27 batch loss 1.15599859\n",
      "Validated batch 28 batch loss 1.09958816\n",
      "Validated batch 29 batch loss 1.22082198\n",
      "Validated batch 30 batch loss 1.26246119\n",
      "Validated batch 31 batch loss 1.03031373\n",
      "Validated batch 32 batch loss 1.21568906\n",
      "Validated batch 33 batch loss 1.16686666\n",
      "Validated batch 34 batch loss 1.22762287\n",
      "Validated batch 35 batch loss 1.21822119\n",
      "Validated batch 36 batch loss 1.19900835\n",
      "Validated batch 37 batch loss 1.13525367\n",
      "Validated batch 38 batch loss 1.10921454\n",
      "Validated batch 39 batch loss 1.17042303\n",
      "Validated batch 40 batch loss 1.19005883\n",
      "Validated batch 41 batch loss 1.21968508\n",
      "Validated batch 42 batch loss 1.20230079\n",
      "Validated batch 43 batch loss 1.40746462\n",
      "Validated batch 44 batch loss 1.2277739\n",
      "Validated batch 45 batch loss 1.17213762\n",
      "Validated batch 46 batch loss 1.08730459\n",
      "Validated batch 47 batch loss 1.11691213\n",
      "Validated batch 48 batch loss 1.11863625\n",
      "Validated batch 49 batch loss 1.17859602\n",
      "Validated batch 50 batch loss 1.07073748\n",
      "Validated batch 51 batch loss 1.13649428\n",
      "Validated batch 52 batch loss 1.21830082\n",
      "Validated batch 53 batch loss 1.2306931\n",
      "Validated batch 54 batch loss 1.26342463\n",
      "Validated batch 55 batch loss 1.19837213\n",
      "Validated batch 56 batch loss 1.12924063\n",
      "Validated batch 57 batch loss 1.23345983\n",
      "Validated batch 58 batch loss 1.35555029\n",
      "Validated batch 59 batch loss 1.29545116\n",
      "Validated batch 60 batch loss 1.26499522\n",
      "Validated batch 61 batch loss 1.2334609\n",
      "Validated batch 62 batch loss 1.30571961\n",
      "Validated batch 63 batch loss 1.30051124\n",
      "Validated batch 64 batch loss 1.08456612\n",
      "Validated batch 65 batch loss 1.17936873\n",
      "Validated batch 66 batch loss 1.2853682\n",
      "Validated batch 67 batch loss 1.22305024\n",
      "Validated batch 68 batch loss 1.18337917\n",
      "Validated batch 69 batch loss 1.1948415\n",
      "Validated batch 70 batch loss 1.14663124\n",
      "Validated batch 71 batch loss 1.06044984\n",
      "Validated batch 72 batch loss 1.15075338\n",
      "Validated batch 73 batch loss 1.11924136\n",
      "Validated batch 74 batch loss 1.12223732\n",
      "Validated batch 75 batch loss 1.22821593\n",
      "Validated batch 76 batch loss 1.22625923\n",
      "Validated batch 77 batch loss 1.16125941\n",
      "Validated batch 78 batch loss 1.18902993\n",
      "Validated batch 79 batch loss 1.13329458\n",
      "Validated batch 80 batch loss 1.20071363\n",
      "Validated batch 81 batch loss 1.21470356\n",
      "Validated batch 82 batch loss 1.11313903\n",
      "Validated batch 83 batch loss 1.14997721\n",
      "Validated batch 84 batch loss 1.2343297\n",
      "Validated batch 85 batch loss 1.13912249\n",
      "Validated batch 86 batch loss 1.35893822\n",
      "Validated batch 87 batch loss 1.22064745\n",
      "Validated batch 88 batch loss 1.09581232\n",
      "Validated batch 89 batch loss 1.24598038\n",
      "Validated batch 90 batch loss 1.17694271\n",
      "Validated batch 91 batch loss 1.08690381\n",
      "Validated batch 92 batch loss 1.21456528\n",
      "Validated batch 93 batch loss 1.22784472\n",
      "Validated batch 94 batch loss 1.22773075\n",
      "Validated batch 95 batch loss 1.10427952\n",
      "Validated batch 96 batch loss 1.18868971\n",
      "Validated batch 97 batch loss 1.19771862\n",
      "Validated batch 98 batch loss 1.20508301\n",
      "Validated batch 99 batch loss 1.1886363\n",
      "Validated batch 100 batch loss 1.19709086\n",
      "Validated batch 101 batch loss 1.15477562\n",
      "Validated batch 102 batch loss 1.27731323\n",
      "Validated batch 103 batch loss 1.1115346\n",
      "Validated batch 104 batch loss 1.084705\n",
      "Validated batch 105 batch loss 1.16238022\n",
      "Validated batch 106 batch loss 1.29328513\n",
      "Validated batch 107 batch loss 1.33172846\n",
      "Validated batch 108 batch loss 1.36137033\n",
      "Validated batch 109 batch loss 1.19788051\n",
      "Validated batch 110 batch loss 1.33627713\n",
      "Validated batch 111 batch loss 1.1949625\n",
      "Validated batch 112 batch loss 1.25013912\n",
      "Validated batch 113 batch loss 1.25736713\n",
      "Validated batch 114 batch loss 0.970433414\n",
      "Validated batch 115 batch loss 1.17139828\n",
      "Validated batch 116 batch loss 1.16230559\n",
      "Validated batch 117 batch loss 1.19717538\n",
      "Validated batch 118 batch loss 1.16215885\n",
      "Validated batch 119 batch loss 1.08630919\n",
      "Validated batch 120 batch loss 1.0971812\n",
      "Validated batch 121 batch loss 1.27346134\n",
      "Validated batch 122 batch loss 1.0999608\n",
      "Validated batch 123 batch loss 1.05433941\n",
      "Validated batch 124 batch loss 1.14936304\n",
      "Validated batch 125 batch loss 1.12102854\n",
      "Validated batch 126 batch loss 1.04810405\n",
      "Validated batch 127 batch loss 1.18855762\n",
      "Validated batch 128 batch loss 1.11416745\n",
      "Validated batch 129 batch loss 1.10938954\n",
      "Validated batch 130 batch loss 1.27720284\n",
      "Validated batch 131 batch loss 1.2791791\n",
      "Validated batch 132 batch loss 1.10486388\n",
      "Validated batch 133 batch loss 1.30769897\n",
      "Validated batch 134 batch loss 0.981024\n",
      "Validated batch 135 batch loss 1.08648658\n",
      "Validated batch 136 batch loss 1.14213037\n",
      "Validated batch 137 batch loss 1.16905117\n",
      "Validated batch 138 batch loss 1.30225885\n",
      "Validated batch 139 batch loss 1.12857985\n",
      "Validated batch 140 batch loss 1.12656653\n",
      "Validated batch 141 batch loss 1.23546\n",
      "Validated batch 142 batch loss 1.1102705\n",
      "Validated batch 143 batch loss 1.21682537\n",
      "Validated batch 144 batch loss 1.35909081\n",
      "Validated batch 145 batch loss 1.03580308\n",
      "Validated batch 146 batch loss 1.1803813\n",
      "Validated batch 147 batch loss 1.14327931\n",
      "Validated batch 148 batch loss 1.18674827\n",
      "Validated batch 149 batch loss 1.23644137\n",
      "Validated batch 150 batch loss 1.13926268\n",
      "Validated batch 151 batch loss 0.90125525\n",
      "Validated batch 152 batch loss 1.17202199\n",
      "Validated batch 153 batch loss 1.08996654\n",
      "Validated batch 154 batch loss 1.12571287\n",
      "Validated batch 155 batch loss 1.23027158\n",
      "Validated batch 156 batch loss 1.04072452\n",
      "Validated batch 157 batch loss 1.18261623\n",
      "Validated batch 158 batch loss 1.2717191\n",
      "Validated batch 159 batch loss 1.23229063\n",
      "Validated batch 160 batch loss 1.15845919\n",
      "Validated batch 161 batch loss 1.08567977\n",
      "Validated batch 162 batch loss 1.14748311\n",
      "Validated batch 163 batch loss 1.21241164\n",
      "Validated batch 164 batch loss 1.24272382\n",
      "Validated batch 165 batch loss 1.09327269\n",
      "Validated batch 166 batch loss 1.19896841\n",
      "Validated batch 167 batch loss 1.2310586\n",
      "Validated batch 168 batch loss 1.27547741\n",
      "Validated batch 169 batch loss 1.32224846\n",
      "Validated batch 170 batch loss 1.24310231\n",
      "Validated batch 171 batch loss 1.12686288\n",
      "Validated batch 172 batch loss 1.17565966\n",
      "Validated batch 173 batch loss 1.21607351\n",
      "Validated batch 174 batch loss 1.18770218\n",
      "Validated batch 175 batch loss 1.25804043\n",
      "Validated batch 176 batch loss 1.35331321\n",
      "Validated batch 177 batch loss 1.17253029\n",
      "Validated batch 178 batch loss 1.27469015\n",
      "Validated batch 179 batch loss 1.21704352\n",
      "Validated batch 180 batch loss 1.17871046\n",
      "Validated batch 181 batch loss 1.17661774\n",
      "Validated batch 182 batch loss 1.04704046\n",
      "Validated batch 183 batch loss 1.22619259\n",
      "Validated batch 184 batch loss 1.14494646\n",
      "Validated batch 185 batch loss 1.22349811\n",
      "Epoch 5 val loss 1.1886335611343384\n",
      "Model /aiffel/aiffel/mpii/models/model-epoch-5-loss-1.1886.h5 saved.\n"
     ]
    }
   ],
   "source": [
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.0007\n",
    "\n",
    "best_model_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f15e59cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = os.path.join(PROJECT_PATH, 'models', 'model-epoch-5-loss-1.1886.h5')\n",
    "\n",
    "model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1)\n",
    "model.load_weights(WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb7812b",
   "metadata": {},
   "source": [
    "#### STEP 3. 예측 엔진 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49057fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_ANKLE = 0\n",
    "R_KNEE = 1\n",
    "R_HIP = 2\n",
    "L_HIP = 3\n",
    "L_KNEE = 4\n",
    "L_ANKLE = 5\n",
    "PELVIS = 6\n",
    "THORAX = 7\n",
    "UPPER_NECK = 8\n",
    "HEAD_TOP = 9\n",
    "R_WRIST = 10\n",
    "R_ELBOW = 11\n",
    "R_SHOULDER = 12\n",
    "L_SHOULDER = 13\n",
    "L_ELBOW = 14\n",
    "L_WRIST = 15\n",
    "\n",
    "MPII_BONES = [\n",
    "    [R_ANKLE, R_KNEE],\n",
    "    [R_KNEE, R_HIP],\n",
    "    [R_HIP, PELVIS],\n",
    "    [L_HIP, PELVIS],\n",
    "    [L_HIP, L_KNEE],\n",
    "    [L_KNEE, L_ANKLE],\n",
    "    [PELVIS, THORAX],\n",
    "    [THORAX, UPPER_NECK],\n",
    "    [UPPER_NECK, HEAD_TOP],\n",
    "    [R_WRIST, R_ELBOW],\n",
    "    [R_ELBOW, R_SHOULDER],\n",
    "    [THORAX, R_SHOULDER],\n",
    "    [THORAX, L_SHOULDER],\n",
    "    [L_SHOULDER, L_ELBOW],\n",
    "    [L_ELBOW, L_WRIST]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c6d7078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_coordinates(heatmaps):\n",
    "    flatten_heatmaps = tf.reshape(heatmaps, (-1, 16))\n",
    "    indices = tf.math.argmax(flatten_heatmaps, axis=0)\n",
    "    y = tf.cast(indices / 64, dtype=tf.int64)\n",
    "    x = indices - 64 * y\n",
    "    return tf.stack([x, y], axis=1).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ca33285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_from_heatmap(heatmaps):\n",
    "    max_keypoints = find_max_coordinates(heatmaps)\n",
    "\n",
    "    padded_heatmap = np.pad(heatmaps, [[1,1],[1,1],[0,0]], mode='constant')\n",
    "    adjusted_keypoints = []\n",
    "    for i, keypoint in enumerate(max_keypoints):\n",
    "        max_y = keypoint[1]+1\n",
    "        max_x = keypoint[0]+1\n",
    "        \n",
    "        patch = padded_heatmap[max_y-1:max_y+2, max_x-1:max_x+2, i]\n",
    "        patch[1][1] = 0\n",
    "        \n",
    "        index = np.argmax(patch)\n",
    "        \n",
    "        next_y = index // 3\n",
    "        next_x = index - next_y * 3\n",
    "        delta_y = (next_y - 1) / 4\n",
    "        delta_x = (next_x - 1) / 4\n",
    "        \n",
    "        adjusted_keypoint_x = keypoint[0] + delta_x\n",
    "        adjusted_keypoint_y = keypoint[1] + delta_y\n",
    "        adjusted_keypoints.append((adjusted_keypoint_x, adjusted_keypoint_y))\n",
    "        \n",
    "    adjusted_keypoints = np.clip(adjusted_keypoints, 0, 64)\n",
    "    normalized_keypoints = adjusted_keypoints / 64\n",
    "    return normalized_keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a389008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image_path):\n",
    "    encoded = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(encoded)\n",
    "    inputs = tf.image.resize(image, (256, 256))\n",
    "    inputs = tf.cast(inputs, tf.float32) / 127.5 - 1\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    outputs = model(inputs, training=False)\n",
    "    if type(outputs) != list:\n",
    "        outputs = [outputs]\n",
    "    heatmap = tf.squeeze(outputs[-1], axis=0).numpy()\n",
    "    kp = extract_keypoints_from_heatmap(heatmap)\n",
    "    return image, kp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff8a50fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        if index is not None and index != i:\n",
    "            continue\n",
    "        plt.scatter(joint_x, joint_y, s=10, c='red', marker='o')\n",
    "    plt.show()\n",
    "\n",
    "def draw_skeleton_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        joints.append((joint_x, joint_y))\n",
    "    \n",
    "    for bone in MPII_BONES:\n",
    "        joint_1 = joints[bone[0]]\n",
    "        joint_2 = joints[bone[1]]\n",
    "        plt.plot([joint_1[0], joint_2[0]], [joint_1[1], joint_2[1]], linewidth=5, alpha=0.7)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "036f1c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image = os.path.join(PROJECT_PATH, 'test_image.jpg')\n",
    "\n",
    "image, keypoints = predict(model, test_image)\n",
    "draw_keypoints_on_image(image, keypoints)\n",
    "draw_skeleton_on_image(image, keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43163150",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4848869",
   "metadata": {},
   "source": [
    "### (결론)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff5c7e6",
   "metadata": {},
   "source": [
    "어느정도 사람 형태를 잡아내기는 했으나 완벽하지 않은 결과물이 출력됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77152015",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05a9113",
   "metadata": {},
   "source": [
    "### 루브릭."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bac2d97",
   "metadata": {},
   "source": [
    "|평가문항|비고|상세기준|비고 |\n",
    "|:---|:---:|:---|---:|\n",
    "|tfrecord를 활용한 데이터셋 구성과 전처리를 통해 프로젝트 베이스라인 구성을 확인하였다. | |MPII 데이터셋을 기반으로 1epoch에 30분 이내에 학습가능한 베이스라인을 구축하였다. | |\n",
    "|simplebaseline 모델을 정상적으로 구현하였다. | |simplebaseline 모델을 구현하여 실습코드의 모델을 대체하여 정상적으로 학습이 진행되었다. | |\n",
    "|Hourglass 모델과 simplebaseline 모델을 비교분석한 결과를 체계적으로 정리하였다. | |두 모델의 pose estimation 테스트결과 이미지 및 학습진행상황 등을 체계적으로 비교분석하였다. | |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09adc0bb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ada24",
   "metadata": {},
   "source": [
    "### 회고록."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638afe0d",
   "metadata": {},
   "source": [
    "HPE모델을 구성하기 위해 MPII 데이터셋을 이용해 직접 구현해보는 마지막 고잉디퍼 프로젝트였다.  \n",
    "어려운 내용이 많았고 참고할만한 깃허브가 없으며 몸상태로 좋지 않았기에 이번 프로젝트의 완성도는 매우 떨어진다.  \n",
    "추후 시간을 들여서 HPE와 simplebaseline 모델을 다시 한번 구현해 볼 필요가 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8c6596",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f472c8",
   "metadata": {},
   "source": [
    "### Reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b21808",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98604c4f",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
